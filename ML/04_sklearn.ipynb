{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn \n",
    "\n",
    "Главная библиотека для классического ML. Сегодня ознакомимся с основным пайплайном на примере задачи бинарной классификации табличных данных.\n",
    "\n",
    "В sklearn есть очень качественный, проиллюстрированный кодом [мануал](https://scikit-learn.org/stable/index.html), который способен заменить учебник по \"ML для практиков\".\n",
    "\n",
    "## Бинарная классификация\n",
    "Датасет состоит из пар $(x,y)$, где $y$ - \"правильный класс\". **Бинарная** классификация - это когда у $y$ всего два значения, кодируемые как 0 или 1.\n",
    "\n",
    "**N.B.** Часто в подобных ситуациях это \"присутствие\" или \"отсутсвие\" какого-то признака. Такая формулировка не влияет на суть происходящего, но позволяет лучше понять мотивацию некоторых концептов ниже. А еще она нужно, конгда мы будем обобщать штуки из сегодняшнего урока на случай нескольких классов. Одна из основных идей - относиться к проблеме классификации с $K$ классами как $K$ задачам бинарной: предсказываем (конкретный класс) vs (остальные).\n",
    "\n",
    "\n",
    "Про датасет мы предполагаем что $x$ - фиксированы (они будут всегда появляться только справа от черты во всех условных вероятностях), а $y$ - независимо порождены из некоторого *истинного* распределения $P(y|x)$.\n",
    "\n",
    "## Метрики классификации\n",
    "\n",
    "Начнем с того, что научимся оценивать уже имеющиеся предсказания, то есть оценка $\\hat P(y|x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": list(range(1, 8)),\n",
    "        \"score\": [\n",
    "            0.5,\n",
    "            0.1,\n",
    "            0.2,\n",
    "            0.6,\n",
    "            0.2,\n",
    "            0.3,\n",
    "            0.0,\n",
    "        ],  # так обычно выглядит выход из бинарного классификатора\n",
    "        \"class\": [0, 0, 0, 1, 1, 1, 0],  # \"правильные\" метки\n",
    "    }\n",
    ")\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP, TN, FP, FN, confusion matrix\n",
    "\n",
    "В бинарной классификации для каждого наблюдаемого бывает всего 4 ситуации, и у каждой свое название. Похоже, что это пришло из Statistical hypothesis testing, где есть \"нулевая гипотеза\" (верная или неверная) и результат статистического теста (отвергнуть гипотезу, или принять).\n",
    "\n",
    "\n",
    "True Positive - предсказано 1, метка -- 1\n",
    "\n",
    "True Negative - предсказано 0, метка -- 0\n",
    "\n",
    "False Positive (a.k.a ошибка первого рода) - предсказано 1, метка -- 0 \n",
    "\n",
    "False Negative (a.k.a ошибка второго рода) - предсказано 0, метка -- 1\n",
    "\n",
    "## Через эти ошибки можно выразиьт целое семейство метрик бинарной классификации:\n",
    "\n",
    "**Accuracy** -- доля верно классифицированных точек данных\n",
    "\n",
    "**Recall** -- доля точек данных предсказанного первого класса среди истинного первого класса\n",
    "\n",
    "**Precision** -- наоборот, доля точек данных, которые принадлежат истинному первому классу среди тех, что модель отнесла к нему\n",
    "\n",
    "**F1-score** -- среднее гармоническое точности и полноты\n",
    "\n",
    "Упражнение: выразить через TP, TN, FP, FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = (TP+TN)/(TP + FN + FP + TN)\n",
    "\n",
    "recall = (TP)/(TP + FN)\n",
    "\n",
    "precision = (TP)/(TP + FP)\n",
    "\n",
    "f1 = 2 * Precision * Recall / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-vs-Rest\n",
    "Представим на секунду, что у нас не два класса, а, скажем, N. \n",
    "\n",
    "Как мне обобщить полученные метрики на этот случай?\n",
    "\n",
    "* macro averaging\n",
    "\n",
    "* weighted averaging\n",
    "\n",
    "* micro averaging\n",
    "\n",
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# это уже не только для бинарной классификации, можно построить confusion matrix для любого числа классов\n",
    "sns.heatmap(\n",
    "    confusion_matrix(df[\"score\"] >= 0.5, df[\"class\"]),  # а почему 0.5?\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, все метрики уже есть в библиотеке. Давайте проверим ваши формулы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "df[\">.2\"] = df[\"score\"].map(lambda x: 1 if x >= 0.2 else 0)\n",
    "df  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\">.2\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = df[(df[\"class\"] == 1) & (df[\">.2\"] == 1)].count()[\"id\"]\n",
    "FN = df[(df[\"class\"] == 1) & (df[\">.2\"] == 0)].count()[\"id\"]\n",
    "FP = df[(df[\"class\"] == 0) & (df[\">.2\"] == 1)].count()[\"id\"]\n",
    "TN = df[(df[\"class\"] == 0) & (df[\">.2\"] == 0)].count()[\"id\"]\n",
    "TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = TP / (TP + FP)\n",
    "assert precision == precision_score(df[\"class\"], df[\">.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = TP / (TP + FN)\n",
    "assert recall == recall_score(df[\"class\"], df[\">.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "assert accuracy == accuracy_score(df[\"class\"], df[\">.2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "# assert f1 == f1_score(df[\"class\"], df[\">.2\"])\n",
    "assert (f1 - f1_score(df[\"class\"], df[\">.2\"])) ** 2 < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC-AUC\n",
    "\n",
    "Это все здорово, но две проблемы: \n",
    "1. Как выбрать порог для бинаризации.  $0.5$ - очень разумная мысль, если верить что предсказания модели являются вероятностями. Но обычно скоры модели - это **совсем не вероятности**, даже если это числа из $[0,1]$, суммирующиеся в единицу! Настоящие вероятности - это когда среди примеров со скором \"примерно 0.8\", примерно 80% действительно принадлежат к этому классу. Но в жизни такое случается редко, и применяются специальные техники калибровки вероятностей, о которых мы поговорим позднее.\n",
    "2. Описанные метрики никак не учитывают абсолютное значение скоров. Модель, которая оценивает вероятности в $50 \\pm \\varepsilon \\%$ получает столько же, сколько модель которая \"уверена\" в своих ответах.\n",
    "\n",
    "\n",
    "И тут на сцену выходит ROC-AUC. \n",
    "\n",
    "* ROC: Reciever Operating Characteristic - кривая соотношения TP/FP в зависимости от порога. Называется так, потому что впервые ее использовали американские военные во время WWII для оценки качества радаров\n",
    "* AUC: Area Under Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(df[\"class\"], df[\"score\"])\n",
    "roc_auc = roc_auc_score(df[\"class\"], df[\"score\"])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 4  # line width\n",
    "\n",
    "plt.plot(\n",
    "    fpr, tpr, color=\"darkorange\", lw=lw, label=\"ROC curve (area = %0.4f)\" % roc_auc\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC example\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in list(thresholds):\n",
    "    TP = df[(df[\"score\"] >= t) & (df[\"class\"] == 1)][\"id\"].count()\n",
    "    TN = df[(df[\"score\"] < t) & (df[\"class\"] == 0)][\"id\"].count()\n",
    "    FP = df[(df[\"score\"] >= t) & (df[\"class\"] == 0)][\"id\"].count()\n",
    "    FN = df[(df[\"score\"] < t) & (df[\"class\"] == 1)][\"id\"].count()\n",
    "    TP_rate = TP / (TP + FN)\n",
    "    FP_rate = FP / (TN + FP)\n",
    "    print(f\" Threshold: {t} TP Rate: {TP_rate:6f} FP Rate: {FP_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но вообще есть `plot_roc_curve` и вот такое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кстати, покажите что ROC-кривая монотонна для любых данных и моделей.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve\n",
    "\n",
    "Другая кривая, в которой изменяется соотношение Precision к Recall.\n",
    "\n",
    "Упражнение: обязательно ли эта кривая будет монотонной?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(df[\"class\"], df[\"score\"])\n",
    "\n",
    "precision, recall, thresholds\n",
    "\n",
    "pr_display = PrecisionRecallDisplay(precision=precision, recall=recall).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика, суммаризующая эту кривую, называется **Average Precision (AP)**. Кстати, это очень популярная метрика в задачах сегментации изображений и object detection.\n",
    "\n",
    "Формально, здесь вообще не будет тресхолда: поскольоку recall монотонен по тресхолду, можнго думать про эту кривую как про функцию Precision(recall). $$AP = \\int\\limits_0^1 Precision(r)dr = \\int\\limits_0^1 Precision(t)\\frac{\\partial Recall(t)}{\\partial t} dt$$\n",
    "\n",
    "\n",
    "**Упражнение.** \n",
    "* Какова worst-case сложность подсчета этой метрики наивно(т е варьируем тресхолд, считаем Precision, Recall и потом усредняем)?\n",
    "* А как посчитать за $O(N \\log N)$, где N-число точек данных?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# И ещё раз, без спешки\n",
    "\n",
    "Самое крупное кораблекрушение Европы в мирное время, в предполагаемых причинах которого до сих пор нет окончательной уверенности.\n",
    "\n",
    "```\n",
    "«Estonia» (ранее «Viking Sally», «Silja Star», «Wasa King») — \n",
    "эстонский паром судоходной компании «Estline», построенный \n",
    "в 1979 году в ФРГ на судоверфи «Meyer Werft» в Папенбурге. \n",
    "Затонул в Балтийском море в ночь с 27 на 28 сентября 1994 года, \n",
    "в результате крушения пропали без вести 757 человек и \n",
    "погибли 95 человек (всего 852) из 989 находившихся \n",
    "на борту пассажиров и членов экипажа. Это крупнейшее \n",
    "в Европе кораблекрушение в мирное время. \n",
    "```\n",
    "\n",
    "Датасет можно скачать [тут](https://www.kaggle.com/datasets/christianlillelund/passenger-list-for-the-estonia-ferry-disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_data = pd.read_csv(\"data/estonia-passenger-list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Country\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data = all_data.drop([\"Firstname\", \"Lastname\", \"PassengerId\"], axis=1)\n",
    "anonymized_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = anonymized_data[\"Survived\"]\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data[anonymized_data[\"Country\"] == \"Belarus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# нарисуем среднюю выживаемость и дисперсию\n",
    "sns.barplot(x=\"Country\", y=\"Survived\", data=anonymized_data)\n",
    "\n",
    "# Поворот  подписей\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зависит ли выживаемость от категории Пассажиры/Passengers; Команда/Сrew\n",
    "sns.barplot(x=\"Category\", y=\"Survived\", data=anonymized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А как распределён возраст?\n",
    "\n",
    "Можно посмотреть встроенными средствами pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(anonymized_data[\"Age\"]).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "#### Dummy-coding AKA One-Hot-Encoding\n",
    "\n",
    "Но чаще всё-таки удобнее кодировать средствами sklearn, увидим ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.get_dummies(anonymized_data[\"Country\"], prefix=\"c\")\n",
    "countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сами разобьём на обучающую и тестовую выборки -- до всех предобработок, это важно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(\n",
    "    anonymized_data.drop([\"Survived\"], axis=1),  # X\n",
    "    anonymized_data[\"Survived\"],  # y\n",
    "    test_size=0.3,  # доля от всех записей\n",
    "    random_state=1337,  # зерно\n",
    "    stratify=anonymized_data[\"Survived\"],  # а это что?\n",
    ")\n",
    "\n",
    "# print(data_train.shape, y_train.shape, data_test.shape, y_test.shape)\n",
    "\n",
    "# np.sum(y_train) / y_train.shape[0], np.sum(y_test) / y_test.shape[0]\n",
    "# nonames_data.shape, data_train.shape\n",
    "# y_train, y_test\n",
    "# y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse as sp\n",
    "\n",
    "\n",
    "# Не все классификаторы умеют обращаться с категориальными признаками.\n",
    "def prepare_features_for_logreg(data: pd.DataFrame, cat_encoder=None, real_scaler=None):\n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]\n",
    "    real_columns = [\"Age\"]\n",
    "\n",
    "    # categorical features\n",
    "    if cat_encoder is None:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "        ohe.fit(data[cat_columns])\n",
    "    else:\n",
    "        ohe = cat_encoder\n",
    "    X_cat = ohe.transform(data[cat_columns])\n",
    "    cat_fnames = ohe.get_feature_names_out(cat_columns)\n",
    "\n",
    "    # real-valued features\n",
    "    if real_scaler is None:\n",
    "        stsc = StandardScaler()\n",
    "        stsc.fit(data[real_columns])\n",
    "    else:\n",
    "        stsc = real_scaler\n",
    "    X_real = stsc.transform(data[real_columns])\n",
    "    feature_matrix = sp.hstack([X_cat, X_real])\n",
    "\n",
    "    return feature_matrix, list(cat_fnames) + real_columns, ohe, stsc\n",
    "\n",
    "\n",
    "X_train_sparse, fnames_sparse, encoder_sparse, scaler = prepare_features_for_logreg(\n",
    "    data_train\n",
    ")\n",
    "X_test_sparse, _, _, _ = prepare_features_for_logreg(data_test, encoder_sparse, scaler)\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape\n",
    "# X_train_sparse.todense()\n",
    "# X_test_sparse.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_linear = LogisticRegression(\n",
    "    C=0.99, class_weight=\"balanced\", solver=\"saga\", penalty=\"l1\"\n",
    ")\n",
    "clf_linear.fit(X_train_sparse, y_train)\n",
    "\n",
    "y_pred = clf_linear.predict(X_test_sparse)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "# clf_linear.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred, y_test\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_pred, y_test), annot=True, fmt=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хороший способ проверить, не ерунду ли мы сделали: внимание на **accuracy** и на метрики в разделе **macro-averaged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf_dummy = DummyClassifier(strategy=\"most_frequent\").fit(X_train_sparse, y_train)\n",
    "y_pred = clf_dummy.predict(X_test_sparse)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим признаки для логических классификаторов -- там обычно можно без разреженных признаков и нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from scipy import sparse as sp\n",
    "\n",
    "\n",
    "def prepare_features_for_logic(data: pd.DataFrame, cat_encoder=None):\n",
    "\n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]\n",
    "    real_columns = [\"Age\"]\n",
    "\n",
    "    # categorical features\n",
    "    if cat_encoder is None:\n",
    "        oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        oe.fit(data[cat_columns])\n",
    "    else:\n",
    "        oe = cat_encoder\n",
    "\n",
    "    X_cat = oe.transform(data[cat_columns])\n",
    "    mapped_cat_values = oe.categories_\n",
    "    cat_fnames = cat_columns\n",
    "\n",
    "    # real-valued features\n",
    "\n",
    "    # todo: вообще очень часто есть смысл отбросить из обучающей выборки примеры,\n",
    "    #       значения которых редки (например, выпадающие далеко \"за три сигмы\")\n",
    "\n",
    "    X_real = data[real_columns].values\n",
    "    feature_matrix = np.hstack([X_cat, X_real])  # note: `np` for dense Numpy matrices\n",
    "\n",
    "    return feature_matrix, list(cat_fnames) + real_columns, oe, mapped_cat_values\n",
    "\n",
    "\n",
    "X_train_dense, fnames_dense, encoder, mapped_cat_values = prepare_features_for_logic(\n",
    "    data_train\n",
    ")\n",
    "X_test_dense, _, _, _ = prepare_features_for_logic(data_test, encoder)\n",
    "\n",
    "\n",
    "X_train_dense.shape, X_test_dense.shape\n",
    "\n",
    "mapped_cat_values\n",
    "X_train_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, class_weight=\"balanced\").fit(\n",
    "    X_train_dense, y_train\n",
    ")\n",
    "y_pred = clf.predict(X_test_dense)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но пока мы даже не попытались настроить модели, результаты ничего не значат.\n",
    "\n",
    "Кстати, смотрите, как можно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(12, 8), dpi=80)\n",
    "\n",
    "\n",
    "plot_tree(\n",
    "    clf, feature_names=fnames_dense, class_names=[\"Dead\", \"Surv\"], proportion=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Познакомились с minimum minimorum, поперебираем\n",
    "попробуем получить результаты получше со случайным лесом\n",
    "\n",
    "Посмотрим на самые важные параметры.\n",
    "\n",
    "```\n",
    "class sklearn.ensemble.RandomForestClassifier(\n",
    "\n",
    "                        n_estimators=100,  \n",
    "                            Число деревьев.              \n",
    "                            \n",
    "                        criterion='gini', \n",
    "                            Критерий: индекс Джини либо энтропия, может зависеть от вида дерева\n",
    "                            \n",
    "                        max_depth=None, \n",
    "                            Самая, пожалуй, естественная регуляризация -- ограничение глубины дерева\n",
    "                            \n",
    "                        min_samples_split=2,\n",
    "                            Сколько должно попасть в вершину объектов, чтобы её можно было ветвить дальше\n",
    "                            \n",
    "                        min_samples_leaf=1, \n",
    "                            Очень мощный и важный регуляризатор! Сколько минимум объектов должно быть в листе\n",
    "                            \n",
    "                        max_features='auto', \n",
    "                            Среди какого числа признаков выбираем очередное ветвление\n",
    "                            \n",
    "                        max_leaf_nodes=None, \n",
    "                            Хороший регуляризатор -- ограничение на количество листьев; добавляются по убыванию\n",
    "                            снижения impurity.\n",
    "                            \n",
    "                        min_impurity_decrease=0.0, \n",
    "                            Порог по уменьшению impurity, который запрещает ветвить дерево дальше.\n",
    "                        \n",
    "                        bootstrap=True, \n",
    "                            Если False, обучаем каждое дерево на всём наборе данных. \n",
    "                            Если True, только на части, размер которой задан в max_samples.\n",
    "                            \n",
    "                        n_jobs=None, \n",
    "                            На сколько job-ов распараллелить.\n",
    "                            \n",
    "                        random_state=None, \n",
    "                            Ну, тут всё понятно: случайный seed, позволяющий воспроизводить результаты.\n",
    "                        \n",
    "                        verbose=0, \n",
    "                            Степень подробности протоколирования хода обучения и всего такого. Обычно 0,1,2.\n",
    "                            \n",
    "                        warm_start=False, \n",
    "                            Это такая возможность переиспользовать обученный ансамбль для последующих задач.\n",
    "                             \n",
    "                        max_samples=None\n",
    "                            Сколько максимум сэмплов брать из датасета для обучения очередного дерева, если bootstrap\n",
    "                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"min_samples_leaf\": [1, 2, 3, 5],\n",
    "    \"max_samples\": [0.3, None],\n",
    "    \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape, type(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение\n",
    "Зачем нужен KFold? Почему нельзя просто считать метрики на тесте?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=3, random_state=100, shuffle=True)\n",
    "\n",
    "print(y_train.values.mean())\n",
    "\n",
    "for array1, array2 in kfold.split(X_train_dense, y_train):\n",
    "    x_train_cv = X_train_dense[array1]\n",
    "    y_train_cv = np.array(y_train)[list(array1)]\n",
    "\n",
    "    x_test_cv = X_train_dense[array2]\n",
    "    y_test_cv = np.array(y_train)[list(array2)]\n",
    "\n",
    "    print(y_train_cv.mean(), y_test_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(RandomForestClassifier(n_jobs=-1, random_state=100), \n",
    "                       param_grid, scoring=score, verbose=1, cv=3)\n",
    "\n",
    "    clf.fit(X_train_dense, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "    \n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    \n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_dense, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_dense)\n",
    "    \n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Укладываем вообще всё в один пайплайн\n",
    "---\n",
    "Результаты вряд ли будут впечатляющими, но на этом примере посмотрим, как можно удобно запрограммировать перебор параметров не только классификации.\n",
    "\n",
    "Понизим размерность и применим KNN.\n",
    "\n",
    "Вариантов понижения размерности [много](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition). Возьмём самый стандартный и работающий с разреженными признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "decomposer = TruncatedSVD(n_components=6, random_state=10, n_iter=200)\n",
    "X_train_svd = decomposer.fit_transform(X_train_sparse)\n",
    "X_test_svd = decomposer.transform(X_test_sparse)\n",
    "\n",
    "X_train_svd.shape, X_test_svd.shape, X_train_sparse.shape, X_train_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"svd\", decomposer), (\"knn\", knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"svd__n_components\": [2, 4, 6, 10],\n",
    "        \"svd__n_iter\": [5, 100, 1000],\n",
    "        \"knn__n_neighbors\": [1, 2, 3, 4, 5],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn__metric\": [\"euclidean\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "scores = [\"f1_macro\"]  # , \"accuracy\"]\n",
    "\n",
    "for score in tqdm(scores):\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(pipe, param_grid, scoring=score, verbose=2, cv=3)\n",
    "    clf.fit(X_train_sparse, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_sparse)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 5. Паром.\n",
    "\n",
    "Получите accuracy > 0.88 на тестовом датасете. Можно пользоваться любым классификатором **из sklearn**. Ансамблями пользоваться можно.\n",
    "\n",
    "### XtreemeGradientBoosting и нейронные сети запрещены. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_data = pd.read_csv(\"data/estonia-passenger-list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_data = all_data.drop([\"Firstname\", \"Lastname\", \"PassengerId\"], axis=1)\n",
    "y = anonymized_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data_train, data_test, y_train, y_test = train_test_split(\n",
    "    anonymized_data.drop([\"Survived\"], axis=1),  # X\n",
    "    anonymized_data[\"Survived\"],  # y\n",
    "    test_size=0.3,  # доля от всех записей\n",
    "    random_state=1337,  # зерно\n",
    "    stratify=anonymized_data[\"Survived\"],  # а это что?\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((692, 4), (297, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse as sp\n",
    "\n",
    "\n",
    "def prepare_features_for_logic(data: pd.DataFrame, cat_encoder=None, real_scaler=None):\n",
    "\n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]\n",
    "    real_columns = [\"Age\"]\n",
    "\n",
    "    # categorical features\n",
    "    if cat_encoder is None:\n",
    "        oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        oe.fit(data[cat_columns])\n",
    "    else:\n",
    "        oe = cat_encoder\n",
    "\n",
    "    X_cat = oe.transform(data[cat_columns])\n",
    "    mapped_cat_values = oe.categories_\n",
    "    cat_fnames = cat_columns\n",
    "\n",
    "    # real-valued features\n",
    "\n",
    "    if real_scaler is None:\n",
    "        stsc = StandardScaler()\n",
    "        stsc.fit(data[real_columns])\n",
    "    else:\n",
    "        stsc = real_scaler\n",
    "    X_real = stsc.transform(data[real_columns])\n",
    "    feature_matrix = np.hstack([X_cat, X_real])[(np.abs(X_real) < 3).T[0], :] # note: `np` for dense Numpy matrices\n",
    "    \n",
    "    return feature_matrix, list(cat_fnames) + real_columns, oe, mapped_cat_values\n",
    "\n",
    "\n",
    "X_train_dense, fnames_dense, encoder, mapped_cat_values = prepare_features_for_logic(data_train)\n",
    "X_test_dense, _, _, _ = prepare_features_for_logic(data_test, encoder)\n",
    "\n",
    "X_train_dense.shape, X_test_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "decomposer = TruncatedSVD(n_components=1, random_state=10, n_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(steps=[(\"svd\", decomposer), (\"knn\", knn)])\n",
    "pipe_nosvd = Pipeline(steps=[(\"knn\", knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"svd__n_components\": [1, 2, 3],\n",
    "        \"svd__n_iter\": [5, 100, 1000],\n",
    "        \"knn__n_neighbors\": [1, 2, 3, 4, 5],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn__metric\": [\"euclidean\"],\n",
    "    }\n",
    "]\n",
    "\n",
    "param_grid_nosvd = [\n",
    "    {\n",
    "        \"knn__n_neighbors\": [1, 2, 3, 4, 5],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn__metric\": [\"euclidean\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "for score in tqdm(scores):\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(pipe, param_grid, scoring=score, verbose=2, cv=3)\n",
    "    clf.fit(X_train_dense, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_dense, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_dense)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "for score in tqdm(scores):\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(pipe_nosvd, param_grid_nosvd, scoring=score, verbose=2, cv=3)\n",
    "    clf.fit(X_train_dense, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_dense, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_dense)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nosvd_new = {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((692, 20), (297, 20))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import sparse as sp\n",
    "\n",
    "\n",
    "# Не все классификаторы умеют обращаться с категориальными признаками.\n",
    "def prepare_features_for_logreg(data: pd.DataFrame, cat_encoder=None, real_scaler=None):\n",
    "    cat_columns = [\"Country\", \"Sex\", \"Category\"]\n",
    "    real_columns = [\"Age\"]\n",
    "\n",
    "    # categorical features\n",
    "    if cat_encoder is None:\n",
    "        ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "        ohe.fit(data[cat_columns])\n",
    "    else:\n",
    "        ohe = cat_encoder\n",
    "    X_cat = ohe.transform(data[cat_columns])\n",
    "    cat_fnames = ohe.get_feature_names_out(cat_columns)\n",
    "\n",
    "    # real-valued features\n",
    "    if real_scaler is None:\n",
    "        stsc = StandardScaler()\n",
    "        stsc.fit(data[real_columns])\n",
    "    else:\n",
    "        stsc = real_scaler\n",
    "    X_real = stsc.transform(data[real_columns])\n",
    "    feature_matrix = sp.hstack([X_cat, X_real])\n",
    "\n",
    "    return feature_matrix, list(cat_fnames) + real_columns, ohe, stsc\n",
    "\n",
    "\n",
    "X_train_sparse, fnames_sparse, encoder_sparse, scaler = prepare_features_for_logreg(\n",
    "    data_train\n",
    ")\n",
    "X_test_sparse, _, _, _ = prepare_features_for_logreg(data_test, encoder_sparse, scaler)\n",
    "\n",
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.53      0.68       256\n",
      "           1       0.23      0.90      0.37        41\n",
      "\n",
      "    accuracy                           0.58       297\n",
      "   macro avg       0.60      0.71      0.53       297\n",
      "weighted avg       0.87      0.58      0.64       297\n",
      "\n",
      "0.5791245791245792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, class_weight=\"balanced\").fit(\n",
    "    X_train_dense, y_train\n",
    ")\n",
    "y_pred = clf.predict(X_test_dense)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        \"svd__n_components\": [1, 5, 10, 15],\n",
    "        \"svd__n_iter\": [5, 100, 1000],\n",
    "        \"knn__n_neighbors\": [1, 2, 3, 4, 5],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "        \"knn__metric\": [\"euclidean\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning for accuracy\n",
      "\n",
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.3s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=1, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=2, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=3, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=4, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=uniform, svd__n_components=15, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=1, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=5, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=10, svd__n_iter=1000; total time=   0.2s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=5; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=100; total time=   0.0s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:38<00:00, 38.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END knn__metric=euclidean, knn__n_neighbors=5, knn__weights=distance, svd__n_components=15, svd__n_iter=1000; total time=   0.1s\n",
      "Best params on dev set:\n",
      "{'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "Scores on development set:\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.792 (+/-0.015) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.783 (+/-0.015) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.789 (+/-0.019) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.793 (+/-0.005) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.785 (+/-0.003) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.790 (+/-0.012) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.786 (+/-0.016) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.788 (+/-0.006) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.785 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.757 (+/-0.040) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.792 (+/-0.015) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.783 (+/-0.015) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.789 (+/-0.019) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.793 (+/-0.005) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.785 (+/-0.003) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.790 (+/-0.012) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.786 (+/-0.016) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.788 (+/-0.006) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.785 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 1, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.845 (+/-0.022) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.845 (+/-0.022) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.845 (+/-0.022) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.837 (+/-0.024) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.835 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.834 (+/-0.017) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.838 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.838 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.840 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.835 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.838 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.840 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.801 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.801 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.801 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.805 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.798 (+/-0.016) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.803 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.808 (+/-0.021) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.801 (+/-0.025) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.806 (+/-0.021) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.802 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.805 (+/-0.030) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.802 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 2, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.819 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.821 (+/-0.038) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.819 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.806 (+/-0.021) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.812 (+/-0.019) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.811 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.814 (+/-0.015) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.815 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.816 (+/-0.012) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.815 (+/-0.016) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.821 (+/-0.017) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.818 (+/-0.018) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.804 (+/-0.042) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.804 (+/-0.042) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.804 (+/-0.042) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.801 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.802 (+/-0.008) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.799 (+/-0.009) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.802 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.805 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.803 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.803 (+/-0.020) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.808 (+/-0.026) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.806 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 3, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.845 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.847 (+/-0.017) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.845 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.837 (+/-0.021) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.837 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.832 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.840 (+/-0.008) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.838 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.838 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.844 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.844 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.844 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.814 (+/-0.039) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.815 (+/-0.043) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.814 (+/-0.039) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.803 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.799 (+/-0.007) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.801 (+/-0.006) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.802 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.798 (+/-0.008) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.801 (+/-0.006) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.806 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.806 (+/-0.028) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.805 (+/-0.030) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 4, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.834 (+/-0.018) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.834 (+/-0.018) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.834 (+/-0.018) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.827 (+/-0.033) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.828 (+/-0.037) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.824 (+/-0.030) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.824 (+/-0.035) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.821 (+/-0.033) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.821 (+/-0.034) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.829 (+/-0.009) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.827 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.829 (+/-0.009) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "0.815 (+/-0.031) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 5}\n",
      "0.815 (+/-0.031) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 100}\n",
      "0.815 (+/-0.031) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 1, 'svd__n_iter': 1000}\n",
      "0.814 (+/-0.012) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 5}\n",
      "0.811 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 100}\n",
      "0.809 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 5, 'svd__n_iter': 1000}\n",
      "0.805 (+/-0.011) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 5}\n",
      "0.806 (+/-0.010) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 100}\n",
      "0.803 (+/-0.014) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 10, 'svd__n_iter': 1000}\n",
      "0.814 (+/-0.012) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 5}\n",
      "0.811 (+/-0.022) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 100}\n",
      "0.809 (+/-0.018) for {'knn__metric': 'euclidean', 'knn__n_neighbors': 5, 'knn__weights': 'distance', 'svd__n_components': 15, 'svd__n_iter': 1000}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92       256\n",
      "           1       0.14      0.02      0.04        41\n",
      "\n",
      "    accuracy                           0.85       297\n",
      "   macro avg       0.50      0.50      0.48       297\n",
      "weighted avg       0.76      0.85      0.80       297\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "\n",
    "scores = [\"accuracy\"]\n",
    "\n",
    "for score in tqdm(scores):\n",
    "    print(\"# Tuning for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(pipe, param_grid, scoring=score, verbose=2, cv=3)\n",
    "    clf.fit(X_train_sparse, y_train)\n",
    "\n",
    "    print(\"Best params on dev set:\")\n",
    "    print(clf.best_params_)\n",
    "\n",
    "    print(\"Scores on development set:\")\n",
    "    means = clf.cv_results_[\"mean_test_score\"]\n",
    "    stds = clf.cv_results_[\"std_test_score\"]\n",
    "\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_[\"params\"]):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "    best_model = clf.best_estimator_\n",
    "    best_model.fit(X_train_sparse, y_train)\n",
    "\n",
    "    y_true, y_pred = y_test, best_model.predict(X_test_sparse)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача 6. Линрег\n",
    "\n",
    "Допустим, у вас есть два множества **A** и **B** точек на плоскости. Линейная регрессия на плоскости -- это \n",
    "просто прямая, ее можно представить как функцию $ y = ax + b$.\n",
    "\n",
    "Линейная регрессия, обученная на множестве **А**, имеет коэффициент **$a > 0$**. То же самое верно и для линейной регрессии, обученной на множестве **B**. Правда ли, что если обучить линейную регрессию на множестве $A \\cup B$, то у полученной прямой коэффициент **a** будет больше 0?\n",
    "\n",
    "Если да -  докажите, если нет - постройте контрпример."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
