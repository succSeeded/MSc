{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c72e1d-c241-4026-831d-f6dbe0d70718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cff8e6-6a18-45f8-a111-82967cb00e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from typing import Dict, Any\n",
    "from IPython.display import display\n",
    "from graphviz import Digraph\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier as SklearnRandomForestClassifier\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a116083-b160-4d44-a4e8-426622726165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: npt.ArrayLike) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / y.shape[0]\n",
    "    return -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Decision tree classifier, which can be trained, can predict class labels(miraculously) and display itself if used in a frontend environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tree: dict = None,\n",
    "        features: list = None,\n",
    "        n_leaves: int = None,\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "    ):\n",
    "        if not tree is None:\n",
    "            self._tree = deepcopy(tree)\n",
    "        else:\n",
    "            self._tree = {}\n",
    "        if not features is None:\n",
    "            self._features = deepcopy(features)\n",
    "        else:\n",
    "            self._features = []\n",
    "        self._n_leaves = 0 if max_depth is None else n_leaves\n",
    "        self._max_depth = -1 if max_depth is None else max_depth\n",
    "        self._min_samples_split = 2 if min_samples_split is None else min_samples_split\n",
    "        self._min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list = None,\n",
    "        cat_features: list = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "\n",
    "        if features is None:\n",
    "            self._features = [f\"x_{i}\" for i in range(X.shape[1])]\n",
    "        else:\n",
    "            self._features = features\n",
    "\n",
    "        if cat_features is None:\n",
    "            self._cat_features = []\n",
    "        else:\n",
    "            self._cat_features = cat_features\n",
    "\n",
    "        if isinstance(y[0], str):\n",
    "            self._prediction_dtype = \"object\"\n",
    "        else:\n",
    "            self._prediction_dtype = \"int64\"\n",
    "\n",
    "        self._tree = DecisionTreeClassifier._c45_algorithm(\n",
    "            X,\n",
    "            y,\n",
    "            self._features,\n",
    "            max_depth=self._max_depth,\n",
    "            min_samples_leaf=self._min_samples_leaf,\n",
    "            min_samples_split=self._min_samples_split,\n",
    "            cat_features=self._cat_features,\n",
    "        )\n",
    "        self._n_leaves = DecisionTreeClassifier._compute_subtree_leaves(self._tree)\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0],), dtype=self._prediction_dtype)\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self._tree\n",
    "            while \"children\" in node:\n",
    "                feature_id = self._features.index(node[\"feature\"])\n",
    "                if node[\"feature\"] in self._cat_features:\n",
    "                    node = node[\"children\"][X[i, feature_id]]\n",
    "                else:\n",
    "                    node = (\n",
    "                        node[\"children\"][f\"<={node['threshold']:0.2f}\"]\n",
    "                        if X[i, feature_id] <= node[\"threshold\"]\n",
    "                        else node[\"children\"][f\">{node['threshold']:0.2f}\"]\n",
    "                    )\n",
    "            ans[i] = node[\"majority_class\"]\n",
    "        return ans\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree construction\n",
    "    # =============================================================================\n",
    "\n",
    "    def _c45_algorithm(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        max_depth: int,\n",
    "        min_samples_split: int,\n",
    "        min_samples_leaf: int,\n",
    "        cat_features: list,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Recursively build an C4.5 decision tree.\"\"\"\n",
    "\n",
    "        majority_class = DecisionTreeClassifier._majority_class(y)\n",
    "        error = np.sum(y != majority_class)\n",
    "\n",
    "        # Base case: all samples same class\n",
    "        if len(set(y)) == 1 or X.shape[1] < 1:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Node contains not enough elements to split\n",
    "        if len(y) < min_samples_split:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Max tree depth is reached\n",
    "        if max_depth == 0:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        best_id, best_feature, threshold = DecisionTreeClassifier._select_best_feature(\n",
    "            X, y, features, cat_features\n",
    "        )\n",
    "        feature_values = np.unique(X[:, best_id])\n",
    "        new_features = features.copy()\n",
    "        new_features.remove(best_feature)\n",
    "\n",
    "        tree = {\n",
    "            \"feature\": best_feature,\n",
    "            \"majority_class\": majority_class,\n",
    "            \"children\": {},\n",
    "            \"error\": error,\n",
    "        }\n",
    "\n",
    "        if best_feature in cat_features:\n",
    "            for value in feature_values:\n",
    "                mask = X[:, best_id] == value\n",
    "                X_sub, y_sub = np.delete(X, best_id, axis=1)[mask], y[mask]\n",
    "                if len(y_sub) < min_samples_leaf:\n",
    "                    child_majority_class = DecisionTreeClassifier._majority_class(y_sub)\n",
    "                    tree[\"children\"][value] = {\n",
    "                        \"majority_class\": child_majority_class,\n",
    "                        \"error\": np.sum(y_sub != child_majority_class),\n",
    "                    }\n",
    "                else:\n",
    "                    tree[\"children\"][value] = DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                    )\n",
    "        else:\n",
    "            mask = X[:, best_id] <= threshold\n",
    "            not_mask = np.logical_not(mask)\n",
    "            if (\n",
    "                np.sum(mask) >= min_samples_leaf\n",
    "                and (mask.shape[0] - np.sum(mask)) >= min_samples_leaf\n",
    "            ):\n",
    "                tree[\"threshold\"] = threshold\n",
    "                X_sub, y_sub = np.delete(X, best_id, axis=1)[mask], y[mask]\n",
    "                tree[\"children\"][f\"<={threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                X_sub, y_sub = np.delete(X, best_id, axis=1)[not_mask], y[not_mask]\n",
    "                tree[\"children\"][f\">{threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _majority_class(y: npt.ArrayLike):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def _gain_function(\n",
    "        X: npt.ArrayLike, y: npt.ArrayLike, feature_idx: int, cat_feature: bool\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate gain for a given feature.\"\"\"\n",
    "\n",
    "        if cat_feature is None or cat_feature == False:\n",
    "\n",
    "            def target_fn(theta: float):\n",
    "                mask_left = X[:, feature_idx] <= theta\n",
    "                mask_right = X[:, feature_idx] > theta\n",
    "                return mask_left.shape[0] / y.shape[0] * entropy(\n",
    "                    y[mask_left]\n",
    "                ) + mask_right.shape[0] / y.shape[0] * entropy(y[mask_right])\n",
    "\n",
    "            minimize_result = minimize(target_fn, x0=np.mean(X[:, feature_idx]))\n",
    "            ans = {\"value\": minimize_result.fun, \"threshold\": minimize_result.x[0]}\n",
    "        else:\n",
    "            values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "            probs = counts / y.shape[0]\n",
    "            entropies = np.array(\n",
    "                list(\n",
    "                    map(\n",
    "                        lambda x: entropy(y[X[:, feature_idx] == x]),\n",
    "                        values,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            ans = {\"value\": np.sum(probs * entropies), \"threshold\": None}\n",
    "\n",
    "        return ans\n",
    "\n",
    "    def _select_best_feature(\n",
    "        X: npt.ArrayLike, y: npt.ArrayLike, features: list, cat_features: list\n",
    "    ) -> list:\n",
    "        \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "        gains = [\n",
    "            DecisionTreeClassifier._gain_function(X, y, i, feature in cat_features)\n",
    "            for i, feature in enumerate(features)\n",
    "        ]\n",
    "        best_idx = np.argmin(gain[\"value\"] for gain in gains)\n",
    "        return [best_idx, features[best_idx], gains[best_idx][\"threshold\"]]\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree pruning\n",
    "    # =============================================================================\n",
    "\n",
    "    def get_pruned_tree(self, alpha: float) -> Dict:\n",
    "        \"\"\"Prune the tree using cost-complexity pruning with parameter `alpha`.\"\"\"\n",
    "        ans = DecisionTreeClassifier(\n",
    "            tree=self._tree,\n",
    "            features=self._features,\n",
    "            n_leaves=self._n_leaves,\n",
    "            max_depth=self._max_depth,\n",
    "            min_samples_leaf=self._min_samples_leaf,\n",
    "            min_samples_split=self._min_samples_split,\n",
    "        )\n",
    "        ans._cat_features = deepcopy(self._cat_features)\n",
    "        ans._prediction_dtype = deepcopy(self._prediction_dtype)\n",
    "        return ans\n",
    "\n",
    "    def prune_tree(self, alpha: float) -> None:\n",
    "        \"\"\"Prune the underlying decision tree using cost-complexity pruning with predefines `alpha`.\"\"\"\n",
    "        self._tree = DecisionTreeClassifier._cost_comprexity_pruning(\n",
    "            self._tree, alpha, inplace=True\n",
    "        )\n",
    "\n",
    "    def _compute_subtree_error(tree: Dict) -> int:\n",
    "        \"\"\"Calculate the total misclassification error of a (sub)tree.\"\"\"\n",
    "\n",
    "        if not \"children\" in tree:\n",
    "            return tree[\"error\"]\n",
    "\n",
    "        total_error = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_error += DecisionTreeClassifier._compute_subtree_error(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_error\n",
    "\n",
    "    def _compute_subtree_leaves(tree: Dict) -> int:\n",
    "        \"\"\"Count the number of leaf nodes in a (sub)tree.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return 1\n",
    "\n",
    "        total_leaves = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_leaves += DecisionTreeClassifier._compute_subtree_leaves(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_leaves\n",
    "\n",
    "    def _collect_pruning_candidates(tree: Dict, candidates: list) -> None:\n",
    "        \"\"\"Collect non-leaf nodes with their effective alpha values.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return candidates\n",
    "\n",
    "        subtree_error = DecisionTreeClassifier._compute_subtree_error(tree)\n",
    "        complexity_error = DecisionTreeClassifier._compute_subtree_leaves(tree)\n",
    "        R = tree[\"error\"]\n",
    "        effective_alpha = (R - subtree_error) / complexity_error\n",
    "\n",
    "        for child in tree[\"children\"]:\n",
    "            DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree[\"children\"][child], candidates\n",
    "            )\n",
    "\n",
    "        candidates.append((tree, effective_alpha))\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    def _cost_comprexity_pruning(self, alpha: float, inplace: bool = None) -> dict:\n",
    "        if inplace is True:\n",
    "            tree_to_prune = deepcopy(self._tree)\n",
    "        else:\n",
    "            tree_to_prune = self._tree\n",
    "        while True:\n",
    "            candidates = []\n",
    "            candidates = DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree_to_prune, candidates\n",
    "            )\n",
    "            candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "            weakest_subtree, weakest_alpha = candidates[0]\n",
    "\n",
    "            if weakest_alpha > alpha:\n",
    "                break\n",
    "\n",
    "            weakest_subtree[\"children\"] = {}\n",
    "            weakest_subtree.pop(\"feature\")\n",
    "\n",
    "        return tree_to_prune\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree visualization\n",
    "    # =============================================================================\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = DecisionTreeClassifier._visualize_tree(self._tree, self._features)\n",
    "        display(dot)\n",
    "\n",
    "    def _visualize_tree(\n",
    "        tree: Dict[str, Any],\n",
    "        feature_names: list,\n",
    "        dot: Digraph = None,\n",
    "        parent: str = None,\n",
    "        edge_label: str = None,\n",
    "    ) -> Digraph:\n",
    "        \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "        if dot is None:\n",
    "            dot = Digraph(comment=\"Decision Tree\")\n",
    "\n",
    "        # Create a unique node ID\n",
    "        node_id = str(id(tree))\n",
    "\n",
    "        # Add the current node\n",
    "        if not \"children\" in tree:\n",
    "            node_label = f\"Class: {tree['majority_class']}\\nError: {tree['error']}\"\n",
    "        else:\n",
    "            node_label = f\"Feature: {tree['feature']}\\nError: {tree['error']}\"\n",
    "        dot.node(node_id, node_label)\n",
    "\n",
    "        # Connect to parent node if exists\n",
    "        if parent is not None:\n",
    "            dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "        # Recursively add children\n",
    "        if \"children\" in tree:\n",
    "            for value, child in tree[\"children\"].items():\n",
    "                DecisionTreeClassifier._visualize_tree(\n",
    "                    child, feature_names, dot, node_id, str(value)\n",
    "                )\n",
    "\n",
    "        return dot\n",
    "\n",
    "    # =============================================================================\n",
    "    # Utility functions\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72e5c56-b552-495e-b016-b5a7f09c785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "    values, counts = np.unique(X, return_counts=True)\n",
    "    return values[np.argmax(counts)]\n",
    "\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators: int = None, max_depth=None) -> None:\n",
    "        if n_estimators is None:\n",
    "            self._n_estimators = 100\n",
    "        else:\n",
    "            self._n_estimators = n_estimators\n",
    "        self._max_depth = max_depth\n",
    "        self._classifiers = []\n",
    "\n",
    "    def fit(self, X: npt.ArrayLike, y: npt.ArrayLike) -> None:\n",
    "        n_data_points = X.shape[0]\n",
    "\n",
    "        for i in range(self._n_estimators):\n",
    "            indices = np.random.choice(n_data_points, n_data_points // 3, replace=True)\n",
    "            X_ = X[indices]\n",
    "            y_ = y[indices]\n",
    "            clf = DecisionTreeClassifier(max_depth=self._max_depth)\n",
    "            clf.fit(X_, y_)\n",
    "            self._classifiers.append(clf)\n",
    "\n",
    "    def predict(self, X) -> np.array:\n",
    "        predictions = np.array([clf.predict(X) for clf in self._classifiers])\n",
    "        result = np.apply_along_axis(lambda x: mode(x), axis=0, arr=predictions)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e4030e-49d6-476d-980c-ad532d73a465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy(mine): 1.00\n",
      "Accuracy(sklearn): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Load sample data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train random forests\n",
    "rf_sklearn = SklearnRandomForestClassifier(criterion=\"entropy\")\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "y_pred_sklearn = rf_sklearn.predict(X_test)\n",
    "print(\n",
    "    f\"Accuracy(mine): {accuracy_score(y_test, y_pred):.2f}\\nAccuracy(sklearn): {accuracy_score(y_test, y_pred_sklearn):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0149153-722c-4619-a5ec-baf38e83e264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "accuracy_score(y_test, dt.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
