{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262a4d14-8832-4fa7-b2d2-ed51c49da6b5",
   "metadata": {},
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c72e1d-c241-4026-831d-f6dbe0d70718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cff8e6-6a18-45f8-a111-82967cb00e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "from typing import Any, Callable, Dict, List, Union\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, TargetEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor as SDecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor as SGradientBoostingRegressor\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b1e9e0-0cdc-487d-9d82-6fff2e98d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(x1: npt.ArrayLike, x2: npt.ArrayLike) -> float:\n",
    "    return np.sqrt(np.power(x1 - x2, 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c5989-8200-4804-9774-43ed2312ea0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Дерево регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e8b2fc-9cf9-4d2f-b1b3-2ff103091c62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Decision tree regressor, something to build gradient boosting algorithms off of.\n",
    "    \"\"\"\n",
    "\n",
    "    SPLIT_CRITERIA = {\n",
    "        \"mae\": lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred)),\n",
    "        \"mse\": lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "        criterion: str = None,\n",
    "    ) -> None:\n",
    "        if criterion is None:\n",
    "            self.criterion = DecisionTreeRegressor.SPLIT_CRITERIA[\"mse\"]\n",
    "        else:\n",
    "            self.criterion = DecisionTreeRegressor.SPLIT_CRITERIA[criterion.lower()]\n",
    "        self.max_depth = -1 if max_depth is None else max_depth\n",
    "        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n",
    "        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list = None,\n",
    "        cat_features: list = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "\n",
    "        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "            X = X.to_numpy()\n",
    "            y = y.to_numpy()\n",
    "\n",
    "        if features is None:\n",
    "            self.features_ = [f\"x_{i}\" for i in range(X.shape[1])]\n",
    "        else:\n",
    "            self.features_ = features\n",
    "\n",
    "        if cat_features is None:\n",
    "            self.cat_features_ = []\n",
    "        else:\n",
    "            self.cat_features_ = cat_features\n",
    "\n",
    "        self.tree_ = DecisionTreeRegressor.build_tree_(\n",
    "            X,\n",
    "            y,\n",
    "            self.features_,\n",
    "            depth=0,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            cat_features=self.cat_features_,\n",
    "            criterion=self.criterion,\n",
    "        )\n",
    "\n",
    "        self.n_leaves_ = DecisionTreeRegressor.count_leaves_(self.tree_)\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0],))\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self.tree_\n",
    "            while not node[\"is_leaf\"]:\n",
    "                # Get the index a the feature on which the split was performed\n",
    "                feature_idx = self.features_.index(node[\"feature\"])\n",
    "\n",
    "                if node[\"feature\"] in self.cat_features_:\n",
    "                    # If the category has not appeared in training set, the tree\n",
    "                    # traversal is terminated and the current node value is used\n",
    "                    if node[\"children\"].get(X[i, feature_idx], False):\n",
    "                        node = node[\"children\"].get(X[i, feature_idx], False)\n",
    "                    else:\n",
    "                        break\n",
    "                else:\n",
    "                    if X[i, feature_idx] <= node[\"threshold\"]:\n",
    "                        node = node[\"children\"][\"lower\"]\n",
    "                    else:\n",
    "                        node = node[\"children\"][\"upper\"]\n",
    "\n",
    "            ans[i] = node[\"value\"]\n",
    "        return ans\n",
    "\n",
    "    def get_n_leaves(self):\n",
    "        return self.n_leaves_\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree construction\n",
    "    # =============================================================================\n",
    "\n",
    "    def build_tree_(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        depth: int,\n",
    "        max_depth: int,\n",
    "        min_samples_split: int,\n",
    "        min_samples_leaf: int,\n",
    "        cat_features: list,\n",
    "        criterion: Callable,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Recursively build a regression tree.\"\"\"\n",
    "\n",
    "        default_value = np.mean(y)\n",
    "\n",
    "        # Terminate if there are no more features to split on\n",
    "        if X.shape[1] == 0 or len(features) == 0:\n",
    "            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n",
    "\n",
    "        # Terminate if all the targets are duplicates of eachother\n",
    "        if np.unique(y).shape[0] == 1:\n",
    "            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n",
    "\n",
    "        # Terminate if all the datapoints are duplicates of eachother\n",
    "        if np.unique(X, axis=0).shape[0] == 1:\n",
    "            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n",
    "\n",
    "        # Terminate if node does not contain enough elements to split\n",
    "        if y.shape[0] < min_samples_split:\n",
    "            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n",
    "\n",
    "        # Terminate if max tree depth is reached\n",
    "        if depth == max_depth:\n",
    "            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n",
    "\n",
    "        best_id, best_feature, threshold = DecisionTreeRegressor.select_best_feature_(\n",
    "            X, y, features, cat_features, criterion\n",
    "        )\n",
    "        new_features = [feature for feature in features if feature != best_feature]\n",
    "\n",
    "        tree = {\n",
    "            \"depth\": depth,\n",
    "            \"feature\": best_feature,\n",
    "            \"is_leaf\": False,\n",
    "            \"value\": default_value,\n",
    "            \"children\": {},\n",
    "        }\n",
    "\n",
    "        if best_feature in cat_features:\n",
    "            categories = np.unique(X[:, best_id])\n",
    "\n",
    "            for category in categories:\n",
    "\n",
    "                mask = X[:, best_id] == category\n",
    "\n",
    "                X_sub, y_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[mask],\n",
    "                    y[mask],\n",
    "                )\n",
    "\n",
    "                # If there is not enough samples to make a leaf the split is aborted and\n",
    "                # the node is considered a leaf\n",
    "                if len(y_sub) < min_samples_leaf:\n",
    "                    tree[\"is_leaf\"] = True\n",
    "                    tree[\"children\"] = {}\n",
    "                    return tree\n",
    "\n",
    "                tree[\"children\"][category] = DecisionTreeRegressor.build_tree_(\n",
    "                    X_sub,\n",
    "                    y_sub,\n",
    "                    new_features,\n",
    "                    depth=depth + 1,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    cat_features=cat_features,\n",
    "                    criterion=criterion,\n",
    "                )\n",
    "        else:\n",
    "            mask_left = X[:, best_id] <= threshold\n",
    "            mask_right = X[:, best_id] > threshold\n",
    "\n",
    "            tree[\"threshold\"] = threshold\n",
    "\n",
    "            # If there is not enough samples to make a leaf the split is aborted and\n",
    "            # the node is considered a leaf\n",
    "            if (\n",
    "                mask_left.sum() < min_samples_leaf\n",
    "                or mask_right.sum() < min_samples_leaf\n",
    "            ):\n",
    "                tree[\"is_leaf\"] = True\n",
    "                return tree\n",
    "\n",
    "            X_sub = np.delete(X, best_id, axis=1)\n",
    "\n",
    "            tree[\"children\"][\"lower\"] = DecisionTreeRegressor.build_tree_(\n",
    "                X_sub[mask_left],\n",
    "                y[mask_left],\n",
    "                new_features,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                min_samples_split=min_samples_split,\n",
    "                cat_features=cat_features,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "\n",
    "            tree[\"children\"][\"upper\"] = DecisionTreeRegressor.build_tree_(\n",
    "                X_sub[mask_right],\n",
    "                y[mask_right],\n",
    "                new_features,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                min_samples_split=min_samples_split,\n",
    "                cat_features=cat_features,\n",
    "                criterion=criterion,\n",
    "            )\n",
    "        return tree\n",
    "\n",
    "    def feature_score_(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        feature_idx: int,\n",
    "        is_cat_feature: bool,\n",
    "        criterion: Callable,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate score for a given feature.\"\"\"\n",
    "\n",
    "        if is_cat_feature is None or is_cat_feature == False:\n",
    "\n",
    "            uniques = np.unique(X[:, feature_idx])\n",
    "\n",
    "            # Splits are not done if all the feature values are the same\n",
    "            if uniques.shape[0] == 1:\n",
    "                return {\"value\": np.inf, \"threshold\": None}\n",
    "\n",
    "            thresholds = [\n",
    "                0.5 * (curr + prev) for prev, curr in zip(uniques, uniques[1:])\n",
    "            ]\n",
    "            split_scores = []\n",
    "\n",
    "            for theta in thresholds:\n",
    "\n",
    "                mask_left = X[:, feature_idx] <= theta\n",
    "                mask_right = X[:, feature_idx] > theta\n",
    "\n",
    "                # Elements lower and higher than threshold are compared to their respective means\n",
    "                y_pred = np.where(\n",
    "                    mask_left, np.mean(y[mask_left]), np.mean(y[mask_right])\n",
    "                )\n",
    "                split_scores += [criterion(y_pred, y)]\n",
    "\n",
    "            best_split_id = np.argmin(split_scores)\n",
    "            best_threshold = thresholds[best_split_id]\n",
    "            return {\"value\": split_scores[best_split_id], \"threshold\": best_threshold}\n",
    "\n",
    "        else:\n",
    "            categories = np.unique(X[:, feature_idx])\n",
    "            y_pred = np.zeros_like(y)\n",
    "\n",
    "            for category in categories:\n",
    "\n",
    "                mask = X[:, feature_idx] == category\n",
    "                # Elements in each(surviving!) category are compared to their respective means\n",
    "                y_pred = np.where(mask, np.mean(y[mask]), y_pred)\n",
    "\n",
    "            score = criterion(y_pred, y)\n",
    "            return {\"value\": score, \"threshold\": None}\n",
    "\n",
    "    def select_best_feature_(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        cat_features: list,\n",
    "        criterion: Callable,\n",
    "    ) -> list:\n",
    "        \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "        scores = [\n",
    "            DecisionTreeRegressor.feature_score_(\n",
    "                X, y, i, feature in cat_features, criterion\n",
    "            )\n",
    "            for i, feature in enumerate(features)\n",
    "        ]\n",
    "\n",
    "        best_idx = np.argmin([score[\"value\"] for score in scores])\n",
    "        return [best_idx, features[best_idx], scores[best_idx][\"threshold\"]]\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree pruning\n",
    "    # =============================================================================\n",
    "\n",
    "    def count_leaves_(tree: Dict) -> int:\n",
    "        \"\"\"Count the number of leaf nodes in a (sub)tree.\"\"\"\n",
    "        if tree[\"is_leaf\"]:\n",
    "            return 1\n",
    "\n",
    "        total_leaves = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_leaves += DecisionTreeRegressor.count_leaves_(tree[\"children\"][child])\n",
    "\n",
    "        return total_leaves\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree visualization\n",
    "    # =============================================================================\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = DecisionTreeRegressor.visualize_tree_(self.tree_, self.features_)\n",
    "        display(dot)\n",
    "\n",
    "    def visualize_tree_(\n",
    "        tree: Dict[str, Any],\n",
    "        feature_names: list,\n",
    "        dot: Digraph = None,\n",
    "        parent: str = None,\n",
    "        edge_label: str = None,\n",
    "    ) -> Digraph:\n",
    "        \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "        if dot is None:\n",
    "            dot = Digraph(comment=\"Decision Tree Regessor\")\n",
    "\n",
    "        # Create a unique node ID\n",
    "        node_id = str(id(tree))\n",
    "\n",
    "        # Add the current node\n",
    "        if tree[\"is_leaf\"]:\n",
    "            node_label = f\"Value: {tree['value']:0.2f}\"\n",
    "        else:\n",
    "            node_label = f\"Feature: {tree['feature']}\"\n",
    "            if tree.get(\"threshold\", False):\n",
    "                node_label += f\"<={tree['threshold']:0.2f}\"\n",
    "        dot.node(node_id, node_label)\n",
    "\n",
    "        # Connect to parent node if exists\n",
    "        if parent is not None:\n",
    "            dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "        # Recursively add children\n",
    "        if \"children\" in tree:\n",
    "            for value, child in tree[\"children\"].items():\n",
    "                DecisionTreeRegressor.visualize_tree_(\n",
    "                    child,\n",
    "                    feature_names,\n",
    "                    dot,\n",
    "                    node_id,\n",
    "                    str(value) if not isinstance(value, str) else value,\n",
    "                )\n",
    "\n",
    "        return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d67f444-6119-4106-8eb4-ebd9e0779e12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e1703a-74ff-443f-b25c-0e057742fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = None,\n",
    "        max_depth: int = None,\n",
    "        n_estimators: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "        criterion: str = None,\n",
    "        tol: float = None,\n",
    "    ) -> None:\n",
    "        self.criterion = criterion\n",
    "        self.tol = 1e-4 if tol is None else tol\n",
    "        self.max_depth = 4 if max_depth is None else max_depth\n",
    "        self.n_estimators = 100 if n_estimators is None else n_estimators\n",
    "        self.learning_rate = 0.1 if learning_rate is None else learning_rate\n",
    "        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n",
    "        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the regressor to data. Note that it requires the categorical features\n",
    "        to be already encoded since otherwise the won't be processed.\n",
    "        This is done in order to reduce the tree sizes and increase learning speeds.\"\"\"\n",
    "        self.trees_ = [\n",
    "            DecisionTreeRegressor(\n",
    "                max_depth=0,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                criterion=self.criterion,\n",
    "            )\n",
    "        ]\n",
    "        self.trees_[0].fit(X, y)\n",
    "        curr_guess = self.trees_[0].predict(X)\n",
    "        prev_guess = np.copy(y)\n",
    "        residue = np.sqrt(np.power((prev_guess - curr_guess), 2).sum())\n",
    "        num_estimators = 1\n",
    "\n",
    "        while num_estimators < self.n_estimators:\n",
    "            if residue < self.tol:\n",
    "                break\n",
    "\n",
    "            self.trees_ += [\n",
    "                DecisionTreeRegressor(\n",
    "                    max_depth=self.max_depth,\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_samples_leaf=self.min_samples_leaf,\n",
    "                    criterion=self.criterion,\n",
    "                )\n",
    "            ]\n",
    "            self.trees_[-1].fit(X, y - curr_guess, cat_features=cat_features)\n",
    "            prev_guess = np.copy(curr_guess)\n",
    "            curr_guess += self.learning_rate * self.trees_[-1].predict(X)\n",
    "            residue = np.sqrt(np.power((prev_guess - curr_guess), 2).sum())\n",
    "            num_estimators += 1\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        ans = self.trees_[0].predict(X)\n",
    "        for tree in self.trees_[1:]:\n",
    "            ans += tree.predict(X) * self.learning_rate\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8721e1-5572-4b8e-a2a7-d7fd9cf92641",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Сравнение различных методов на датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287ee09-f522-4f52-b9ba-6304a6a654c3",
   "metadata": {},
   "source": [
    "Рассмотрим признаки, имеющиеся в датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca467b-dfd1-4c57-ac89-20c3aa2c85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"train.csv\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175341c1-d95e-4fa0-ab6b-a4ae30b42807",
   "metadata": {},
   "source": [
    "Удалим признаки с дублирующие информацию или не несущие практической пользы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba66c-97a9-4ace-ba51-831489d8a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\n",
    "    \"type\",\n",
    "    \"track_href\",\n",
    "    \"track_href\",\n",
    "    \"uri\",\n",
    "    \"track_album_name\",\n",
    "    \"analysis_url\",\n",
    "    \"track_id\",\n",
    "    \"track_name\",\n",
    "    \"track_artist\",\n",
    "    \"track_album_id\",\n",
    "    \"track_album_release_date\",\n",
    "    \"id\",\n",
    "    \"playlist_id\",\n",
    "    \"playlist_name\",\n",
    "]\n",
    "\n",
    "df = df.drop(\n",
    "    drop_list + [\"Unnamed: 0\"],\n",
    "    axis=1,\n",
    ")\n",
    "cat_features = [\n",
    "    \"playlist_genre\",\n",
    "    \"time_signature\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c886c2-0874-40d3-8209-ceb151cef6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feace29-3662-49b1-97b0-1bacb20419d3",
   "metadata": {},
   "source": [
    "Очистим датасет от пустых и повторяющихся строк:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffc470-f3e2-488f-b7d1-6e047d6f34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Размер датасета до чистки: \", df.shape)\n",
    "df = df.dropna()\n",
    "print(\"Размер датасета после удаления пустых строк: \", df.shape)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Размер датасета после удаления повторяющихся строк: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af93e2-1d6e-4dbe-8858-640b2ed61ea8",
   "metadata": {},
   "source": [
    "Из-за ограничений имплементации дерева регрессии, созданного в рамках данной работы, категориальные признаки не получится использовать напрямую --- их придется кодировать ординальным кодированием (меняем `str` на `int`, при этом `int`'ы восприниматся как названия категорий, не их порядок/номер). Также полезным будет убрать значения категориальных признаков, которые встречаются крайне редко($\\leq 2$ раз на всем сете): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20c1ad-30bd-427d-a578-df9414ba14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = (df.drop(\"popularity\", axis=1), df[\"popularity\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.8, random_state=12389014\n",
    ")\n",
    "\n",
    "# Change unpopular values to \"other\"\n",
    "for feature in cat_features:\n",
    "    for val, count in X_train[feature].value_counts().items():\n",
    "        if count <= 2:\n",
    "            X_train.loc[X_train[feature] == val, feature] = \"other\"\n",
    "            X_test.loc[X_test[feature] == val, feature] = \"other\"\n",
    "\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "for feature in cat_features:\n",
    "    encoder.fit(X_train[[feature]])\n",
    "    X_train[[feature]] = encoder.transform(X_train[[feature]])\n",
    "    X_test[[feature]] = encoder.transform(X_test[[feature]])\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d68691-de6e-47cd-9e7d-2fb85d69c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d8aa2-736d-4adb-bbdd-1fd82f316308",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor(\n",
    "    max_depth=200, min_samples_leaf=3, min_samples_split=10\n",
    ")\n",
    "sregressor = SDecisionTreeRegressor(\n",
    "    max_depth=200, min_samples_leaf=3, min_samples_split=10\n",
    ")\n",
    "\n",
    "regressor.fit(\n",
    "    X_train.to_numpy(),\n",
    "    y_train.to_numpy().flatten(),\n",
    "    features=list(X_train.columns),\n",
    "    cat_features=cat_features,\n",
    ")\n",
    "\n",
    "sregressor.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"RMSE(dummy regressor): {root_mean_squared_error(y_test, np.ones_like(y_test)*y_train.mean()):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(DecisionTreeRegressor): {root_mean_squared_error(y_test, regressor.predict(X_test.to_numpy())):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(Sklearn DecisionTreeRegressor): {root_mean_squared_error(y_test, sregressor.predict(X_test)):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44eb749-28b4-4d50-afb8-e3979771923a",
   "metadata": {},
   "source": [
    "### Поиск лучших параметров для регрессионного дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1280a7f-cd39-4eb4-a691-c3934eaf66f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for {'max_depth': 200, 'min_samples_split': 2, 'min_samples_leaf': 1}:\n",
      "\t[15.14138396 14.56156765]\n",
      "Mean score: 14.8515\tstd: 0.2899\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 6, 'min_samples_leaf': 5}:\n",
      "\t[19.9463916 19.8297314]\n",
      "Mean score: 19.8881\tstd: 0.0583\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 10, 'min_samples_leaf': 1}:\n",
      "\t[14.32201009 12.92747257]\n",
      "Mean score: 13.6247\tstd: 0.6973\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 10, 'min_samples_leaf': 2}:\n",
      "\t[12.88444371 13.16525306]\n",
      "Mean score: 13.0248\tstd: 0.1404\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 10, 'min_samples_leaf': 3}:\n",
      "\t[12.7806464  13.08928303]\n",
      "Mean score: 12.9350\tstd: 0.1543\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 3}:\n",
      "\t[13.01920023 12.39410481]\n",
      "Mean score: 12.7067\tstd: 0.3125\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 3}:\n",
      "\t[13.76449921 12.53628452]\n",
      "Mean score: 13.1504\tstd: 0.6141\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}:\n",
      "\t[12.82172881 11.89175526]\n",
      "Mean score: 12.3567\tstd: 0.4650\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 10, 'min_samples_leaf': 4}:\n",
      "\t[12.54074677 19.87102068]\n",
      "Mean score: 16.2059\tstd: 3.6651\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 12, 'min_samples_leaf': 5}:\n",
      "\t[19.72381629 20.05421809]\n",
      "Mean score: 19.8890\tstd: 0.1652\n",
      "\n",
      "\n",
      "Scores for {'max_depth': 200, 'min_samples_split': 15, 'min_samples_leaf': 5}:\n",
      "\t[19.76739728 20.000445  ]\n",
      "Mean score: 19.8839\tstd: 0.1165\n",
      "\n",
      "\n",
      "Best parameter set for a RegressionTree: {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"train.csv\"))\n",
    "\n",
    "drop_list = [\n",
    "    \"type\",\n",
    "    \"track_href\",\n",
    "    \"track_href\",\n",
    "    \"uri\",\n",
    "    \"track_album_name\",\n",
    "    \"analysis_url\",\n",
    "    \"track_id\",\n",
    "    \"track_name\",\n",
    "    \"track_artist\",\n",
    "    \"track_album_id\",\n",
    "    \"track_album_release_date\",\n",
    "    \"id\",\n",
    "    \"playlist_id\",\n",
    "    \"playlist_name\",\n",
    "]\n",
    "\n",
    "df = df.drop(\n",
    "    drop_list + [\"Unnamed: 0\"],\n",
    "    axis=1,\n",
    ")\n",
    "cat_features = [\n",
    "    \"playlist_genre\",\n",
    "    \"time_signature\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "]\n",
    "num_features = [\n",
    "    col for col in df.columns if (not col in cat_features) and col != \"popularity\"\n",
    "]\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "X, y = (df.drop(\"popularity\", axis=1), df[\"popularity\"])\n",
    "\n",
    "n_folds = 2\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "scores = []\n",
    "\n",
    "enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "params_grid = [\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 6, \"min_samples_leaf\": 5},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 10, \"min_samples_leaf\": 1},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 10, \"min_samples_leaf\": 2},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "    {\"max_depth\": 100, \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "    {\"max_depth\": 50, \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "    {\"max_depth\": 10, \"min_samples_split\": 10, \"min_samples_leaf\": 3},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 10, \"min_samples_leaf\": 4},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 12, \"min_samples_leaf\": 5},\n",
    "    {\"max_depth\": 200, \"min_samples_split\": 15, \"min_samples_leaf\": 5},\n",
    "]\n",
    "\n",
    "for params in params_grid:\n",
    "    dtree = DecisionTreeRegressor(**params)\n",
    "    local_scores = np.zeros(n_folds)\n",
    "    for idx, (train_ids, val_ids) in enumerate(kf.split(df)):\n",
    "\n",
    "        X_train, X_val, y_train, y_val = (\n",
    "            X.iloc[train_ids].copy(),\n",
    "            X.iloc[val_ids].copy(),\n",
    "            y.iloc[train_ids].copy(),\n",
    "            y.iloc[val_ids].copy(),\n",
    "        )\n",
    "\n",
    "        for feature in cat_features:\n",
    "            for val, count in X_train[feature].value_counts().items():\n",
    "                if count <= 2:\n",
    "                    X_train.loc[X_train[feature] == val, feature] = \"other\"\n",
    "                    X_val.loc[X_val[feature] == val, feature] = \"other\"\n",
    "\n",
    "        enc.fit(X_train[cat_features])\n",
    "        X_train[cat_features] = enc.transform(X_train[cat_features])\n",
    "        X_val[cat_features] = enc.transform(X_val[cat_features])\n",
    "\n",
    "        dtree.fit(\n",
    "            X_train.to_numpy(),\n",
    "            y_train.to_numpy(),\n",
    "            features=list(X.columns),\n",
    "            cat_features=cat_features,\n",
    "        )\n",
    "\n",
    "        local_scores[idx] = root_mean_squared_error(\n",
    "            y_val, dtree.predict(X_val.to_numpy())\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Scores for {params}:\\n\\t{local_scores}\\nMean score: {local_scores.mean():0.4f}\\tstd: {local_scores.std():0.4f}\\n\\n\"\n",
    "    )\n",
    "    scores += [local_scores.mean()]\n",
    "\n",
    "best_tree_params = params_grid[np.argmin(scores)]\n",
    "print(f\"Best parameter set for a RegressionTree: {best_tree_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d0372-988c-4547-b067-3f7c7a12ae30",
   "metadata": {},
   "source": [
    "### Реализация `K-Fold Target encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c821133-354d-47a0-bcb2-6cfd3690d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cv: int = None,\n",
    "        handle_unknown: str = None,\n",
    "        unknown_value: float = None,\n",
    "        shuffle: bool = None,\n",
    "    ) -> None:\n",
    "        self.encoded_missing_value_ = np.nan\n",
    "        self.cv_ = 5 if cv is None else cv\n",
    "        self.shuffle_ = False if shuffle is None else shuffle\n",
    "\n",
    "        # Check if `handle_unknown` is set to a known value:\n",
    "        if handle_unknown is None:\n",
    "            self.handle_unknown_ = \"mean\"\n",
    "        else:\n",
    "            if handle_unknown == \"mean\":\n",
    "                self.handle_unknown_ = \"mean\"\n",
    "            elif handle_unknown == \"use_encoded_value\":\n",
    "                self.handle_unknown_ = \"use_encoded_value\"\n",
    "                self.unknown_value_ = 0.0 if unknown_value is None else unknown_value\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: Union[npt.ArrayLike, pd.Series, pd.DataFrame],\n",
    "        target: npt.ArrayLike,\n",
    "    ):\n",
    "        self.data_ = data\n",
    "        self.target_ = target\n",
    "\n",
    "        if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series):\n",
    "            self.data_ = self.data_.to_numpy(dtype=np.object_)\n",
    "\n",
    "        self.transformed_data_ = np.zeros_like(self.data_, dtype=float)\n",
    "\n",
    "        if isinstance(target, pd.DataFrame):\n",
    "            self.target_ = self.target_.to_numpy(dtype=np.object_)\n",
    "\n",
    "        if self.handle_unknown_ == \"mean\":\n",
    "            self.unknown_value_ = self.target_.mean()\n",
    "\n",
    "        kf = KFold(n_splits=self.cv_, shuffle=self.shuffle_)\n",
    "\n",
    "        for train_idx, val_idx in kf.split(self.data_):\n",
    "\n",
    "            X_train, X_val = (self.data_[train_idx, :], self.data_[val_idx, :])\n",
    "            y_train = self.target_[train_idx]\n",
    "\n",
    "            for feature in range(X_train.shape[1]):\n",
    "                for unique_val in set(X_val[:, feature]):\n",
    "                    train_mask = X_train[:, feature] == unique_val\n",
    "                    weight = (\n",
    "                        train_mask.sum()\n",
    "                        * (\n",
    "                            y_train[train_mask].mean()\n",
    "                            if len(y_train[train_mask]) >= 1\n",
    "                            else 0.0\n",
    "                        )\n",
    "                        + y_train.mean() * (X_train.shape[0] - train_mask.sum())\n",
    "                    ) / X_train.shape[0]\n",
    "                    self.transformed_data_[val_idx, feature] = np.where(\n",
    "                        X_val[:, feature] == unique_val,\n",
    "                        weight,\n",
    "                        self.transformed_data_[val_idx, feature],\n",
    "                    )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_transform(\n",
    "        self,\n",
    "        data: Union[npt.ArrayLike, pd.Series, pd.DataFrame],\n",
    "        target: npt.ArrayLike,\n",
    "    ) -> npt.ArrayLike:\n",
    "        self.fit(data, target)\n",
    "        return self.transformed_data_\n",
    "\n",
    "    def transform(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "\n",
    "        ans = np.zeros_like(X)\n",
    "\n",
    "        for feature in range(self.data_.shape[1]):\n",
    "            for unique_val in set(X[:, feature]):\n",
    "                mask = self.data_[:, feature] == unique_val\n",
    "                weight = (\n",
    "                    self.unknown_value_\n",
    "                    if mask.sum() == 0\n",
    "                    else self.target_[mask].mean()\n",
    "                )\n",
    "                ans[:, feature] = np.where(\n",
    "                    X[:, feature] == unique_val, weight, ans[:, feature]\n",
    "                )\n",
    "        return ans\n",
    "        # for col in self.columns_:\n",
    "\n",
    "        #     encoded = col + \"_\" + \"kfold_target\"\n",
    "        #     col_mean = self.data_[[col, encoded]].groupby(col).mean().reset_index()\n",
    "\n",
    "        #     replacements = {}\n",
    "        #     for index, row in col_mean.iterrows():\n",
    "        #         replacements[row[col]] = row[encoded]\n",
    "\n",
    "        #     # X.insert(len(X.columns), encoded, X[col])\n",
    "        #     X.insert(\n",
    "        #         len(X.columns),\n",
    "        #         encoded,\n",
    "        #         X[col].map(\n",
    "        #             lambda x: (\n",
    "        #                 self.unknown_value_\n",
    "        #                 if not x in replacements\n",
    "        #                 else replacements[x]\n",
    "        #             )\n",
    "        #         ),\n",
    "        #     )\n",
    "        #     X[col].where(X[col].isin(replacements), other=self.unknown_value_)\n",
    "        #     X = X.replace({encoded: replacements}).infer_objects(copy=False)\n",
    "\n",
    "        #     if self.inplace_:\n",
    "        #         X = X.drop([col], axis=1)\n",
    "\n",
    "        # return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f3289-1224-4a86-a4d8-4ebbea9593af",
   "metadata": {},
   "source": [
    "### Тестируем бустинг с `K-Fold Target Encoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1109cec7-1c93-49e4-9853-219ac9cf5b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_fold = [\n",
    "    \"type\",\n",
    "    \"track_href\",\n",
    "    \"track_href\",\n",
    "    \"uri\",\n",
    "    \"analysis_url\",\n",
    "    \"track_id\",\n",
    "    \"track_album_id\",\n",
    "    \"track_name\",\n",
    "    \"track_album_name\",\n",
    "    \"id\",\n",
    "    \"playlist_id\",\n",
    "]\n",
    "\n",
    "cat_features_fold = [\n",
    "    \"playlist_name\",\n",
    "    \"playlist_genre\",\n",
    "    \"track_artist\",\n",
    "    \"time_signature\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"track_album_release_date\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "]\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"train.csv\"))\n",
    "df_train = df_train.drop(\n",
    "    drop_list_fold + [\"Unnamed: 0\"],\n",
    "    axis=1,\n",
    ")\n",
    "df_train = df_train.dropna()\n",
    "df_train = df_train.drop_duplicates()\n",
    "\n",
    "df_train.loc[:, \"track_album_release_date\"] = (\n",
    "    pd.to_datetime(\n",
    "        df_train[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n",
    "    ).dt.year\n",
    ").astype(str)\n",
    "\n",
    "gbX, gby = (df_train.drop(\"popularity\", axis=1), df_train[\"popularity\"])\n",
    "\n",
    "gbX_train, gbX_test, gby_train, gby_test = train_test_split(\n",
    "    gbX, gby, train_size=0.7, random_state=42\n",
    ")\n",
    "\n",
    "# encoder = TargetEncoder(cv=5, target_type=\"continuous\")\n",
    "# gbX_train.loc[:, cat_features_fold] = encoder.fit_transform(\n",
    "#     gbX_train[cat_features_fold], gby_train\n",
    "# )\n",
    "# gbX_test.loc[:, cat_features_fold] = encoder.transform(gbX_test[cat_features_fold])\n",
    "encoder = KFoldTargetEncoder(cv=5)\n",
    "gbX_train.loc[:, cat_features_fold] = encoder.fit_transform(\n",
    "    gbX_train[cat_features_fold].to_numpy(),\n",
    "    gby_train.to_numpy(),\n",
    ")\n",
    "gbX_test.loc[:, cat_features_fold] = encoder.transform(\n",
    "    gbX_test[cat_features_fold].to_numpy()\n",
    ")\n",
    "# encoder = KFoldTargetEncoder(inplace=True, n_folds=10)\n",
    "# encoder.fit(gbX_train[cat_features_fold], gby_train)\n",
    "# gbX_train = gbX_train.join(\n",
    "#     encoder.data_.drop(cat_features_fold + [\"popularity\"], axis=1), how=\"outer\"\n",
    "# ).drop(cat_features_fold, axis=1)\n",
    "# gbX_test = gbX_test.join(\n",
    "#     encoder.transform(gbX_test[cat_features_fold]), how=\"outer\"\n",
    "# ).drop(cat_features_fold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "538d87a0-d470-4798-800c-a0cc25a8bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>tempo</th>\n",
       "      <th>danceability</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>loudness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>mode</th>\n",
       "      <th>key</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>playlist_subgenre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3086</th>\n",
       "      <td>0.779</td>\n",
       "      <td>128.028</td>\n",
       "      <td>0.937</td>\n",
       "      <td>54.88526</td>\n",
       "      <td>-3.539</td>\n",
       "      <td>0.0408</td>\n",
       "      <td>0.950</td>\n",
       "      <td>54.676091</td>\n",
       "      <td>54.939152</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>55.083493</td>\n",
       "      <td>54.654061</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>54.425280</td>\n",
       "      <td>54.578650</td>\n",
       "      <td>155625.0</td>\n",
       "      <td>0.220</td>\n",
       "      <td>55.083493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.538</td>\n",
       "      <td>146.933</td>\n",
       "      <td>0.759</td>\n",
       "      <td>55.135882</td>\n",
       "      <td>-8.256</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.831</td>\n",
       "      <td>54.676091</td>\n",
       "      <td>54.939152</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>54.76064</td>\n",
       "      <td>54.391507</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.425280</td>\n",
       "      <td>54.773891</td>\n",
       "      <td>184490.0</td>\n",
       "      <td>0.151</td>\n",
       "      <td>54.76064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>0.878</td>\n",
       "      <td>129.996</td>\n",
       "      <td>0.521</td>\n",
       "      <td>54.407245</td>\n",
       "      <td>-4.424</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>0.563</td>\n",
       "      <td>54.671788</td>\n",
       "      <td>54.939152</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>54.652924</td>\n",
       "      <td>54.025048</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>54.926903</td>\n",
       "      <td>54.433759</td>\n",
       "      <td>219723.0</td>\n",
       "      <td>0.158</td>\n",
       "      <td>54.652924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0.541</td>\n",
       "      <td>140.021</td>\n",
       "      <td>0.662</td>\n",
       "      <td>54.88526</td>\n",
       "      <td>-7.236</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.167</td>\n",
       "      <td>54.676091</td>\n",
       "      <td>54.939152</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>54.61174</td>\n",
       "      <td>54.025048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54.926903</td>\n",
       "      <td>54.630279</td>\n",
       "      <td>144583.0</td>\n",
       "      <td>0.449</td>\n",
       "      <td>54.683622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>0.598</td>\n",
       "      <td>90.029</td>\n",
       "      <td>0.695</td>\n",
       "      <td>54.619229</td>\n",
       "      <td>-8.186</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.376</td>\n",
       "      <td>54.679724</td>\n",
       "      <td>54.939152</td>\n",
       "      <td>0.1360</td>\n",
       "      <td>54.619229</td>\n",
       "      <td>54.654061</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>54.926903</td>\n",
       "      <td>54.773891</td>\n",
       "      <td>127333.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>54.619229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      energy    tempo  danceability playlist_genre  loudness  liveness  \\\n",
       "3086   0.779  128.028         0.937       54.88526    -3.539    0.0408   \n",
       "499    0.538  146.933         0.759      55.135882    -8.256    0.1260   \n",
       "1353   0.878  129.996         0.521      54.407245    -4.424    0.1240   \n",
       "436    0.541  140.021         0.662       54.88526    -7.236    0.2080   \n",
       "365    0.598   90.029         0.695      54.619229    -8.186    0.1120   \n",
       "\n",
       "      valence track_artist  time_signature  speechiness playlist_name  \\\n",
       "3086    0.950    54.676091       54.939152       0.3290     55.083493   \n",
       "499     0.831    54.676091       54.939152       0.3840      54.76064   \n",
       "1353    0.563    54.671788       54.939152       0.0273     54.652924   \n",
       "436     0.167    54.676091       54.939152       0.0663      54.61174   \n",
       "365     0.376    54.679724       54.939152       0.1360     54.619229   \n",
       "\n",
       "     track_album_release_date  instrumentalness       mode        key  \\\n",
       "3086                54.654061          0.000068  54.425280  54.578650   \n",
       "499                 54.391507          0.000000  54.425280  54.773891   \n",
       "1353                54.025048          0.000299  54.926903  54.433759   \n",
       "436                 54.025048          0.000000  54.926903  54.630279   \n",
       "365                 54.654061          0.000006  54.926903  54.773891   \n",
       "\n",
       "      duration_ms  acousticness playlist_subgenre  \n",
       "3086     155625.0         0.220         55.083493  \n",
       "499      184490.0         0.151          54.76064  \n",
       "1353     219723.0         0.158         54.652924  \n",
       "436      144583.0         0.449         54.683622  \n",
       "365      127333.0         0.330         54.619229  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbX_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3786cef2-9b58-44c2-8763-3acef0ed6780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>tempo</th>\n",
       "      <th>danceability</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>loudness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>mode</th>\n",
       "      <th>key</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>playlist_subgenre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>0.703</td>\n",
       "      <td>108.123</td>\n",
       "      <td>0.669</td>\n",
       "      <td>50.680851</td>\n",
       "      <td>-6.113</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.762</td>\n",
       "      <td>48.714286</td>\n",
       "      <td>55.284762</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>46.984</td>\n",
       "      <td>54.842262</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>54.581478</td>\n",
       "      <td>52.937824</td>\n",
       "      <td>164213.0</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>46.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>0.802</td>\n",
       "      <td>96.015</td>\n",
       "      <td>0.729</td>\n",
       "      <td>30.702479</td>\n",
       "      <td>-6.346</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>0.705</td>\n",
       "      <td>54.994447</td>\n",
       "      <td>55.284762</td>\n",
       "      <td>0.1790</td>\n",
       "      <td>27.833333</td>\n",
       "      <td>48.43662</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>55.326180</td>\n",
       "      <td>52.907258</td>\n",
       "      <td>233177.0</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>27.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>0.581</td>\n",
       "      <td>130.033</td>\n",
       "      <td>0.613</td>\n",
       "      <td>50.680851</td>\n",
       "      <td>-8.588</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.551</td>\n",
       "      <td>76.5</td>\n",
       "      <td>55.284762</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>69.333333</td>\n",
       "      <td>48.43662</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>54.581478</td>\n",
       "      <td>53.568182</td>\n",
       "      <td>239560.0</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>69.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>0.327</td>\n",
       "      <td>90.025</td>\n",
       "      <td>0.707</td>\n",
       "      <td>50.487179</td>\n",
       "      <td>-17.176</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.181</td>\n",
       "      <td>70.0</td>\n",
       "      <td>55.284762</td>\n",
       "      <td>0.0687</td>\n",
       "      <td>50.487179</td>\n",
       "      <td>54.842262</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>54.581478</td>\n",
       "      <td>53.568182</td>\n",
       "      <td>176000.0</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>50.487179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>0.814</td>\n",
       "      <td>107.094</td>\n",
       "      <td>0.688</td>\n",
       "      <td>53.136364</td>\n",
       "      <td>-4.614</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>0.761</td>\n",
       "      <td>54.994447</td>\n",
       "      <td>55.284762</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>53.136364</td>\n",
       "      <td>54.842262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.326180</td>\n",
       "      <td>56.259494</td>\n",
       "      <td>183193.0</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>62.214765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      energy    tempo  danceability playlist_genre  loudness  liveness  \\\n",
       "3042   0.703  108.123         0.669      50.680851    -6.113    0.1570   \n",
       "1005   0.802   96.015         0.729      30.702479    -6.346    0.0905   \n",
       "2038   0.581  130.033         0.613      50.680851    -8.588    0.2500   \n",
       "3156   0.327   90.025         0.707      50.487179   -17.176    0.1550   \n",
       "1454   0.814  107.094         0.688      53.136364    -4.614    0.2740   \n",
       "\n",
       "      valence track_artist  time_signature  speechiness playlist_name  \\\n",
       "3042    0.762    48.714286       55.284762       0.0581        46.984   \n",
       "1005    0.705    54.994447       55.284762       0.1790     27.833333   \n",
       "2038    0.551         76.5       55.284762       0.0424     69.333333   \n",
       "3156    0.181         70.0       55.284762       0.0687     50.487179   \n",
       "1454    0.761    54.994447       55.284762       0.0452     53.136364   \n",
       "\n",
       "     track_album_release_date  instrumentalness       mode        key  \\\n",
       "3042                54.842262          0.000019  54.581478  52.937824   \n",
       "1005                 48.43662          0.000004  55.326180  52.907258   \n",
       "2038                 48.43662          0.000345  54.581478  53.568182   \n",
       "3156                54.842262          0.014800  54.581478  53.568182   \n",
       "1454                54.842262          0.000000  55.326180  56.259494   \n",
       "\n",
       "      duration_ms  acousticness playlist_subgenre  \n",
       "3042     164213.0        0.0788            46.984  \n",
       "1005     233177.0        0.0924         27.833333  \n",
       "2038     239560.0        0.5370         69.333333  \n",
       "3156     176000.0        0.1850         50.487179  \n",
       "1454     183193.0        0.0196         62.214765  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbX_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e6cef0-c2c8-4c00-bd35-207f376bc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE(DecisionTreeRegressor): 19.6699\n"
     ]
    }
   ],
   "source": [
    "dt_regressor = DecisionTreeRegressor(**best_tree_params)\n",
    "dt_regressor.fit(\n",
    "    gbX_train.to_numpy(dtype=float),\n",
    "    gby_train.to_numpy(dtype=float),\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"RMSE(DecisionTreeRegressor): {root_mean_squared_error(gby_test, dt_regressor.predict(gbX_test.to_numpy())):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9643624-b77c-4dc1-830d-251331479db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE(dummy regressor): 20.5391\n",
      "RMSE(GradientBoostingRegressor): 16.7948\n",
      "RMSE(GradientBoostingRegressor | train set): 11.5138\n",
      "RMSE(Sklearn GradientBoostingRegressor): 16.6782\n",
      "RMSE(Sklearn GradientBoostingRegressor | train set): 10.4195\n"
     ]
    }
   ],
   "source": [
    "gbbig_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=25,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=6,\n",
    "    learning_rate=0.1,\n",
    "    tol=1e-4,\n",
    ")\n",
    "gbbig_regressor.fit(\n",
    "    gbX_train.to_numpy(dtype=float),\n",
    "    gby_train.to_numpy(dtype=float),\n",
    ")\n",
    "\n",
    "sbig_regressor = SGradientBoostingRegressor(\n",
    "    n_estimators=25,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=6,\n",
    "    learning_rate=0.1,\n",
    "    tol=1e-4,\n",
    ")\n",
    "sbig_regressor.fit(\n",
    "    gbX_train,\n",
    "    gby_train,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"RMSE(dummy regressor): {root_mean_squared_error(gby_test, np.ones_like(gby_test)*gby_train.mean()):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(GradientBoostingRegressor): {root_mean_squared_error(gby_test, gbbig_regressor.predict(gbX_test.to_numpy(dtype=float))):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(GradientBoostingRegressor | train set): {root_mean_squared_error(gby_train, gbbig_regressor.predict(gbX_train.to_numpy(dtype=float))):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(Sklearn GradientBoostingRegressor): {root_mean_squared_error(gby_test, sbig_regressor.predict(gbX_test)):0.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"RMSE(Sklearn GradientBoostingRegressor | train set): {root_mean_squared_error(gby_train, sbig_regressor.predict(gbX_train)):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e6ba6-02d5-474e-921a-dd1e01ea6c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gbbig_regressor.trees_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d060a51-4be8-4bf1-9acc-69e74090039b",
   "metadata": {},
   "source": [
    "### Сбор результатов на отправку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6af825-47c7-4994-8870-e26864dba389",
   "metadata": {},
   "source": [
    "Для бустинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79e823-f8b6-4535-a8ba-8b23c1a4fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list_gb = [\n",
    "    \"type\",\n",
    "    \"track_href\",\n",
    "    \"track_href\",\n",
    "    \"uri\",\n",
    "    \"analysis_url\",\n",
    "    \"track_id\",\n",
    "    \"track_album_id\",\n",
    "    \"track_name\",\n",
    "    \"track_album_name\",\n",
    "    \"id\",\n",
    "    \"playlist_id\",\n",
    "]\n",
    "\n",
    "cat_features_gb = [\n",
    "    \"playlist_name\",\n",
    "    \"playlist_genre\",\n",
    "    \"track_artist\",\n",
    "    \"time_signature\",\n",
    "    \"playlist_subgenre\",\n",
    "    \"track_album_release_date\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "]\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"train.csv\"))\n",
    "df_train = df_train.drop(\n",
    "    drop_list_gb + [\"Unnamed: 0\"],\n",
    "    axis=1,\n",
    ")\n",
    "df_train = df_train.dropna()\n",
    "df_train = df_train.drop_duplicates()\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"test.csv\"))\n",
    "df_test = df_test.drop(\n",
    "    drop_list_gb,\n",
    "    axis=1,\n",
    ")\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "df_train.loc[:, \"track_album_release_date\"] = (\n",
    "    pd.to_datetime(\n",
    "        df_train[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n",
    "    ).dt.year\n",
    ").astype(str)\n",
    "\n",
    "df_test.loc[:, \"track_album_release_date\"] = (\n",
    "    pd.to_datetime(\n",
    "        df_test[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n",
    "    ).dt.year\n",
    ").astype(str)\n",
    "\n",
    "gbX, gby = (df_train.drop(\"popularity\", axis=1), df_train[\"popularity\"])\n",
    "\n",
    "encoder = TargetEncoder(cv=5, target_type=\"continuous\")\n",
    "gbX.loc[:, cat_features_gb] = encoder.fit_transform(gbX[cat_features_fold], gby)\n",
    "df_test.loc[:, cat_features_gb] = encoder.transform(df_test[cat_features_fold])\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=3,\n",
    "    min_samples_split=10,\n",
    "    learning_rate=0.1,\n",
    "    tol=1e-4,\n",
    ")\n",
    "gb_regressor.fit(\n",
    "    gbX.to_numpy(dtype=float),\n",
    "    gby.to_numpy(dtype=float),\n",
    ")\n",
    "\n",
    "y_pred = pd.DataFrame(\n",
    "    gb_regressor.predict(df_test.to_numpy(dtype=float)),\n",
    "    index=pd.Index(df_test.index, name=\"Id\"),\n",
    "    columns=[\"popularity\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4f726-689c-400f-a926-22e650979539",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv(\n",
    "    path_or_buf=os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"submission.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462c259-fbc7-4d50-9dcf-37d132502862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
