{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c72e1d-c241-4026-831d-f6dbe0d70718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cff8e6-6a18-45f8-a111-82967cb00e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from typing import Dict, Any, Union\n",
    "from IPython.display import display\n",
    "from graphviz import Digraph\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_iris, make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTreeClassifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a116083-b160-4d44-a4e8-426622726165",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(y: npt.ArrayLike, weights: npt.ArrayLike = None) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\"\n",
    "    counts = np.unique(y, return_counts=True)\n",
    "    if weights is None:\n",
    "        probs = counts[1] / y.shape[0]\n",
    "        ans = -np.sum(probs * np.log(probs + 1e-10))\n",
    "    else:\n",
    "        ans = 0.0\n",
    "        total_weight = weights.sum()\n",
    "        for value in counts[0]:\n",
    "            prob = weights[y == value].sum() / total_weight\n",
    "            ans -= prob * np.log(prob + 1e-10)\n",
    "    return ans\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Decision tree classifier, which can be trained, can predict class labels(miraculously) and display itself if used in a frontend environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "    ):\n",
    "        self.max_depth = -1 if max_depth is None else max_depth\n",
    "        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n",
    "        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list = None,\n",
    "        cat_features: list = None,\n",
    "        sample_weight: list = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "\n",
    "        if features is None:\n",
    "            self._features = [f\"x_{i}\" for i in range(X.shape[1])]\n",
    "        else:\n",
    "            self._features = features\n",
    "\n",
    "        if cat_features is None:\n",
    "            self._cat_features = []\n",
    "        else:\n",
    "            self._cat_features = cat_features\n",
    "\n",
    "        self._weights = sample_weight\n",
    "\n",
    "        if isinstance(y[0], str):\n",
    "            self._prediction_dtype = \"object\"\n",
    "        else:\n",
    "            self._prediction_dtype = \"int64\"\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = self.classes_.shape[0]\n",
    "\n",
    "        self._tree = DecisionTreeClassifier._c45_algorithm(\n",
    "            X,\n",
    "            y,\n",
    "            self._features,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            cat_features=self._cat_features,\n",
    "            weights=self._weights,\n",
    "        )\n",
    "        self._n_leaves = DecisionTreeClassifier._compute_subtree_leaves(self._tree)\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0],), dtype=self._prediction_dtype)\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self._tree\n",
    "            while \"children\" in node:\n",
    "                feature_id = self._features.index(node[\"feature\"])\n",
    "                if node[\"feature\"] in self._cat_features:\n",
    "                    node = node[\"children\"][X[i, feature_id]]\n",
    "                else:\n",
    "                    node = (\n",
    "                        node[\"children\"][f\"<={node['threshold']:0.2f}\"]\n",
    "                        if X[i, feature_id] <= node[\"threshold\"]\n",
    "                        else node[\"children\"][f\">{node['threshold']:0.2f}\"]\n",
    "                    )\n",
    "            ans[i] = node[\"majority_class\"]\n",
    "        return ans\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree construction\n",
    "    # =============================================================================\n",
    "\n",
    "    def _c45_algorithm(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        max_depth: int,\n",
    "        min_samples_split: int,\n",
    "        min_samples_leaf: int,\n",
    "        cat_features: list,\n",
    "        weights: list,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Recursively build an C4.5 decision tree.\"\"\"\n",
    "\n",
    "        majority_class = DecisionTreeClassifier._majority_class(y)\n",
    "        error = np.sum(y != majority_class)\n",
    "\n",
    "        # Base case: all samples same class\n",
    "        if len(set(y)) == 1 or X.shape[1] < 1:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Node contains not enough elements to split\n",
    "        if len(y) < min_samples_split:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Max tree depth is reached\n",
    "        if max_depth == 0:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        best_id, best_feature, threshold = DecisionTreeClassifier._select_best_feature(\n",
    "            X, y, features, cat_features, weights\n",
    "        )\n",
    "        feature_values = np.unique(X[:, best_id])\n",
    "        new_features = features.copy()\n",
    "        new_features.remove(best_feature)\n",
    "\n",
    "        tree = {\n",
    "            \"feature\": best_feature,\n",
    "            \"majority_class\": majority_class,\n",
    "            \"children\": {},\n",
    "            \"error\": error,\n",
    "        }\n",
    "\n",
    "        if best_feature in cat_features:\n",
    "            for value in feature_values:\n",
    "                mask = X[:, best_id] == value\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[mask],\n",
    "                    y[mask],\n",
    "                    weights[mask],\n",
    "                )\n",
    "                if len(y_sub) < min_samples_leaf:\n",
    "                    child_majority_class = DecisionTreeClassifier._majority_class(y_sub)\n",
    "                    tree[\"children\"][value] = {\n",
    "                        \"majority_class\": child_majority_class,\n",
    "                        \"error\": np.sum(y_sub != child_majority_class),\n",
    "                    }\n",
    "                else:\n",
    "                    tree[\"children\"][value] = DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights,\n",
    "                    )\n",
    "        else:\n",
    "            mask = X[:, best_id] <= threshold\n",
    "            not_mask = np.logical_not(mask)\n",
    "            if (\n",
    "                np.sum(mask) >= min_samples_leaf\n",
    "                and (mask.shape[0] - np.sum(mask)) >= min_samples_leaf\n",
    "            ):\n",
    "                tree[\"threshold\"] = threshold\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[mask],\n",
    "                    y[mask],\n",
    "                    weights[mask],\n",
    "                )\n",
    "                tree[\"children\"][f\"<={threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights_sub,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[not_mask],\n",
    "                    y[not_mask],\n",
    "                    weights[not_mask],\n",
    "                )\n",
    "                tree[\"children\"][f\">{threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights_sub,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _majority_class(y: npt.ArrayLike):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def _gain_function(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        feature_idx: int,\n",
    "        cat_feature: bool,\n",
    "        weights: npt.ArrayLike,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate gain for a given feature.\"\"\"\n",
    "\n",
    "        if cat_feature is None or cat_feature == False:\n",
    "\n",
    "            def target_fn(\n",
    "                theta: float, X: npt.ArrayLike, y: npt.ArrayLike, weights: npt.ArrayLike\n",
    "            ):\n",
    "                mask_left = X[:, feature_idx] <= theta\n",
    "                mask_right = X[:, feature_idx] > theta\n",
    "                # raise ValueError(\n",
    "                #     f\"mask:{mask_left.shape}\\nw:{weights.shape}\\ny:{y.shape}\\n{weights[mask_left].shape}\"\n",
    "                # )\n",
    "                if weights is None:\n",
    "                    return mask_left.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_left],\n",
    "                        weights=weights,\n",
    "                    ) + mask_right.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_right],\n",
    "                        weights=weights,\n",
    "                    )\n",
    "                else:\n",
    "                    return mask_left.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_left],\n",
    "                        weights=weights[mask_left],\n",
    "                    ) + mask_right.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_right],\n",
    "                        weights=weights[mask_right],\n",
    "                    )\n",
    "\n",
    "            target_fn0 = lambda T: target_fn(T, X, y, weights)\n",
    "\n",
    "            minimize_result = minimize(target_fn0, x0=np.mean(X[:, feature_idx]))\n",
    "            ans = {\"value\": minimize_result.fun, \"threshold\": minimize_result.x[0]}\n",
    "        else:\n",
    "            values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "            probs = counts / y.shape[0]\n",
    "            entropies = np.array(\n",
    "                list(\n",
    "                    map(\n",
    "                        lambda x: entropy(\n",
    "                            y[X[:, feature_idx] == x],\n",
    "                            weights=weights[X[:, feature_idx] == x],\n",
    "                        ),\n",
    "                        values,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            ans = {\"value\": np.sum(probs * entropies), \"threshold\": None}\n",
    "\n",
    "        return ans\n",
    "\n",
    "    def _select_best_feature(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        cat_features: list,\n",
    "        weights: list,\n",
    "    ) -> list:\n",
    "        \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "        gains = [\n",
    "            DecisionTreeClassifier._gain_function(\n",
    "                X, y, i, feature in cat_features, weights\n",
    "            )\n",
    "            for i, feature in enumerate(features)\n",
    "        ]\n",
    "        best_idx = np.argmin(gain[\"value\"] for gain in gains)\n",
    "        return [best_idx, features[best_idx], gains[best_idx][\"threshold\"]]\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree pruning\n",
    "    # =============================================================================\n",
    "\n",
    "    def get_pruned_tree(self, alpha: float) -> Dict:\n",
    "        \"\"\"Prune the tree using cost-complexity pruning with parameter `alpha`.\"\"\"\n",
    "        ans = DecisionTreeClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "        )\n",
    "        ans._tree = deepcopy(self._tree)\n",
    "        ans._features = deepcopy(self._features)\n",
    "        ans._n_leaves = self._n_leaves\n",
    "        ans._cat_features = deepcopy(self._cat_features)\n",
    "        ans._prediction_dtype = deepcopy(self._prediction_dtype)\n",
    "        return ans\n",
    "\n",
    "    def prune_tree(self, alpha: float) -> None:\n",
    "        \"\"\"Prune the underlying decision tree using cost-complexity pruning with predefines `alpha`.\"\"\"\n",
    "        self._tree = DecisionTreeClassifier._cost_comprexity_pruning(\n",
    "            self._tree, alpha, inplace=True\n",
    "        )\n",
    "\n",
    "    def _compute_subtree_error(tree: Dict) -> int:\n",
    "        \"\"\"Calculate the total misclassification error of a (sub)tree.\"\"\"\n",
    "\n",
    "        if not \"children\" in tree:\n",
    "            return tree[\"error\"]\n",
    "\n",
    "        total_error = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_error += DecisionTreeClassifier._compute_subtree_error(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_error\n",
    "\n",
    "    def _compute_subtree_leaves(tree: Dict) -> int:\n",
    "        \"\"\"Count the number of leaf nodes in a (sub)tree.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return 1\n",
    "\n",
    "        total_leaves = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_leaves += DecisionTreeClassifier._compute_subtree_leaves(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_leaves\n",
    "\n",
    "    def _collect_pruning_candidates(tree: Dict, candidates: list) -> None:\n",
    "        \"\"\"Collect non-leaf nodes with their effective alpha values.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return candidates\n",
    "\n",
    "        subtree_error = DecisionTreeClassifier._compute_subtree_error(tree)\n",
    "        complexity_error = DecisionTreeClassifier._compute_subtree_leaves(tree)\n",
    "        R = tree[\"error\"]\n",
    "        effective_alpha = (R - subtree_error) / complexity_error\n",
    "\n",
    "        for child in tree[\"children\"]:\n",
    "            DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree[\"children\"][child], candidates\n",
    "            )\n",
    "\n",
    "        candidates.append((tree, effective_alpha))\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    def _cost_comprexity_pruning(self, alpha: float, inplace: bool = None) -> dict:\n",
    "        if inplace is True:\n",
    "            tree_to_prune = deepcopy(self._tree)\n",
    "        else:\n",
    "            tree_to_prune = self._tree\n",
    "        while True:\n",
    "            candidates = []\n",
    "            candidates = DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree_to_prune, candidates\n",
    "            )\n",
    "            candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "            weakest_subtree, weakest_alpha = candidates[0]\n",
    "\n",
    "            if weakest_alpha > alpha:\n",
    "                break\n",
    "\n",
    "            weakest_subtree[\"children\"] = {}\n",
    "            weakest_subtree.pop(\"feature\")\n",
    "\n",
    "        return tree_to_prune\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree visualization\n",
    "    # =============================================================================\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = DecisionTreeClassifier._visualize_tree(self._tree, self._features)\n",
    "        display(dot)\n",
    "\n",
    "    def _visualize_tree(\n",
    "        tree: Dict[str, Any],\n",
    "        feature_names: list,\n",
    "        dot: Digraph = None,\n",
    "        parent: str = None,\n",
    "        edge_label: str = None,\n",
    "    ) -> Digraph:\n",
    "        \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "        if dot is None:\n",
    "            dot = Digraph(comment=\"Decision Tree\")\n",
    "\n",
    "        # Create a unique node ID\n",
    "        node_id = str(id(tree))\n",
    "\n",
    "        # Add the current node\n",
    "        if not \"children\" in tree:\n",
    "            node_label = f\"Class: {tree['majority_class']}\\nError: {tree['error']}\"\n",
    "        else:\n",
    "            node_label = f\"Feature: {tree['feature']}\\nError: {tree['error']}\"\n",
    "        dot.node(node_id, node_label)\n",
    "\n",
    "        # Connect to parent node if exists\n",
    "        if parent is not None:\n",
    "            dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "        # Recursively add children\n",
    "        if \"children\" in tree:\n",
    "            for value, child in tree[\"children\"].items():\n",
    "                DecisionTreeClassifier._visualize_tree(\n",
    "                    child, feature_names, dot, node_id, str(value)\n",
    "                )\n",
    "\n",
    "        return dot\n",
    "\n",
    "    # =============================================================================\n",
    "    # Utility functions\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e363d42a-10f1-4316-9783-876e1ff11c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifierScratch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator: object = None, n_estimators: int = None):\n",
    "        self.n_estimators = 50 if n_estimators is None else n_estimators\n",
    "        self.estimator = DecisionTreeClassifier if estimator is None else estimator\n",
    "        self.classifier_weights_ = np.zeros(self.n_estimators)\n",
    "        self.classifiers_ = [self.estimator for i in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X: npt.ArrayLike, y: npt.ArrayLike):\n",
    "        n_samples = X.shape[0]\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classifier_sample_weights_ = np.zeros((self.n_estimators, n_samples))\n",
    "        self.classifier_sample_weights_[0, :] = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            self.classifiers_[i].fit(\n",
    "                X, y, sample_weight=self.classifier_sample_weights_[i, :]\n",
    "            )\n",
    "            print(f\"#{i+1}...OK!\")\n",
    "            preds = self.classifiers_[i].predict(X)\n",
    "            errors_mask = preds != y\n",
    "\n",
    "            # the error term is artificially increased so that the method continues working if classifier does perfect classification\n",
    "            classifier_error = (\n",
    "                np.sum(self.classifier_sample_weights_[i, errors_mask])\n",
    "                / self.classifier_sample_weights_[i, :].sum()\n",
    "            ) + 1e-10\n",
    "\n",
    "            print(f\"Error term: {classifier_error:0.2f}\")\n",
    "            if classifier_error < 1.0 - 1.0 / self.classes_.shape[0]:\n",
    "                self.classifier_weights_[i] = (\n",
    "                    np.log(1.0 - classifier_error)\n",
    "                    - np.log(classifier_error)\n",
    "                    + np.log(self.classes_.shape[0] - 1.0)\n",
    "                )\n",
    "                if i < self.n_estimators - 1:\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        self.classifier_sample_weights_[i, :]\n",
    "                        * np.exp(self.classifier_weights_[i] * errors_mask)\n",
    "                    )\n",
    "\n",
    "                    # Normalize the weights\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        self.classifier_sample_weights_[i + 1, :]\n",
    "                        / self.classifier_sample_weights_[i + 1, :].sum()\n",
    "                    )\n",
    "            else:\n",
    "                print(\"Bad error value. Resetting the weights...\")\n",
    "                self.classifier_weights_[i] = 0.0\n",
    "                if i < self.n_estimators - 1:\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        np.ones(n_samples) / n_samples\n",
    "                    )\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike):\n",
    "        cls_predictions = np.zeros((self.n_estimators, X.shape[0]))\n",
    "        for cls_num, cls in enumerate(self.classifiers_):\n",
    "            cls_predictions[cls_num, :] = cls.predict(X)\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for i in range(predictions.shape[0]):\n",
    "            predictions[i] = self.classes_[\n",
    "                np.argmax(\n",
    "                    [\n",
    "                        (\n",
    "                            self.classifier_weights_\n",
    "                            * (cls_predictions[:, i] == pred_class)\n",
    "                        ).sum(axis=0)\n",
    "                        for pred_class in self.classes_\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44820ab8-b5b2-4640-81a7-06a76ca1c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoost(BaseEstimator):\n",
    "    def __init__(self, estimator: object = None, n_estimators: int = None):\n",
    "        self.n_estimators = 50 if n_estimators is None else n_estimators\n",
    "        self.estimator = DecisionTreeClassifier if estimator is None else estimator\n",
    "        self.classifier_weights_ = np.zeros(self.n_estimators)\n",
    "        self.classifiers_ = [self.estimator for i in range(self.n_estimators)]  \n",
    "\n",
    "    def fit(self, X: npt.ArrayLike, y: npt.ArrayLike):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(f\"Incorrect input array shape: {X.shape} and {y.shape}\")\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aca467b-dfd1-4c57-ac89-20c3aa2c85ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'energy', 'tempo', 'danceability', 'playlist_genre',\n",
       "       'loudness', 'liveness', 'valence', 'track_artist', 'time_signature',\n",
       "       'speechiness', 'track_href', 'uri', 'track_album_name', 'playlist_name',\n",
       "       'analysis_url', 'track_id', 'track_name', 'track_album_release_date',\n",
       "       'instrumentalness', 'track_album_id', 'mode', 'key', 'duration_ms',\n",
       "       'acousticness', 'id', 'playlist_subgenre', 'type', 'playlist_id',\n",
       "       'popularity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"spotify-songs\", \"train.csv\"))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08fba66c-97a9-4ace-ba51-831489d8a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    [\n",
    "        \"Unnamed: 0\",\n",
    "        \"type\",\n",
    "        \"track_href\",\n",
    "        \"track_href\",\n",
    "        \"uri\",\n",
    "        \"track_album_name\",\n",
    "        \"analysis_url\",\n",
    "        \"track_id\",\n",
    "        \"track_name\",\n",
    "        \"track_album_id\",\n",
    "        \"id\",\n",
    "        \"playlist_id\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "cat_features = [\n",
    "    \"playlist_genre\",\n",
    "    \"valence\",\n",
    "    \"track_artist\",\n",
    "    \"track_signature\",\n",
    "    \"playlist_name\",\n",
    "    \"mode\",\n",
    "    \"key\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c886c2-0874-40d3-8209-ceb151cef6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>tempo</th>\n",
       "      <th>danceability</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>loudness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>mode</th>\n",
       "      <th>key</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>playlist_subgenre</th>\n",
       "      <th>popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00948</td>\n",
       "      <td>67.237</td>\n",
       "      <td>0.188</td>\n",
       "      <td>wellness</td>\n",
       "      <td>-37.230</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.0738</td>\n",
       "      <td>Burgundy Skies</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>Yoga &amp; Meditation</td>\n",
       "      <td>2024-03-29</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>151340.0</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>yoga</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.41300</td>\n",
       "      <td>94.938</td>\n",
       "      <td>0.494</td>\n",
       "      <td>pop</td>\n",
       "      <td>-10.432</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>Gigi Perez</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>Global Top 50</td>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>211979.0</td>\n",
       "      <td>0.6820</td>\n",
       "      <td>global</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.38600</td>\n",
       "      <td>142.127</td>\n",
       "      <td>0.519</td>\n",
       "      <td>world</td>\n",
       "      <td>-12.732</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>Yume.Play</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>Chinese Traditional</td>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>0.773000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>124620.0</td>\n",
       "      <td>0.6800</td>\n",
       "      <td>chinese</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.89800</td>\n",
       "      <td>132.027</td>\n",
       "      <td>0.779</td>\n",
       "      <td>gospel</td>\n",
       "      <td>-4.589</td>\n",
       "      <td>0.1820</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>Olamilekan Akamo</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>Modern Gospel</td>\n",
       "      <td>2023-09-24</td>\n",
       "      <td>0.142000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1034000.0</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>modern</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.40900</td>\n",
       "      <td>170.071</td>\n",
       "      <td>0.714</td>\n",
       "      <td>electronic</td>\n",
       "      <td>-6.476</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.4970</td>\n",
       "      <td>Dele GT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>French Touch</td>\n",
       "      <td>2024-01-13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136324.0</td>\n",
       "      <td>0.0821</td>\n",
       "      <td>french</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    energy    tempo  danceability playlist_genre  loudness  liveness  valence  \\\n",
       "0  0.00948   67.237         0.188       wellness   -37.230    0.1090   0.0738   \n",
       "1  0.41300   94.938         0.494            pop   -10.432    0.1930   0.2730   \n",
       "2  0.38600  142.127         0.519          world   -12.732    0.3750   0.3130   \n",
       "3  0.89800  132.027         0.779         gospel    -4.589    0.1820   0.6750   \n",
       "4  0.40900  170.071         0.714     electronic    -6.476    0.0883   0.4970   \n",
       "\n",
       "       track_artist  time_signature  speechiness        playlist_name  \\\n",
       "0    Burgundy Skies             4.0       0.0376    Yoga & Meditation   \n",
       "1        Gigi Perez             4.0       0.0254        Global Top 50   \n",
       "2         Yume.Play             4.0       0.0375  Chinese Traditional   \n",
       "3  Olamilekan Akamo             4.0       0.0850        Modern Gospel   \n",
       "4           Dele GT             4.0       0.0787         French Touch   \n",
       "\n",
       "  track_album_release_date  instrumentalness  mode   key  duration_ms  \\\n",
       "0               2024-03-29          0.960000   0.0   7.0     151340.0   \n",
       "1               2024-07-26          0.000067   1.0  11.0     211979.0   \n",
       "2               2024-03-15          0.773000   0.0   3.0     124620.0   \n",
       "3               2023-09-24          0.142000   1.0   8.0    1034000.0   \n",
       "4               2024-01-13          0.000000   1.0   0.0     136324.0   \n",
       "\n",
       "   acousticness playlist_subgenre  popularity  \n",
       "0        0.9710              yoga          46  \n",
       "1        0.6820            global          93  \n",
       "2        0.6800           chinese          34  \n",
       "3        0.0220            modern          30  \n",
       "4        0.0821            french          17  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49a4be31-733b-463a-8bf9-3ce9c4853215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_signature\n",
       "4.0    3219\n",
       "3.0     251\n",
       "5.0     123\n",
       "1.0      29\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"time_signature\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6648dd-17a2-4730-b4c5-4b60acb8380a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
