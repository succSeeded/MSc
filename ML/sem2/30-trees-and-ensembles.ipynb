{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv(\"tennis.csv.txt\")\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A decision tree for data above\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt4.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Algorithms\n",
    "\n",
    "| Feature                | ID.3 (Ross Quinlan et al.)    | CART (Leo Breiman et al.)     | C4.5, C5 (Ross Quinlan et al.) |\n",
    "|------------------------|-------------------------------|-------------------------------|-------------------------------|\n",
    "| **Splitting Criterion**| Information Gain              | Gini (classification) / Variance (regression) | Gain Ratio                   |\n",
    "| **Data Types**         | Categorical only              | Categorical + Continuous      | Categorical + Continuous      |\n",
    "| **Tree Structure**     | Multi-way                     | Binary                        | Multi-way for categorical features, binary for numerical           |\n",
    "| **Pruning**            | None                          | Cost-complexity               | Pessimistic pruning           |\n",
    "| **Task Support**       | Classification                | Classification + Regression   | Classification                |\n",
    "| **Overfitting**        | High (no pruning)             | Moderate (pruning, binary splits) | Low (pruning, gain ratio) |\n",
    "| **Missing Values**     | Not handled                   | Surrogate splits              | Fractional instances          |\n",
    "\n",
    "**scikit-learn decision trees are based on CART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Criterions a.k.a Impurity measures\n",
    "\n",
    "In the decision tree, each node corresponds to a subset of the train data $S$. Let $\\{p_i\\}_{i=1}^C$ be the target distribution over $C$ classes for this subset $S$.\n",
    "\n",
    "* __Entropy:__ $H(S) = -\\sum p_i\\log(p_i)$\n",
    "* __Gini Index:__ $Gini(S) = 1-\\sum p_i^2$\n",
    "\n",
    "> NOTE: the perfomance is usually similar, it doesn't matter which one to chose\n",
    "\n",
    "These are an **Impurity measures** of single node, now we want to select an optimal split. For candidate $S=S_1 \\sqcup S_2 \\dots \\sqcup S_{K_A}$ respecting the values of feature $A$\n",
    "\n",
    "* __Information gain__, aka mutual information $$IG(S, A) = H(S) - H(S|A) = I(S;A) = \\sum_{i=1}^{K_A}\\frac{|S_i|}{|S|}H(S_i)$$\n",
    "> NOTE: What are the limits of IG values? Can it be nega\n",
    "* __Gain Ratio__: $$GainRatio(S, A) = IG(S, A) \\cdot \\frac{1}{-\\sum_{i=1}^{K_A} \\frac{|S_i|}{|S|}\\log\\frac{|S_i|}{|S|}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Construction for the toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data.play.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy = - E(log_p)\n",
    "Entropy_Play = -5 / 14 * np.log(5 / 14) - 9 / 14 * np.log(9 / 14)\n",
    "Entropy_Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain on splitting by Outlook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == \"sunny\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=Sunny)\n",
    "Entropy_Play_Outlook_Sunny = -2 / 5 * np.log(2 / 5) - 3 / 5 * np.log(3 / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play_Outlook_Sunny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == \"overcast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=overcast)\n",
    "# Since, it's a homogenous data entropy will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == \"rainy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=rainy)\n",
    "Entropy_Play_Outlook_Rain = -2 / 5 * np.log(2 / 5) - 3 / 5 * np.log(3 / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play_Outlook_Rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain on splitting by attribute outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Informatio_Gain = (\n",
    "    Entropy_Play\n",
    "    - 5 / 14 * Entropy_Play_Outlook_Sunny\n",
    "    - 5 / 14 * Entropy_Play_Outlook_Rain\n",
    ")\n",
    "Informatio_Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other gains\n",
    "* Gain(Play, Temperature) - 0.029\n",
    "* Gain(Play, Humidity) - 0.151\n",
    "* Gain(Play, Wind) - 0.048\n",
    "\n",
    "#### Conclusion - Outlook is winner & thus becomes root of the tree\n",
    "<img src=\"https://i1.wp.com/sefiks.com/wp-content/uploads/2017/11/tree-v1.png?zoom=1.25&resize=728%2C252&ssl=1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to find the next splitting criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == \"overcast\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion - If outlook is overcast, play is true\n",
    "\n",
    "### Let's find the next splitting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'sunny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=Sunny)\n",
    "Entropy_Play_Outlook_Sunny = ...\n",
    "Entropy_Play_Outlook_Sunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Windy_False = ...\n",
    "Entropy_Windy_True = ...\n",
    "IG_Windy= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Humidity is the best choice on sunny branch\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt3.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tree\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt4.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "[The guide on decision trees is awesome](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, plot_tree\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainX, testX, trainY, testY = train_test_split(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "toy_data_enc = encoder.fit_transform(toy_data.drop(\"play\", axis=1))\n",
    "dt.fit(toy_data_enc, toy_data[\"play\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = export_graphviz(dt)\n",
    "graph = graphviz.Source(tree)\n",
    "graph.render()\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the tree\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt6.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> * CART (which is used inside sklearn DecisionTreeClassifier) will convert features with continues values into categorical values\n",
    "> * Different tree will be generated with same faeatures given in different order. This is because `max_features` algorithm and random feature subsampling inside.\n",
    "\n",
    "#### Feature Importances\n",
    "* Important features will be higher up the tree\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n",
    "Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Decision Boundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(\n",
    "    n_features=2, n_samples=1000, cluster_std=0.8, centers=4, random_state=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=5, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_step = 0.2\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = dt.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=5, cmap=\"viridis\")\n",
    "plt.scatter(xx.ravel(), yy.ravel(), c=outcome, s=1, alpha=1, cmap=\"viridis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt8.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Implementation\n",
    "\n",
    "For now, we will focus on the ID3 algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taste</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Texture</th>\n",
       "      <th>Eat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salty</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spicy</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Soft</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Taste Temperature Texture Eat\n",
       "0  Salty         Hot    Soft  No\n",
       "1  Spicy         Hot    Soft  No"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset and convert to DataFrame\n",
    "dataset = {\n",
    "    'Taste': ['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "    'Temperature': ['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "    'Texture': ['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "    'Eat': ['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']\n",
    "}\n",
    "dataframe = pd.DataFrame(dataset, columns=['Taste','Temperature','Texture','Eat'])\n",
    "\n",
    "# Prepare features and target\n",
    "feature_names = dataframe.columns[:-1].tolist()\n",
    "X = dataframe[feature_names].to_numpy()\n",
    "y = dataframe['Eat'].to_numpy()\n",
    "features = list(range(len(feature_names)))\n",
    "\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from IPython.display import display\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\"\n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / y.shape[0]\n",
    "    return -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "\n",
    "def information_gain(X: np.ndarray, y: np.ndarray, feature_idx: int) -> float:\n",
    "    \"\"\"Calculate the information gain for a given feature.\"\"\"\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "    probs = counts / y.shape[0]\n",
    "    entropies = np.array(\n",
    "        list(map(lambda x: entropy(y[X[:, feature_idx] == x]), values))\n",
    "    )\n",
    "    conditional_entropy = np.sum(probs * entropies)\n",
    "\n",
    "    return parent_entropy - conditional_entropy\n",
    "\n",
    "\n",
    "def select_best_feature(X: np.ndarray, y: np.ndarray, features: list) -> list:\n",
    "    \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "    best_idx = np.argmax([information_gain(X, y, i) for i in range(X.shape[1])])\n",
    "    return [best_idx, features[best_idx]]\n",
    "\n",
    "\n",
    "def id3_algorithm(X: np.ndarray, y: np.ndarray, features: list) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively build the ID3 decision tree.\"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    majority_class = classes[np.argmax(counts)]\n",
    "\n",
    "    # Base case: all samples same class\n",
    "    if len(classes) == 1 or not features:\n",
    "        return {\"class\": classes[0], \"majority_class\": classes[0]}\n",
    "\n",
    "    best_id, best_feature = select_best_feature(X, y, features)\n",
    "    feature_values = np.unique(X[:, best_id])\n",
    "    new_features = features.copy()\n",
    "    new_features.remove(best_feature)\n",
    "\n",
    "    tree = {\"feature\": best_feature, \"majority_class\": majority_class, \"children\": {}}\n",
    "\n",
    "    for value in feature_values:\n",
    "        mask = X[:, best_id] == value\n",
    "        X_sub, y_sub = np.delete(X, best_id, axis=1)[mask], y[mask]\n",
    "        if len(y_sub) == 0:\n",
    "            tree[\"children\"][value] = {\n",
    "                \"class\": majority_class,\n",
    "                \"majority_class\": majority_class,\n",
    "            }\n",
    "        else:\n",
    "            tree[\"children\"][value] = id3_algorithm(X_sub, y_sub, new_features)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def visualize_tree(\n",
    "    tree: Dict[str, Any],\n",
    "    feature_names: list,\n",
    "    dot: Digraph = None,\n",
    "    parent: str = None,\n",
    "    edge_label: str = None,\n",
    ") -> Digraph:\n",
    "    \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "    if dot is None:\n",
    "        dot = Digraph(comment=\"Decision Tree\")\n",
    "\n",
    "    # Create a unique node ID\n",
    "    node_id = str(id(tree))\n",
    "\n",
    "    # Add the current node\n",
    "    if \"class\" in tree:\n",
    "        node_label = f\"Class: {tree['class']}\"\n",
    "    else:\n",
    "        node_label = f\"Feature: {tree['feature']}\"\n",
    "    dot.node(node_id, node_label)\n",
    "\n",
    "    # Connect to parent node if exists\n",
    "    if parent is not None:\n",
    "        dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "    # Recursively add children\n",
    "    if \"children\" in tree:\n",
    "        for value, child in tree[\"children\"].items():\n",
    "            visualize_tree(child, feature_names, dot, node_id, str(value))\n",
    "\n",
    "    return dot\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Decision tree classifier, which can be trained, can predict class labels(miraculously) and display itself if used in a frontend environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._features = []\n",
    "        self._tree = {}\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, features: list) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "        self._features = features\n",
    "        self._tree = id3_algorithm(X, y, features)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0], 1), dtype=\"object\")\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self._tree\n",
    "            while \"class\" not in node:\n",
    "                feature_id = self._features.index(node[\"feature\"])\n",
    "                if X[i, feature_id] in node[\"children\"]:\n",
    "                    node = node[\"children\"][X[i, feature_id]]\n",
    "                else:\n",
    "                    break\n",
    "            if \"class\" in node:\n",
    "                ans[i, 0] = node[\"class\"]\n",
    "            else:\n",
    "                ans[i, 0] = node[\"majority_class\"]\n",
    "        return ans\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = visualize_tree(self._tree, self._features)\n",
    "        display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new data point: [['Yes']]\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTree()\n",
    "classifier.fit(X, y, feature_names.copy())\n",
    "\n",
    "# New data point for prediction\n",
    "new_data_point = np.array([[\"Salty\", \"Hot\", \"Hard\"]], dtype=\"object\")\n",
    "prediction = classifier.predict(new_data_point)\n",
    "\n",
    "print(f\"Prediction for the new data point: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.1.2 (20240928.0832)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"633pt\" height=\"310pt\"\n",
       " viewBox=\"0.00 0.00 632.56 309.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 305.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-305.5 628.56,-305.5 628.56,4 -4,4\"/>\n",
       "<!-- 2470777234368 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2470777234368</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"323.3\" cy=\"-283.5\" rx=\"62.61\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"323.3\" y=\"-278.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Feature: Taste</text>\n",
       "</g>\n",
       "<!-- 2470777236224 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2470777236224</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"143.3\" cy=\"-195\" rx=\"71.31\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"143.3\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Feature: Texture</text>\n",
       "</g>\n",
       "<!-- 2470777234368&#45;&gt;2470777236224 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2470777234368&#45;&gt;2470777236224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.36,-267.63C262.88,-253.46 218.27,-232.03 185.53,-216.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"187.25,-213.24 176.72,-212.06 184.22,-219.54 187.25,-213.24\"/>\n",
       "<text text-anchor=\"middle\" x=\"263.3\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Salty</text>\n",
       "</g>\n",
       "<!-- 2470778606848 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2470778606848</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"323.3\" cy=\"-195\" rx=\"90.25\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"323.3\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Feature: Temperature</text>\n",
       "</g>\n",
       "<!-- 2470777234368&#45;&gt;2470778606848 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2470777234368&#45;&gt;2470778606848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M323.3,-265.41C323.3,-253.76 323.3,-238.05 323.3,-224.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"326.8,-224.86 323.3,-214.86 319.8,-224.86 326.8,-224.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"338.68\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Spicy</text>\n",
       "</g>\n",
       "<!-- 2470778610304 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>2470778610304</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"481.3\" cy=\"-195\" rx=\"49.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"481.3\" y=\"-189.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: Yes</text>\n",
       "</g>\n",
       "<!-- 2470777234368&#45;&gt;2470778610304 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>2470777234368&#45;&gt;2470778610304</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.55,-267.04C377.56,-252.8 416.3,-231.59 444.67,-216.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"446.2,-219.21 453.29,-211.34 442.84,-213.07 446.2,-219.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"432.96\" y=\"-234.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Sweet</text>\n",
       "</g>\n",
       "<!-- 2470778610112 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2470778610112</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"49.3\" cy=\"-106.5\" rx=\"49.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"49.3\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: Yes</text>\n",
       "</g>\n",
       "<!-- 2470777236224&#45;&gt;2470778610112 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2470777236224&#45;&gt;2470778610112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.18,-177.32C110.96,-164.23 90.97,-145.84 75.08,-131.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.58,-128.76 67.85,-124.57 72.84,-133.91 77.58,-128.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"118.22\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Hard</text>\n",
       "</g>\n",
       "<!-- 2470777236416 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2470777236416</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"164.3\" cy=\"-106.5\" rx=\"47.26\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"164.3\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: No</text>\n",
       "</g>\n",
       "<!-- 2470777236224&#45;&gt;2470777236416 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2470777236224&#45;&gt;2470777236416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M147.45,-176.91C150.28,-165.26 154.1,-149.55 157.38,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.78,-136.86 159.73,-126.32 153.97,-135.21 160.78,-136.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.94\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Soft</text>\n",
       "</g>\n",
       "<!-- 2470255043328 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2470255043328</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"312.3\" cy=\"-106.5\" rx=\"71.31\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.3\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Feature: Texture</text>\n",
       "</g>\n",
       "<!-- 2470778606848&#45;&gt;2470255043328 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2470778606848&#45;&gt;2470255043328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M321.13,-176.91C319.65,-165.26 317.65,-149.55 315.93,-136.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"319.44,-135.83 314.7,-126.35 312.49,-136.71 319.44,-135.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"331.91\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Cold</text>\n",
       "</g>\n",
       "<!-- 2470778608128 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2470778608128</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"473.3\" cy=\"-106.5\" rx=\"71.31\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"473.3\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Feature: Texture</text>\n",
       "</g>\n",
       "<!-- 2470778606848&#45;&gt;2470778608128 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2470778606848&#45;&gt;2470778608128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M351.87,-177.53C375.66,-163.81 409.72,-144.17 435.72,-129.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"437.38,-132.26 444.29,-124.23 433.88,-126.19 437.38,-132.26\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.86\" y=\"-145.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Hot</text>\n",
       "</g>\n",
       "<!-- 2470777236288 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2470777236288</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"208.3\" cy=\"-18\" rx=\"47.26\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.3\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: No</text>\n",
       "</g>\n",
       "<!-- 2470255043328&#45;&gt;2470777236288 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2470255043328&#45;&gt;2470777236288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.25,-88.82C276.27,-75.53 253.71,-56.76 236,-42.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"238.33,-39.42 228.41,-35.72 233.86,-44.81 238.33,-39.42\"/>\n",
       "<text text-anchor=\"middle\" x=\"283.12\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Hard</text>\n",
       "</g>\n",
       "<!-- 2470778609664 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2470778609664</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"323.3\" cy=\"-18\" rx=\"49.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"323.3\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: Yes</text>\n",
       "</g>\n",
       "<!-- 2470255043328&#45;&gt;2470778609664 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2470255043328&#45;&gt;2470778609664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M314.48,-88.41C315.96,-76.76 317.96,-61.05 319.68,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"323.12,-48.21 320.91,-37.85 316.17,-47.33 323.12,-48.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"330.04\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Soft</text>\n",
       "</g>\n",
       "<!-- 2470778608704 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2470778608704</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"462.3\" cy=\"-18\" rx=\"49.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.3\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: Yes</text>\n",
       "</g>\n",
       "<!-- 2470778608128&#45;&gt;2470778608704 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2470778608128&#45;&gt;2470778608704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.13,-88.41C469.65,-76.76 467.65,-61.05 465.93,-47.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"469.44,-47.33 464.7,-37.85 462.49,-48.21 469.44,-47.33\"/>\n",
       "<text text-anchor=\"middle\" x=\"482.29\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Hard</text>\n",
       "</g>\n",
       "<!-- 2470778609600 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2470778609600</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"577.3\" cy=\"-18\" rx=\"47.26\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"577.3\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Class: No</text>\n",
       "</g>\n",
       "<!-- 2470778608128&#45;&gt;2470778609600 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2470778608128&#45;&gt;2470778609600</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M493.36,-88.82C509.34,-75.53 531.9,-56.76 549.61,-42.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"551.75,-44.81 557.2,-35.72 547.27,-39.42 551.75,-44.81\"/>\n",
       "<text text-anchor=\"middle\" x=\"545.87\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Soft</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x23f26bdfac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.show_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "* Trees always tends to overfit\n",
    "* A technique of machine learning which reduces height of the tree by chopping off parts of the tree that's not doing anything significant in prediction\n",
    "* Prepruning & Postpruning\n",
    "  - Prepruning : Don't allow tree to grow beyond this point (`min_leaf_size`, `max_depth`, `min_impurity_decrease`) --> **implement it in the code above if some time is left**\n",
    "  - Postpruning : Allows tree to grow as much as possible, then prune the tree (`ccp_alpha`)\n",
    "\n",
    "## The cost-complexity prunning\n",
    "Cost-complexity measure:\n",
    "$$\n",
    "R_\\alpha(T)=R(T)+\\alpha∣T∣\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $R(T)$ - The misclassification error (or impurity) of the tree T on the training data.\n",
    "* $|T∣$ - The number of terminal nodes (leaves) in the tree TT, representing its complexity.\n",
    "* $\\alpha$ - A tuning parameter that controls the trade-off between the tree's accuracy and complexity.\n",
    "\n",
    "The we want to minimize the $R_\\alpha$ over the space of subtrees of given tree T. \n",
    "\n",
    "> BWT, what kind of tree we will get if set $\\alpha$ to $0, \\infty$?\n",
    "\n",
    "Greedy algorithm:\n",
    "1. Build the full tree\n",
    "2. For each node $t$ compute $R_\\alpha(T_t)$ of corresponding subtree $T_t$ and $R_\\alpha(t)$ of subtree collapsed into that node.\n",
    "3. Prune the branch, if the $R_\\alpha(T_t) < R_\\alpha(T)$\n",
    "\n",
    "> Do we need to recaclucate the $R_\\alpha$ for non-prunned nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation?\n",
    "\n",
    "import copy\n",
    "\n",
    "def compute_subtree_error(tree: Dict) -> int:\n",
    "    \"\"\"Calculate the total misclassification error of a subtree.\"\"\"\n",
    "    ...\n",
    "    return total_error\n",
    "\n",
    "def compute_subtree_leaves(tree: Dict) -> int:\n",
    "    \"\"\"Count the number of leaf nodes in a subtree.\"\"\"\n",
    "    ...\n",
    "    return total_leaves\n",
    "\n",
    "def collect_pruning_candidates(tree: Dict, candidates: list) -> None:\n",
    "    \"\"\"Collect non-leaf nodes with their effective alpha values.\"\"\"\n",
    "    ...\n",
    "\n",
    "def cost_complexity_pruning(tree: Dict, alpha: float) -> Dict:\n",
    "    \"\"\"Prune the tree using cost-complexity pruning with parameter alpha.\"\"\"\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the tree with alpha=0.1 (adjust alpha based on your needs)\n",
    "alpha = 0.1\n",
    "pruned_tree = cost_complexity_pruning(decision_tree, alpha)\n",
    "\n",
    "# Visualize the pruned tree in Jupyter Notebook\n",
    "dot_pruned = visualize_tree(pruned_tree, feature_names)\n",
    "display(dot_pruned)  # Display directly in the notebook\n",
    "\n",
    "# Test prediction with the pruned tree\n",
    "pruned_prediction = predict(pruned_tree, feature_names, new_data_point)\n",
    "print(f\"Pruned tree prediction: {pruned_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Prunnning your trees (like any other regularization technique) introduces a bias-variance tradeoff. Oversimplificated trees has larger bias (but low variance), overfitted trees has zero noise yet large variance. \n",
    "\n",
    "Random forest is one possible way to overcome this, it trains $n$ decision trees on subsets of the train data, sampled with repetitions, and then average predictions for these trees.\n",
    "\n",
    "**Why it works?** For a simple demonstration, suppose that each of your trees has variance $\\sigma^2$. As it was trained on a random subsets of the original data, it will be biased, but the distribution over this bias has zero mean and in average the overall bias is zero due to large number law. \n",
    "\n",
    "At the same time, variance of the whole ensemble is $$n\\frac{\\sigma^2}{n^2} + \\text{covariance terms}$$\n",
    "\n",
    "With growing N we have decreased bias and increased convariance terms, and we need to balance between them.\n",
    "\n",
    "> Should I usee depth constraints or pruning inside forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', max_depth=None):\n",
    "        ...\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ...\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train random forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=None)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task 25: Prunning\n",
    "Select any kind of prunning algorithms except for Cost-Complexity Prunnig (e.g. from [here](http://dspace.mit.edu/bitstream/handle/1721.1/6453/AIM-930.pdf?sequence=2)) and implement it.\n",
    "\n",
    "## Bonus Task 26: Extra trees\n",
    "\n",
    "Random forest isn't the only option to combine decision trees into ensembles. Your goal is to write a code for [`ExtraTrees`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html). In addition to that, explain the difference with RF and provide some insights on practical tips, like \n",
    "* should I use more trees?\n",
    "* should I use deeper trees?\n",
    "* etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
