{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c72e1d-c241-4026-831d-f6dbe0d70718",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58cff8e6-6a18-45f8-a111-82967cb00e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "from typing import Dict, Any, Union\n",
    "from IPython.display import display\n",
    "from graphviz import Digraph\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import load_iris, make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier as SklearnDecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a116083-b160-4d44-a4e8-426622726165",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(y: npt.ArrayLike, weights: npt.ArrayLike = None) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\"\n",
    "    counts = np.unique(y, return_counts=True)\n",
    "    if weights is None:\n",
    "        probs = counts[1] / y.shape[0]\n",
    "        ans = -np.sum(probs * np.log(probs + 1e-10))\n",
    "    else:\n",
    "        ans = 0.0\n",
    "        total_weight = weights.sum()\n",
    "        for value in counts[0]:\n",
    "            prob = weights[y == value].sum() / total_weight\n",
    "            ans -= prob * np.log(prob + 1e-10)\n",
    "    return ans\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Decision tree classifier, which can be trained, can predict class labels(miraculously) and display itself if used in a frontend environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: int = None,\n",
    "        min_samples_split: int = None,\n",
    "        min_samples_leaf: int = None,\n",
    "    ):\n",
    "        self.max_depth = -1 if max_depth is None else max_depth\n",
    "        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n",
    "        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list = None,\n",
    "        cat_features: list = None,\n",
    "        sample_weight: list = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "\n",
    "        if features is None:\n",
    "            self._features = [f\"x_{i}\" for i in range(X.shape[1])]\n",
    "        else:\n",
    "            self._features = features\n",
    "\n",
    "        if cat_features is None:\n",
    "            self._cat_features = []\n",
    "        else:\n",
    "            self._cat_features = cat_features\n",
    "\n",
    "        self._weights = sample_weight\n",
    "\n",
    "        if isinstance(y[0], str):\n",
    "            self._prediction_dtype = \"object\"\n",
    "        else:\n",
    "            self._prediction_dtype = \"int64\"\n",
    "\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = self.classes_.shape[0]\n",
    "\n",
    "        self._tree = DecisionTreeClassifier._c45_algorithm(\n",
    "            X,\n",
    "            y,\n",
    "            self._features,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            cat_features=self._cat_features,\n",
    "            weights=self._weights,\n",
    "        )\n",
    "        self._n_leaves = DecisionTreeClassifier._compute_subtree_leaves(self._tree)\n",
    "\n",
    "    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0],), dtype=self._prediction_dtype)\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self._tree\n",
    "            while \"children\" in node:\n",
    "                feature_id = self._features.index(node[\"feature\"])\n",
    "                if node[\"feature\"] in self._cat_features:\n",
    "                    node = node[\"children\"][X[i, feature_id]]\n",
    "                else:\n",
    "                    node = (\n",
    "                        node[\"children\"][f\"<={node['threshold']:0.2f}\"]\n",
    "                        if X[i, feature_id] <= node[\"threshold\"]\n",
    "                        else node[\"children\"][f\">{node['threshold']:0.2f}\"]\n",
    "                    )\n",
    "            ans[i] = node[\"majority_class\"]\n",
    "        return ans\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree construction\n",
    "    # =============================================================================\n",
    "\n",
    "    def _c45_algorithm(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        max_depth: int,\n",
    "        min_samples_split: int,\n",
    "        min_samples_leaf: int,\n",
    "        cat_features: list,\n",
    "        weights: list,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Recursively build an C4.5 decision tree.\"\"\"\n",
    "\n",
    "        majority_class = DecisionTreeClassifier._majority_class(y)\n",
    "        error = np.sum(y != majority_class)\n",
    "\n",
    "        # Base case: all samples same class\n",
    "        if len(set(y)) == 1 or X.shape[1] < 1:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Node contains not enough elements to split\n",
    "        if len(y) < min_samples_split:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        # Max tree depth is reached\n",
    "        if max_depth == 0:\n",
    "            return {\"majority_class\": majority_class, \"error\": error}\n",
    "\n",
    "        best_id, best_feature, threshold = DecisionTreeClassifier._select_best_feature(\n",
    "            X, y, features, cat_features, weights\n",
    "        )\n",
    "        feature_values = np.unique(X[:, best_id])\n",
    "        new_features = features.copy()\n",
    "        new_features.remove(best_feature)\n",
    "\n",
    "        tree = {\n",
    "            \"feature\": best_feature,\n",
    "            \"majority_class\": majority_class,\n",
    "            \"children\": {},\n",
    "            \"error\": error,\n",
    "        }\n",
    "\n",
    "        if best_feature in cat_features:\n",
    "            for value in feature_values:\n",
    "                mask = X[:, best_id] == value\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[mask],\n",
    "                    y[mask],\n",
    "                    weights[mask],\n",
    "                )\n",
    "                if len(y_sub) < min_samples_leaf:\n",
    "                    child_majority_class = DecisionTreeClassifier._majority_class(y_sub)\n",
    "                    tree[\"children\"][value] = {\n",
    "                        \"majority_class\": child_majority_class,\n",
    "                        \"error\": np.sum(y_sub != child_majority_class),\n",
    "                    }\n",
    "                else:\n",
    "                    tree[\"children\"][value] = DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights,\n",
    "                    )\n",
    "        else:\n",
    "            mask = X[:, best_id] <= threshold\n",
    "            not_mask = np.logical_not(mask)\n",
    "            if (\n",
    "                np.sum(mask) >= min_samples_leaf\n",
    "                and (mask.shape[0] - np.sum(mask)) >= min_samples_leaf\n",
    "            ):\n",
    "                tree[\"threshold\"] = threshold\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[mask],\n",
    "                    y[mask],\n",
    "                    weights[mask],\n",
    "                )\n",
    "                tree[\"children\"][f\"<={threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights_sub,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                X_sub, y_sub, weights_sub = (\n",
    "                    np.delete(X, best_id, axis=1)[not_mask],\n",
    "                    y[not_mask],\n",
    "                    weights[not_mask],\n",
    "                )\n",
    "                tree[\"children\"][f\">{threshold:0.2f}\"] = (\n",
    "                    DecisionTreeClassifier._c45_algorithm(\n",
    "                        X_sub,\n",
    "                        y_sub,\n",
    "                        new_features,\n",
    "                        max_depth=max_depth - 1,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        min_samples_split=min_samples_split,\n",
    "                        cat_features=cat_features,\n",
    "                        weights=weights_sub,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def _majority_class(y: npt.ArrayLike):\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "    def _gain_function(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        feature_idx: int,\n",
    "        cat_feature: bool,\n",
    "        weights: npt.ArrayLike,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate gain for a given feature.\"\"\"\n",
    "\n",
    "        if cat_feature is None or cat_feature == False:\n",
    "\n",
    "            def target_fn(\n",
    "                theta: float, X: npt.ArrayLike, y: npt.ArrayLike, weights: npt.ArrayLike\n",
    "            ):\n",
    "                mask_left = X[:, feature_idx] <= theta\n",
    "                mask_right = X[:, feature_idx] > theta\n",
    "                # raise ValueError(\n",
    "                #     f\"mask:{mask_left.shape}\\nw:{weights.shape}\\ny:{y.shape}\\n{weights[mask_left].shape}\"\n",
    "                # )\n",
    "                if weights is None:\n",
    "                    return mask_left.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_left],\n",
    "                        weights=weights,\n",
    "                    ) + mask_right.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_right],\n",
    "                        weights=weights,\n",
    "                    )\n",
    "                else:\n",
    "                    return mask_left.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_left],\n",
    "                        weights=weights[mask_left],\n",
    "                    ) + mask_right.sum() / y.shape[0] * entropy(\n",
    "                        y[mask_right],\n",
    "                        weights=weights[mask_right],\n",
    "                    )\n",
    "\n",
    "            target_fn0 = lambda T: target_fn(T, X, y, weights)\n",
    "\n",
    "            minimize_result = minimize(target_fn0, x0=np.mean(X[:, feature_idx]))\n",
    "            ans = {\"value\": minimize_result.fun, \"threshold\": minimize_result.x[0]}\n",
    "        else:\n",
    "            values, counts = np.unique(X[:, feature_idx], return_counts=True)\n",
    "            probs = counts / y.shape[0]\n",
    "            entropies = np.array(\n",
    "                list(\n",
    "                    map(\n",
    "                        lambda x: entropy(\n",
    "                            y[X[:, feature_idx] == x],\n",
    "                            weights=weights[X[:, feature_idx] == x],\n",
    "                        ),\n",
    "                        values,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            ans = {\"value\": np.sum(probs * entropies), \"threshold\": None}\n",
    "\n",
    "        return ans\n",
    "\n",
    "    def _select_best_feature(\n",
    "        X: npt.ArrayLike,\n",
    "        y: npt.ArrayLike,\n",
    "        features: list,\n",
    "        cat_features: list,\n",
    "        weights: list,\n",
    "    ) -> list:\n",
    "        \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "        gains = [\n",
    "            DecisionTreeClassifier._gain_function(\n",
    "                X, y, i, feature in cat_features, weights\n",
    "            )\n",
    "            for i, feature in enumerate(features)\n",
    "        ]\n",
    "        best_idx = np.argmin(gain[\"value\"] for gain in gains)\n",
    "        return [best_idx, features[best_idx], gains[best_idx][\"threshold\"]]\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree pruning\n",
    "    # =============================================================================\n",
    "\n",
    "    def get_pruned_tree(self, alpha: float) -> Dict:\n",
    "        \"\"\"Prune the tree using cost-complexity pruning with parameter `alpha`.\"\"\"\n",
    "        ans = DecisionTreeClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "        )\n",
    "        ans._tree = deepcopy(self._tree)\n",
    "        ans._features = deepcopy(self._features)\n",
    "        ans._n_leaves = self._n_leaves\n",
    "        ans._cat_features = deepcopy(self._cat_features)\n",
    "        ans._prediction_dtype = deepcopy(self._prediction_dtype)\n",
    "        return ans\n",
    "\n",
    "    def prune_tree(self, alpha: float) -> None:\n",
    "        \"\"\"Prune the underlying decision tree using cost-complexity pruning with predefines `alpha`.\"\"\"\n",
    "        self._tree = DecisionTreeClassifier._cost_comprexity_pruning(\n",
    "            self._tree, alpha, inplace=True\n",
    "        )\n",
    "\n",
    "    def _compute_subtree_error(tree: Dict) -> int:\n",
    "        \"\"\"Calculate the total misclassification error of a (sub)tree.\"\"\"\n",
    "\n",
    "        if not \"children\" in tree:\n",
    "            return tree[\"error\"]\n",
    "\n",
    "        total_error = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_error += DecisionTreeClassifier._compute_subtree_error(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_error\n",
    "\n",
    "    def _compute_subtree_leaves(tree: Dict) -> int:\n",
    "        \"\"\"Count the number of leaf nodes in a (sub)tree.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return 1\n",
    "\n",
    "        total_leaves = 0\n",
    "        for child in tree[\"children\"]:\n",
    "            total_leaves += DecisionTreeClassifier._compute_subtree_leaves(\n",
    "                tree[\"children\"][child]\n",
    "            )\n",
    "\n",
    "        return total_leaves\n",
    "\n",
    "    def _collect_pruning_candidates(tree: Dict, candidates: list) -> None:\n",
    "        \"\"\"Collect non-leaf nodes with their effective alpha values.\"\"\"\n",
    "        if not \"children\" in tree:\n",
    "            return candidates\n",
    "\n",
    "        subtree_error = DecisionTreeClassifier._compute_subtree_error(tree)\n",
    "        complexity_error = DecisionTreeClassifier._compute_subtree_leaves(tree)\n",
    "        R = tree[\"error\"]\n",
    "        effective_alpha = (R - subtree_error) / complexity_error\n",
    "\n",
    "        for child in tree[\"children\"]:\n",
    "            DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree[\"children\"][child], candidates\n",
    "            )\n",
    "\n",
    "        candidates.append((tree, effective_alpha))\n",
    "\n",
    "        return candidates\n",
    "\n",
    "    def _cost_comprexity_pruning(self, alpha: float, inplace: bool = None) -> dict:\n",
    "        if inplace is True:\n",
    "            tree_to_prune = deepcopy(self._tree)\n",
    "        else:\n",
    "            tree_to_prune = self._tree\n",
    "        while True:\n",
    "            candidates = []\n",
    "            candidates = DecisionTreeClassifier._collect_pruning_candidates(\n",
    "                tree_to_prune, candidates\n",
    "            )\n",
    "            candidates.sort(key=lambda x: x[1])\n",
    "\n",
    "            if not candidates:\n",
    "                break\n",
    "\n",
    "            weakest_subtree, weakest_alpha = candidates[0]\n",
    "\n",
    "            if weakest_alpha > alpha:\n",
    "                break\n",
    "\n",
    "            weakest_subtree[\"children\"] = {}\n",
    "            weakest_subtree.pop(\"feature\")\n",
    "\n",
    "        return tree_to_prune\n",
    "\n",
    "    # =============================================================================\n",
    "    # Tree visualization\n",
    "    # =============================================================================\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = DecisionTreeClassifier._visualize_tree(self._tree, self._features)\n",
    "        display(dot)\n",
    "\n",
    "    def _visualize_tree(\n",
    "        tree: Dict[str, Any],\n",
    "        feature_names: list,\n",
    "        dot: Digraph = None,\n",
    "        parent: str = None,\n",
    "        edge_label: str = None,\n",
    "    ) -> Digraph:\n",
    "        \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "        if dot is None:\n",
    "            dot = Digraph(comment=\"Decision Tree\")\n",
    "\n",
    "        # Create a unique node ID\n",
    "        node_id = str(id(tree))\n",
    "\n",
    "        # Add the current node\n",
    "        if not \"children\" in tree:\n",
    "            node_label = f\"Class: {tree['majority_class']}\\nError: {tree['error']}\"\n",
    "        else:\n",
    "            node_label = f\"Feature: {tree['feature']}\\nError: {tree['error']}\"\n",
    "        dot.node(node_id, node_label)\n",
    "\n",
    "        # Connect to parent node if exists\n",
    "        if parent is not None:\n",
    "            dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "        # Recursively add children\n",
    "        if \"children\" in tree:\n",
    "            for value, child in tree[\"children\"].items():\n",
    "                DecisionTreeClassifier._visualize_tree(\n",
    "                    child, feature_names, dot, node_id, str(value)\n",
    "                )\n",
    "\n",
    "        return dot\n",
    "\n",
    "    # =============================================================================\n",
    "    # Utility functions\n",
    "    # ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e363d42a-10f1-4316-9783-876e1ff11c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifierScratch(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator: object, n_estimators: int = None):\n",
    "        self.n_estimators = 50 if n_estimators is None else n_estimators\n",
    "        self.estimator = DecisionTreeClassifier if estimator is None else estimator\n",
    "        self.classifier_weights_ = np.zeros(self.n_estimators)\n",
    "        self.classifiers_ = [self.estimator for i in range(self.n_estimators)]\n",
    "\n",
    "    def fit(self, X: npt.ArrayLike, y: npt.ArrayLike):\n",
    "        n_samples = X.shape[0]\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.classifier_sample_weights_ = np.zeros((self.n_estimators, n_samples))\n",
    "        self.classifier_sample_weights_[0, :] = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            self.classifiers_[i].fit(\n",
    "                X, y, sample_weight=self.classifier_sample_weights_[i, :]\n",
    "            )\n",
    "            print(f\"#{i+1}...OK!\")\n",
    "            preds = self.classifiers_[i].predict(X)\n",
    "            errors_mask = preds != y\n",
    "\n",
    "            # the error term is artificially increased so that the method continues working if classifier does perfect classification\n",
    "            classifier_error = (\n",
    "                np.sum(self.classifier_sample_weights_[i, errors_mask])\n",
    "                / self.classifier_sample_weights_[i, :].sum()\n",
    "            ) + 1e-10\n",
    "\n",
    "            print(f\"Error term: {classifier_error:0.2f}\")\n",
    "            if classifier_error < 1.0 - 1.0 / self.classes_.shape[0]:\n",
    "                self.classifier_weights_[i] = (\n",
    "                    np.log(1.0 - classifier_error)\n",
    "                    - np.log(classifier_error)\n",
    "                    + np.log(self.classes_.shape[0] - 1.0)\n",
    "                )\n",
    "                if i < self.n_estimators - 1:\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        self.classifier_sample_weights_[i, :]\n",
    "                        * np.exp(self.classifier_weights_[i] * errors_mask)\n",
    "                    )\n",
    "\n",
    "                    # Normalize the weights\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        self.classifier_sample_weights_[i + 1, :]\n",
    "                        / self.classifier_sample_weights_[i + 1, :].sum()\n",
    "                    )\n",
    "            else:\n",
    "                print(\"Bad error value. Resetting the weights...\")\n",
    "                self.classifier_weights_[i] = 0.0\n",
    "                if i < self.n_estimators - 1:\n",
    "                    self.classifier_sample_weights_[i + 1, :] = (\n",
    "                        np.ones(n_samples) / n_samples\n",
    "                    )\n",
    "\n",
    "    def predict(self, X):\n",
    "        cls_predictions = np.zeros((self.n_estimators, X.shape[0]))\n",
    "        for cls_num, cls in enumerate(self.classifiers_):\n",
    "            cls_predictions[cls_num, :] = cls.predict(X)\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for i in range(predictions.shape[0]):\n",
    "            predictions[i] = self.classes_[\n",
    "                np.argmax(\n",
    "                    [\n",
    "                        (\n",
    "                            self.classifier_weights_\n",
    "                            * (cls_predictions[:, i] == pred_class)\n",
    "                        ).sum(axis=0)\n",
    "                        for pred_class in self.classes_\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d9670e-5e87-4cc3-b6f6-a2cdfe97a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find weights for scatter plot\n",
    "# def get_sample_weights_per_stage(ada, X, y):\n",
    "#     \"\"\"Recalculate sample weights at each stage of boosting\"\"\"\n",
    "#     n_samples = len(X)\n",
    "#     weights = np.ones(n_samples) / n_samples\n",
    "#     stage_weights = []\n",
    "\n",
    "#     for estimator, alpha in zip(ada.estimators_, ada.estimator_weights_):\n",
    "#         stage_weights.append(weights.copy())\n",
    "#         pred = estimator.predict(X)\n",
    "#         incorrect = (y != pred).astype(\"float\")\n",
    "#         weights *= np.exp(alpha * incorrect)\n",
    "#         weights = weights / weights.sum()\n",
    "\n",
    "#     return stage_weights\n",
    "\n",
    "\n",
    "# stage_weights = get_sample_weights_per_stage(ada, X_train, y_train)\n",
    "# stage_weights\n",
    "# stage_weights = [np.ones_like(y) for clf in ada.estimators_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377eecd5-66f2-4d8d-b295-a33bdae41f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_stages(estimator, weights_per_stage):\n",
    "#     n_estimators = len(estimator.estimators_)\n",
    "#     fig, axes = plt.subplots(n_estimators, 2, figsize=(12, 4 * n_estimators))\n",
    "#     xx, yy = np.meshgrid(np.linspace(-2, 3, 300), np.linspace(-2, 2, 300))\n",
    "#     X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "#     cumulative_pred = np.zeros_like(xx)\n",
    "#     for i in range(n_estimators):\n",
    "#         ax_left, ax_right = axes[i]\n",
    "#         tree = estimator.estimators_[i]\n",
    "#         weight = estimator.estimator_weights_[i]\n",
    "#         sample_weights = weights_per_stage[i]\n",
    "#         point_sizes = 200 * sample_weights / np.max(sample_weights)\n",
    "#         pred_grid = tree.predict(X_grid).reshape(xx.shape)\n",
    "#         cumulative_pred += weight * pred_grid\n",
    "\n",
    "#         # -- Left: cumulative raw score as heatmap + boundary --\n",
    "#         im = ax_left.contourf(\n",
    "#             xx, yy, cumulative_pred, levels=100, cmap=\"RdBu\", alpha=0.5\n",
    "#         )\n",
    "#         ax_left.contour(\n",
    "#             xx, yy, cumulative_pred, levels=[0], colors=\"k\", linewidths=1.2\n",
    "#         )  # decision boundary\n",
    "#         ax_left.scatter(\n",
    "#             X_train[:, 0],\n",
    "#             X_train[:, 1],\n",
    "#             c=y_train,\n",
    "#             s=point_sizes,\n",
    "#             edgecolors=\"black\",\n",
    "#             # cmap=\"bwr\",\n",
    "#         )\n",
    "#         ax_left.set_title(f\"Cumulative after {i+1} learners\\nWeight = {weight:.2f}\")\n",
    "\n",
    "#         # -- Right: single estimator prediction --\n",
    "#         ax_right.contourf(xx, yy, pred_grid, levels=100, cmap=\"RdBu\", alpha=0.5)\n",
    "#         ax_right.contour(xx, yy, pred_grid, levels=[0], colors=\"k\", linewidths=1.2)\n",
    "#         ax_right.scatter(\n",
    "#             X_train[:, 0],\n",
    "#             X_train[:, 1],\n",
    "#             c=y_train,\n",
    "#             s=point_sizes,\n",
    "#             edgecolors=\"black\",\n",
    "#             # cmap=\"RdBu\",\n",
    "#         )\n",
    "#         ax_right.set_title(f\"Estimator {i+1} prediction\\nWeight = {weight:.2f}\")\n",
    "#         # break\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# plot_stages(ada, stage_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909877b4-6674-40af-b8f1-f5c44b96bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate 2D data\n",
    "# X, y = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
    "# y = 2 * y - 1  # Convert to {-1, 1}\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529d9c94-6c4d-4493-bf5c-a9888b3dc5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada1 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=4,\n",
    ")\n",
    "ada1.fit(X_train, y_train)\n",
    "y_pred_sklearn1 = ada1.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_sklearn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5f3581-2a42-4b9f-a4de-922da7bb8172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada2 = AdaBoostClassifier(\n",
    "    estimator=SklearnDecisionTreeClassifier(),\n",
    "    n_estimators=4,\n",
    ")\n",
    "ada2.fit(X_train, y_train)\n",
    "y_pred_sklearn2 = ada2.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_sklearn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e921d44-7603-4146-bd7e-aff07edd90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1...OK!\n",
      "Error term: 0.06\n",
      "#2...OK!\n",
      "Error term: 0.67\n",
      "#3...OK!\n",
      "Error term: 0.67\n",
      "Bad error value. Resetting the weights...\n",
      "#4...OK!\n",
      "Error term: 0.06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scratch_boost1 = AdaBoostClassifierScratch(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=4,\n",
    ")\n",
    "scratch_boost1.fit(X_train, y_train)\n",
    "y_pred1 = scratch_boost1.predict(X_test)\n",
    "accuracy_score(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa52de73-a829-4101-87af-7407713de93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1...OK!\n",
      "Error term: 0.00\n",
      "#2...OK!\n",
      "Error term: 0.00\n",
      "#3...OK!\n",
      "Error term: 0.00\n",
      "#4...OK!\n",
      "Error term: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scratch_boost2 = AdaBoostClassifierScratch(\n",
    "    estimator=SklearnDecisionTreeClassifier(),\n",
    "    n_estimators=4,\n",
    ")\n",
    "scratch_boost2.fit(X_train, y_train)\n",
    "y_pred2 = scratch_boost2.predict(X_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca467b-dfd1-4c57-ac89-20c3aa2c85ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
