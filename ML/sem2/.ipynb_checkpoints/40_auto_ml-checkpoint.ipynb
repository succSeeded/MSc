{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "| Library          | Method                     | Key Features                                                                 |\n",
    "|------------------|----------------------------|------------------------------------------------------------------------------|\n",
    "| Hyperopt         | Bayesian optimization (TPE) | Uses Tree-structured Parzen Estimator (TPE) to model promising parameters.    |\n",
    "| Optuna           | Bayesian optimization (TPE/CMA-ES) | Supports pruning, parallel trials, and dynamic search spaces.           |\n",
    "| Scikit-Optimize  | Bayesian optimization (GP) | Uses Gaussian Processes (GP) as a surrogate model.                            |\n",
    "\n",
    "## CMA-ES (Covariance Matrix Adaptation Evolution Strategy)\n",
    "\n",
    "1. Samples Points: At each iteration, it generates a population of candidate solutions (points) from a multivariate Gaussian distribution (defined by a mean and covariance matrix).\n",
    "2. Evaluates Fitness: Ranks the points based on how well they perform \n",
    "4. Updates Distribution:\n",
    "    * Mean: Moves toward better-performing points.\n",
    "    * Covariance Matrix: Adapts to \"stretch\" the distribution toward promising directions (learning correlations between variables).\n",
    "    * Step Size: Adjusts globally (large steps early, smaller steps as it converges).\n",
    "\n",
    "5. Repeat\n",
    " \n",
    "## TPE (Tree Parzen Estimator)\n",
    "> Remember the class on kernel density estimations??\n",
    "\n",
    "1. Samples Points Randomly (First Phase):\n",
    "2. Builds two groups: \"Good\" (e.g. lowest 10%) and \"Bad\" (e.g. top 10%) points.\n",
    "3. Estimates probability density for these two groups\n",
    "4. Optimizes the \"Promisingness\" Ratio: $\\frac{\\mathbb{P}(\\text{good}|x)}{ \\mathbb{P}(\\text{bad}|x)}$\n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_slice,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "X = X[:5000]\n",
    "y = y[:5000]\n",
    "feature_names = X.columns\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "df_data = X\n",
    "df_data[\"target\"] = y\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-9, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-9, 10.0, log=True),\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params, n_estimators=1000)\n",
    "    score = cross_val_score(\n",
    "        model, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    "    ).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", storage=\"sqlite:///optuna.db\")\n",
    "study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "print(\"Best trial AUC:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna-dashboard\n",
    "# !pip install optuna-dashboard\n",
    "# optuna-dashboard sqlite:///optuna.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML\n",
    "After participating in several competitions, you may have noticed that you keep repeating the same steps: reading the dataset, filtering, converting data types, splitting into train-val-test, initializing a model, training on the train set (or even on KFold splits), tuning hyperparameters, generating predictions, saving results... These tasks feel so routine that they perform should definitely be automated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h2o.H2OFrame(df_data)\n",
    "aml = H2OAutoML(max_models=10, seed=42, max_runtime_secs=120)\n",
    "aml.train(x=data.columns[:-1], y=\"target\", training_frame=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard\n",
    "print(lb.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightAutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdg6WD8O_-Z-"
   },
   "source": [
    "\n",
    "**LightAutoML** was created for exactly this purpose—it’s a Sber project. Here’s what it offers:  \n",
    "- **Automatic hyperparameter tuning and data processing**  \n",
    "- **Automatic type detection and feature selection**  \n",
    "- **Efficient time management**  \n",
    "- **Automated report generation**  \n",
    "- **A modular, user-friendly framework for building custom pipelines**  \n",
    "\n",
    "To give you a taste of its power:  \n",
    "[**0.77 accuracy on Titanic with just 5 lines of code (excluding imports)**](https://www.kaggle.com/code/alexryzhkov/lightautoml-extreme-short-titanic-solution)  \n",
    "\n",
    "(Note: I kept the link formatting as-is since it appears to be a reference to external content. If this is meant to be embedded differently, let me know!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP0tMgvT8CDu"
   },
   "outputs": [],
   "source": [
    "# !pip install lightautoml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_72zYMVIYT8"
   },
   "source": [
    "[Cannot run in Colab: python3.10 not supported](https://github.com/sb-ai-lab/LightAutoML/issues/87#issuecomment-1527406824)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "sLyoUvRtEpJO",
    "outputId": "d8a6dd7d-2358-4be2-ac77-e1c78e25ef53"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "# Essential DS libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# LightAutoML presets, task and report generation\n",
    "from lightautoml.report.report_deco import ReportDeco\n",
    "from lightautoml.tasks import Task\n",
    "from lightautoml.automl.presets.tabular_presets import (\n",
    "    TabularAutoML,\n",
    "    TabularUtilizedAutoML,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uRPiot8GeG4"
   },
   "outputs": [],
   "source": [
    "N_THREADS = 6\n",
    "N_FOLDS = 5\n",
    "\n",
    "RANDOM_STATE = 261\n",
    "TEST_SIZE = 0.2\n",
    "TIMEOUT = 300\n",
    "\n",
    "TARGET_NAME = \"TARGET\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./data/\"\n",
    "DATASET_NAME = \"sampled_app_train.csv\"\n",
    "DATASET_FULLNAME = os.path.join(DATASET_DIR, DATASET_NAME)\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/AILab-MLTools/LightAutoML/master/examples/data/sampled_app_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_FULLNAME):\n",
    "\n",
    "    os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "    dataset = requests.get(DATASET_URL).text\n",
    "\n",
    "    with open(DATASET_FULLNAME, \"w\", encoding=\"utf-8\") as output:\n",
    "        output.write(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATASET_DIR + DATASET_NAME)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data, test_size=TEST_SIZE, stratify=data[TARGET_NAME], random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Data is splitted. Parts sizes: train_data = {train_data.shape}, test_data = {test_data.shape}\"\n",
    ")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following task types are available:\n",
    "\n",
    "* 'binary' - for binary classification.\n",
    "* 'reg’ - for regression.\n",
    "*  ‘multiclass’ - for multiclass classification.\n",
    "* 'multi:reg - for multiple regression.\n",
    "* 'multilabel' - for multi-label classification.\n",
    "\n",
    "More [here](https://lightautoml.readthedocs.io/en/latest/pages/modules/generated/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = Task(\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library infers the types automatically, but they can be overriden:\n",
    "\n",
    "* 'numeric' - numerical feature\n",
    "* 'category' - categorical feature\n",
    "* 'text' - text data\n",
    "* 'datetime' - features with date and time\n",
    "* 'date' - features with date only\n",
    "* 'group' - features by which the data can be divided into groups and which can be taken into account for group k-fold validation (so the same group is not represented in both testing and training sets)\n",
    "* 'drop' - features to drop, they will not be used in model building\n",
    "* 'weights' - object weights for the loss and metric\n",
    "* 'path' - image file paths (for CV tasks)\n",
    "* 'treatment' - object group in uplift modelling tasks: treatment or control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles = {\"target\": TARGET_NAME, \"drop\": [\"SK_ID_CURR\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline \n",
    "![img](https://raw.githubusercontent.com/sb-ai-lab/LightAutoML/master/imgs/tutorial_1_laml_big.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample stage:\n",
    "![](https://raw.githubusercontent.com/sb-ai-lab/LightAutoML/ac3c1b38873437eb74354fb44e68a449a0200aa6/imgs/tutorial_blackbox_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = TabularAutoML(\n",
    "    task=task,\n",
    "    timeout=TIMEOUT,\n",
    "    cpu_limit=N_THREADS,\n",
    "    reader_params={\"n_jobs\": N_THREADS, \"cv\": N_FOLDS, \"random_state\": RANDOM_STATE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out_of_fold_predictions = automl.fit_predict(train_data, roles=roles, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_predictions = automl.predict(test_data)\n",
    "print(\n",
    "    f\"Prediction for test_data:\\n{test_predictions}\\nShape = {test_predictions.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"OOF score: {roc_auc_score(train_data[TARGET_NAME].values, out_of_fold_predictions.data[:, 0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"HOLDOUT score: {roc_auc_score(test_data[TARGET_NAME].values, test_predictions.data[:, 0])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(automl.create_model_str_desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RD = ReportDeco(output_path=\"tabularAutoML_model_report\")\n",
    "\n",
    "automl_rd = RD(\n",
    "    TabularAutoML(\n",
    "        task=task,\n",
    "        timeout=60,\n",
    "        cpu_limit=N_THREADS,\n",
    "        reader_params={\n",
    "            \"n_jobs\": N_THREADS,\n",
    "            \"cv\": N_FOLDS,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "out_of_fold_predictions = automl_rd.fit_predict(train_data, roles=roles, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls tabularAutoML_model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_predictions = automl_rd.predict(test_data)\n",
    "print(\n",
    "    f\"Prediction for test_data:\\n{test_predictions}\\nShape = {test_predictions.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"OOF score: {roc_auc_score(train_data[TARGET_NAME].values, out_of_fold_predictions.data[:, 0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"HOLDOUT score: {roc_auc_score(test_data[TARGET_NAME].values, test_predictions.data[:, 0])}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
