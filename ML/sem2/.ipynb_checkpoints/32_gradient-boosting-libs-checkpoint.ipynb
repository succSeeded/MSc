{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Libraries\n",
    "\n",
    "In the previous class, we finished with implementing the classical **Adaptive Boosting** algorithm for binary classification. Essentially, this algorithm **greedy minimizes** specific exponential loss. The natural expansion of this idea is the **Gradient Boosting** algorithm, in which every new estimator is trained to move the loss toward its -gradient. We won't implement the **Gradient Boosting** algorithm as it will be a part of your next homework. Instead, today we will focus on available libraries and their APIs.\n",
    "\n",
    "|                       | Scikit-Learn GradientBoostingX                                                                                 | XGBoost                                                                                             | LightGBM                                                                                               | CatBoost                                                                                               |\n",
    "|-----------------------------|----------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\n",
    "| **Developer**               | Scikit-Learn Community                                                                                         | DMLC                                                                                                | Microsoft                                                                                              | Yandex                                                                                                 |\n",
    "| **Release Year**            | 2007                                                                                                           | 2014                                                                                                | 2016                                                                                                  | 2017                                                                                                   |\n",
    "| **Tree Growth Strategy**    | Level-wise growth                                                                                             | Level-wise growth                                                                                   | Leaf-wise growth                                                                                       | Symmetric trees                                                                                        |\n",
    "| **Categorical Feature Handling** | Requires manual preprocessing (e.g., one-hot encoding)                                                       | Requires preprocessing (e.g., one-hot encoding or label encoding)                                   | Supports categorical features with manual specification                                                | Native handling of categorical features                                                                |\n",
    "| **Missing Value Handling**  | Requires manual imputation                                                                                     | Natively supports missing values                                                                    | Natively supports missing values                                                                       | Natively supports missing values                                                                       |\n",
    "| **Training Speed**          | Generally slower compared to others                                                                            | Faster than Scikit-Learn but can be slower than LightGBM and CatBoost                               | Optimized for speed; typically faster on large datasets                                                | Competitive training speed; optimized for datasets with categorical features                           |\n",
    "| **Regularization Techniques** | Supports L2 regularization and early stopping                                                                  | Supports L1 and L2 regularization, shrinkage, and early stopping                                    | Supports L1 and L2 regularization, early stopping, and exclusive feature bundling                      | Utilizes ordered boosting and L2 regularization to prevent overfitting                                 |\n",
    "| **Parallel and GPU Support** | Supports parallelism; no native GPU support                                                                    | Supports parallel and distributed computing; GPU acceleration available                             | Supports parallel and distributed computing; optimized for GPU acceleration                            | Supports CPU and GPU training; optimized implementations for both                                       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose dataset that is large enought to see perfomance difference\n",
    "X, y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "feature_names = X.columns\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_regressor = GradientBoostingRegressor(\n",
    "    n_estimators=200, learning_rate=0.1, max_depth=3, random_state=42\n",
    ")\n",
    "\n",
    "hist_regressor = HistGradientBoostingRegressor(\n",
    "    early_stopping=False, max_iter=200, learning_rate=0.1, max_depth=3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gb_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# No sort!\n",
    "hist_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_train_pred = gb_regressor.predict(X_train)\n",
    "gb_test_pred = gb_regressor.predict(X_test)\n",
    "hist_train_pred = hist_regressor.predict(X_train)\n",
    "hist_test_pred = hist_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "def print_metrics(name, y_train, y_test, train_pred, test_pred):\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"Train explained variance: {r2_score(y_train, train_pred):.4f}\")\n",
    "    print(f\"Test explained variance: {r2_score(y_test, test_pred):.4f}\")\n",
    "    print(f\"Train MSE: {mean_squared_error(y_train, train_pred):.2f}\")\n",
    "    print(f\"Test MSE: {mean_squared_error(y_test, test_pred):.2f}\")\n",
    "    print(f\"Train MAE: {mean_absolute_error(y_train, train_pred):.2f}\")\n",
    "    print(f\"Test MAE: {mean_absolute_error(y_test, test_pred):.2f}\")\n",
    "\n",
    "\n",
    "print_metrics(\"Gradient Boosting\", y_train, y_test, gb_train_pred, gb_test_pred)\n",
    "print_metrics(\"HistGradient Boosting\", y_train, y_test, hist_train_pred, hist_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "## TODO: how feature importances are calculated in GMB??\n",
    "sorted_idx = gb_regressor.feature_importances_.argsort()\n",
    "plt.barh(\n",
    "    np.array(feature_names)[sorted_idx], gb_regressor.feature_importances_[sorted_idx]\n",
    ")\n",
    "plt.title(\"Gradient Boosting Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "\n",
    "# NO IMPORTANCES FOR HIST GB!\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sorted_idx = hist_regressor.feature_importances_.argsort()\n",
    "# plt.barh(np.array(feature_names)[sorted_idx], hist_regressor.feature_importances_[sorted_idx])\n",
    "# plt.title(\"HistGradient Boosting Feature Importance\")\n",
    "# plt.xlabel(\"Importance Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For HistGradientBoosting we can use black-box importance estimators, and they are oftern preferred over impurity-based importances.\n",
    "\n",
    "> What are the problems with impurity-based importances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(\n",
    "    hist_regressor, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Sort and plot\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(\n",
    "    range(len(sorted_idx)),\n",
    "    result.importances_mean[sorted_idx],\n",
    "    xerr=result.importances_std[sorted_idx],\n",
    "    align=\"center\",\n",
    ")\n",
    "plt.yticks(range(len(sorted_idx)), np.array(feature_names)[sorted_idx])\n",
    "plt.xlabel(\"Mean decrease in R^2 score (permutation importance)\")\n",
    "plt.title(\"Permutation Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    hist_regressor,\n",
    "    X_train,\n",
    "    features=[np.where(np.array(feature_names) == \"Latitude\")[0][0]],\n",
    "    ax=ax,\n",
    "    grid_resolution=50,\n",
    "    line_kw={\"color\": \"red\"},\n",
    ")\n",
    "plt.title(\"Partial Dependence Plot for BMI (HistGradient Boosting)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plot_tree(\n",
    "    gb_regressor.estimators_[1, 0],\n",
    "    feature_names=feature_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    proportion=True,\n",
    "    node_ids=True,\n",
    "    impurity=False,\n",
    "    precision=2,\n",
    ")\n",
    "plt.title(\"First Tree in Gradient Boosting Ensemble\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learnign Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# TODO: unfit before running\n",
    "\n",
    "gb_train_sizes, gb_train_scores, gb_test_scores = learning_curve(\n",
    "    gb_regressor,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    ")\n",
    "\n",
    "hist_train_sizes, hist_train_scores, hist_test_scores = learning_curve(\n",
    "    hist_regressor,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(gb_train_sizes, -gb_train_scores.mean(axis=1), label=\"[GB] Train MSE\")\n",
    "plt.plot(gb_train_sizes, -gb_test_scores.mean(axis=1), label=\"[GB] Validation MSE\")\n",
    "\n",
    "plt.plot(hist_train_sizes, -hist_train_scores.mean(axis=1), label=\"[HIST] Train MSE\")\n",
    "plt.plot(\n",
    "    hist_train_sizes, -hist_test_scores.mean(axis=1), label=\"[HIST] Validation MSE\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Learning Curve for Gradient Boosting\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    alpha=0.1,\n",
    "    # lambda=0.1,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],  # Validation data\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_test = model.predict(X_test)\n",
    "pred_train = model.predict(X_train)\n",
    "\n",
    "print_metrics(\"XGBoost\", y_train, y_test, pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature Importances:\")\n",
    "for name, score in zip(feature_names, model.feature_importances_):\n",
    "    print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "xgb.plot_importance(model, importance_type=\"weight\", ylabel=np.array(feature_names))\n",
    "plt.title(\"Feature Importance (Weight)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(model, num_trees=0, rankdir=\"LR\")\n",
    "plt.title(\"First Tree in Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset wrappers\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lambda\": 0.1,\n",
    "}\n",
    "\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    metrics={\"rmse\"},\n",
    "    early_stopping_rounds=50,\n",
    "    seed=42,\n",
    "    as_pandas=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCross-Validation Results:\")\n",
    "print(cv_results.tail())\n",
    "print(f\"\\nBest RMSE: {cv_results['test-rmse-mean'].min():.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "results = model.evals_result()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results[\"validation_0\"][\"rmse\"], label=\"Validation RMSE\")\n",
    "plt.xlabel(\"Boosting Round\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_model(\"xgb_regression_model.json\")\n",
    "\n",
    "# Load model for later use\n",
    "loaded_model = xgb.XGBRegressor()\n",
    "loaded_model.load_model(\"xgb_regression_model.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Wrappers!\n",
    "train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_names.tolist())\n",
    "test_data = lgb.Dataset(\n",
    "    X_test, label=y_test, reference=train_data, feature_name=feature_names.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"num_leaves\": 31,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print_metrics(\"LGB\", y_train, y_test, pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "importance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": feature_names,\n",
    "        \"split_importance\": model.feature_importance(importance_type=\"split\"),\n",
    "        \"gain_importance\": model.feature_importance(importance_type=\"gain\"),\n",
    "    }\n",
    ").sort_values(\"gain_importance\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance Table:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Feature Importance Plot (Gain-based)\n",
    "lgb.plot_importance(\n",
    "    model,\n",
    "    importance_type=\"gain\",\n",
    "    max_num_features=10,\n",
    "    title=\"Gain-based Feature Importance\",\n",
    ")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times feature is used to split\n",
    "lgb.plot_importance(\n",
    "    model,\n",
    "    importance_type=\"split\",\n",
    "    max_num_features=10,\n",
    "    title=\"Split-based Feature Importance\",\n",
    ")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Visualization\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "lgb.plot_tree(model, tree_index=3, ax=ax, figsize=(18, 12))\n",
    "plt.title(\"Example Decision Tree Structure\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pools for efficient memory handling\n",
    "train_pool = cb.Pool(X_train, y_train)\n",
    "test_pool = cb.Pool(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cb.CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"R2\",\n",
    "    random_seed=42,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    plot=True,  # Displays live training metrics in notebook\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = model.predict(X_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print_metrics(\"CatBoost\", y_train, y_test, pred_train, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importance, y=feature_names)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evals_result_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results[\"learn\"][\"RMSE\"], label=\"Train RMSE\")\n",
    "plt.plot(results[\"validation\"][\"RMSE\"], label=\"Validation RMSE\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Training History\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: this is broken\n",
    "# model.plot_tree(tree_idx=24, pool=train_pool)\n",
    "# plt.title(\"First Tree Structure\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "cv_params = model.get_params()\n",
    "cv_data = cb.cv(train_pool, cv_params, fold_count=5, plot=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_model(\"catboost_regression_model.cbm\")\n",
    "\n",
    "# Load model for prediction\n",
    "loaded_model = cb.CatBoostRegressor()\n",
    "loaded_model.load_model(\"catboost_regression_model.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus tasks 29-31\n",
    "\n",
    "Today's bonus tasks can be completed only by 1-2 students per task. If you want to participate, DM me in Telegram **before Sunday, 23:59.** If more than 8 people want to participate, I will choose those with fewer stars. Each task costs 2 points, if 2 students share one task, they achieve 1 point each.\n",
    "\n",
    "The task is to prepare a detailed presentation (20 mins) on each of these algorithms. \n",
    "\n",
    "* **Bonus Task 29:** [XGBoost](https://arxiv.org/abs/1603.02754)\n",
    "* **Bonus Task 30:** [LightGBM](https://papers.nips.cc/paper_files/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)\n",
    "* **Bonus Task 21:** [CatBoost](https://arxiv.org/abs/1706.09516)\n",
    "\n",
    "Your presentation should cover the details of each algorithm, including the proofs where applicable. Implementation from scratch is not required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
