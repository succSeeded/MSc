{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c8f6f-f62a-4a69-b258-f2ff374b90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ID.3 decision tree algorithm realization.\n",
    "\"\"\"\n",
    "from typing import Dict, Any\n",
    "from IPython.display import display\n",
    "from graphviz import Digraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def select_best_feature(X: np.ndarray, y: np.ndarray, features: list) -> list:\n",
    "    \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "    best_idx = np.argmax([information_gain(X, y, i) for i in range(X.shape[1])])\n",
    "    return [best_idx, features[best_idx]]\n",
    "\n",
    "\n",
    "def id3_algorithm(X: np.ndarray, y: np.ndarray, features: list) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively build the ID3 decision tree.\"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "    majority_class = classes[np.argmax(counts)]\n",
    "\n",
    "    # Base case: all samples same class\n",
    "    if len(classes) == 1 or not features:\n",
    "        return {'class': classes[0], 'majority_class': classes[0]}\n",
    "\n",
    "    best_id, best_feature = select_best_feature(X, y, features)\n",
    "    feature_values = np.unique(X[:, best_id])\n",
    "    new_features = features.copy()\n",
    "    new_features.remove(best_feature)\n",
    "\n",
    "    tree = {\n",
    "        'feature': best_feature,\n",
    "        'majority_class': majority_class,\n",
    "        'children': {}\n",
    "    }\n",
    "\n",
    "    for value in feature_values:\n",
    "        mask = X[:, best_id] == value\n",
    "        X_sub, y_sub = np.delete(X, best_id, axis=1)[mask], y[mask]\n",
    "        if len(y_sub) == 0:\n",
    "            tree['children'][value] = {'class': majority_class, 'majority_class': majority_class}\n",
    "        else:\n",
    "            tree['children'][value] = id3_algorithm(X_sub, y_sub, new_features)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def entropy(y: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\" \n",
    "    _, counts = np.unique(y, return_counts=True)\n",
    "    probs = counts / y.shape[0]\n",
    "    return -np.sum(probs * np.log(probs + 1e-10))\n",
    "\n",
    "\n",
    "def information_gain(X: np.ndarray, y: np.ndarray, feature_idx: int) -> float:\n",
    "    \"\"\"Calculate the information gain for a given feature.\"\"\"\n",
    "    parent_entropy = entropy(y)\n",
    "    conditional_entropy = 0\n",
    "\n",
    "    for value, count in zip(*np.unique(X[:, feature_idx], return_counts=True)):\n",
    "        p = count / y.shape[0]\n",
    "        conditional_entropy += p * entropy(y[X[:, feature_idx] == value])\n",
    "\n",
    "    return parent_entropy - conditional_entropy\n",
    "\n",
    "\n",
    "def visualize_tree(tree: Dict[str, Any], feature_names: list, dot: Digraph = None, parent: str = None, edge_label: str = None) -> Digraph:\n",
    "    \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "    if dot is None:\n",
    "        dot = Digraph(comment='Decision Tree')\n",
    "\n",
    "    # Create a unique node ID\n",
    "    node_id = str(id(tree))\n",
    "\n",
    "    # Add the current node\n",
    "    if 'class' in tree:\n",
    "        node_label = f\"Class: {tree['class']}\"\n",
    "    else:\n",
    "        node_label = f\"Feature: {feature_names[tree['feature']]}\"\n",
    "    dot.node(node_id, node_label)\n",
    "\n",
    "    # Connect to parent node if exists\n",
    "    if parent is not None:\n",
    "        dot.edge(parent, node_id, label=edge_label)\n",
    "\n",
    "    # Recursively add children\n",
    "    if 'children' in tree:\n",
    "        for value, child in tree['children'].items():\n",
    "            visualize_tree(child, feature_names, dot, node_id, str(value))\n",
    "\n",
    "    return dot\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._features = []\n",
    "        self._tree = {}\n",
    "\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, features: list) -> None:\n",
    "        \"\"\"Fit the decision tree to passed data.\"\"\"\n",
    "        self._features = features\n",
    "        self._tree = id3_algorithm(X, y, features)\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels for given data.\"\"\"\n",
    "        ans = np.zeros((X.shape[0], 1), dtype=\"object\")\n",
    "        for i in range(ans.shape[0]):\n",
    "            node = self._tree\n",
    "            while \"class\" not in node:\n",
    "                feature_id = self._features.index(node[\"feature\"])\n",
    "                node = node[\"children\"][X[i, feature_id]]\n",
    "            ans[i,0] = node[\"class\"]\n",
    "        return ans\n",
    "\n",
    "\n",
    "    def show_tree(self):\n",
    "        \"\"\"Visualize the decision tree.\"\"\"\n",
    "        dot = visualize_tree(self._tree, self._features)\n",
    "        display(dot)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # df = pd.read_csv(\"E:\\\\Work\\\\Studies\\\\MSc\\\\ML\\\\sem2\\\\data\\\\tennis.txt\")\n",
    "    # x_train = df.drop([\"play\"], axis=1).to_numpy()\n",
    "    # y_train = df[\"play\"].to_numpy()\n",
    "    # classifier = DecisionTree()\n",
    "    # classifier.fit(x_train, y_train, list(df.columns[:-1]))\n",
    "    # x_test = np.array([[\"rainy\", \"hot\", \"high\", False],\n",
    "    #                    [\"sunny\", \"hot\", \"high\", False],\n",
    "    #                    [\"rainy\", \"cool\", \"normal\", True],\n",
    "    #                    [\"overcast\", \"mild\", \"high\", False]], dtype=np.object_)\n",
    "    # print(classifier.predict(x_test))\n",
    "    \n",
    "    dataset = {\n",
    "        'Taste': ['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "        'Temperature': ['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "        'Texture': ['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "        'Eat': ['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']\n",
    "    }\n",
    "    dataframe = pd.DataFrame(dataset, columns=['Taste','Temperature','Texture','Eat'])\n",
    "\n",
    "    # Prepare features and target\n",
    "    feature_names = dataframe.columns[:-1].tolist()\n",
    "    x_train = dataframe[feature_names].to_numpy()\n",
    "    y_train = dataframe['Eat'].to_numpy()\n",
    "    feature_names = list(range(len(feature_names)))\n",
    "    classifier = DecisionTree()\n",
    "    classifier.fit(x_train, y_train, feature_names.copy())\n",
    "\n",
    "    classifier.show_tree()\n",
    "\n",
    "    x_test = np.array([[\"Salty\", \"Hot\", \"Hard\"]], dtype=np.object_)\n",
    "    prediction = classifier.predict(x_test)\n",
    "    print(f\"Prediction for the new data point: {prediction}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
