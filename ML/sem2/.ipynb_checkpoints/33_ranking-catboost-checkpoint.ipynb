{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: CatBoost for Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking problem\n",
    "\n",
    "* Let $Q = \\{q_1, \\dots, q_n\\}$ be the set of queries\n",
    "* $D_q = \\{d_{q1}, \\dots, d_{qm}\\}$ -- set of objects retrieved for a group $q$\n",
    "* $L_q = \\{l_{q1}, \\dots, l_{qm}\\}$ -- relevance labels for the objects from the set $D_q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking quality metrics:\n",
    "* __Precision__\n",
    "    $$ \\mbox{P}=\\frac{|\\{\\mbox{relevant docs}\\}\\cap\\{\\mbox{retrieved docs}\\}|}{|\\{\\mbox{retrieved docs}\\}|} $$\n",
    "* __Recall__\n",
    "    $$ \\mbox{R}=\\frac{|\\{\\mbox{relevant docs}\\}\\cap\\{\\mbox{retrieved docs}\\}|}{|\\{\\mbox{relevant docs}\\}|} $$\n",
    "    \n",
    "    Notation $@k$ means that metric is calculated on the first $k$ documents from ranking list.\n",
    "\n",
    "    For example, if 1,2,5,7,9 is the ranks of relevant documents (enumerations starts from number 1) from the retrivied then $P@5$ will be $\\frac{3}{5}$.\n",
    "\n",
    "* __Mean average precision (MAP)__\n",
    "    $$\\frac{1}{|Q|}\\sum_{q \\in Q} \\frac{1}{|\\mbox{relevant docs in } D_q|} \\sum_{k} P@k(q) \\times rel(q, k) $$\n",
    "    \n",
    "    Where $rel(q, k)$ is a relevance label of the document at k-th position in our ranking of $D_q$. This metric calculates average precision for a query weighted with document relevances and then calculate mean between all queries.\n",
    "    \n",
    "* __Discounted cumulative gain (DCG)__\n",
    "    $$\\sum_{k=1}^{mq} \\frac{2 ^ {l_{qk}}}{\\log_2(k+1)}$$\n",
    "    \n",
    "    This metric takes into account user behavior: user attention is high on the top and then nonlinear decrease to the end.\n",
    "    \n",
    "* __NDCG__ - normalized DCG = DCG $~ / ~$ IDCG, where IDCG is a maximum possible value of DCG with given set of relevance labels.\n",
    "\n",
    "* __AverageGain__ - represents the average value of the label values for objects with the defined top  label values.\n",
    "   \n",
    "More on wiki: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)\n",
    "\n",
    "Parameter $@k$ for every metric can be specified through metric parameter \"top\", for example \"NDCG:top=10\", would mean NDCG@10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download part of [MSRank](https://www.microsoft.com/en-us/research/project/mslr/) dataset from CatBoost datasets storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from catboost.datasets import msrank_10k\n",
    "\n",
    "train_df, test_df = msrank_10k()\n",
    "\n",
    "X_train = train_df.drop([0, 1], axis=1).values\n",
    "y_train = train_df[0].values\n",
    "queries_train = train_df[1].values\n",
    "\n",
    "X_test = test_df.drop([0, 1], axis=1).values\n",
    "y_test = test_df[0].values\n",
    "queries_test = test_df[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[0], X_train.shape[1], np.unique(queries_train).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_train).items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notmalize to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_relevance = np.max(y_train)\n",
    "y_train /= max_relevance\n",
    "y_test /= max_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of CatBoost pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = cb.Pool(data=X_train, label=y_train, group_id=queries_train)\n",
    "\n",
    "test = cb.Pool(data=X_test, label=y_test, group_id=queries_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also create pools from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"./msrank\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "test_file = os.path.join(data_dir, \"test.csv\")\n",
    "\n",
    "train_df.to_csv(train_file, index=False, header=False)\n",
    "test_df.to_csv(test_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "description_file = os.path.join(data_dir, \"dataset.cd\")\n",
    "with open(description_file, \"w\") as f:\n",
    "    f.write(\"0\\tLabel\\n\")\n",
    "    f.write(\"1\\tQueryId\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cb.Pool(data=train_file, column_description=description_file, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ce2029\">Attention:</span> all objects in dataset must be sorted by group_id\n",
    "\n",
    "For example, if the dataset consits of five documents \n",
    "\\[d1, d2, d3, d4, d5\\] with corresponding queries \\[q1, q2, q2, q1, q2\\] then the dataset should be look like:\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "    d_1, q_1, f_1\\\\\n",
    "    d_4, q_1, f_4\\\\\n",
    "    d_2, q_2, f_2\\\\\n",
    "    d_3, q_2, f_3\\\\\n",
    "    d_5, q_2, f_5\\\\\n",
    "\\end{pmatrix} \\hspace{6px} \\texttt{or} \\hspace{6px}\n",
    "\\begin{pmatrix}\n",
    "    d_2, q_2, f_2\\\\\n",
    "    d_3, q_2, f_3\\\\\n",
    "    d_5, q_2, f_5\\\\\n",
    "    d_1, q_1, f_1\\\\\n",
    "    d_4, q_1, f_4\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where $f_i$ is feature vector of i-th document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "\n",
    "The first and simplest idea is to try predicting document relevance $l_q$ minimizing RMSE.\n",
    "\n",
    "$$\\frac{1}{N}\\sqrt{ \\sum_q \\sum_{d_{qk}} \\left(f(d_{qk}) - l_{qk} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(loss_function, additional_params=None, train_pool=train, test_pool=test):\n",
    "    parameters = {\n",
    "        \"iterations\": 2000,\n",
    "        \"custom_metric\": [\n",
    "            \"NDCG\",\n",
    "            \"MAP:top=10\",\n",
    "            \"PrecisionAt:top=1\",\n",
    "            \"PrecisionAt:top=10\",\n",
    "        ],\n",
    "        \"verbose\": False,\n",
    "        \"random_seed\": 0,\n",
    "    }\n",
    "    parameters[\"loss_function\"] = loss_function\n",
    "    parameters[\"train_dir\"] = loss_function\n",
    "\n",
    "    if additional_params is not None:\n",
    "        parameters.update(additional_params)\n",
    "\n",
    "    model = cb.CatBoostRanker(**parameters)\n",
    "    model.fit(train_pool, eval_set=test_pool, plot=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the simplest model and also demonstrate precision/recall metrics from introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_model(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group weights parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(queries):\n",
    "    queries_set = np.unique(queries)\n",
    "    queries_weight = np.random.uniform(size=queries_set.shape[0])\n",
    "    weights = np.zeros_like(queries, dtype=float)\n",
    "    for i, query_id in enumerate(queries_set):\n",
    "        weights[queries == query_id] = queries_weight[i]\n",
    "    return weights\n",
    "\n",
    "\n",
    "create_weights(queries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_weights = cb.Pool(\n",
    "    data=X_train,\n",
    "    label=y_train,\n",
    "    group_weight=create_weights(queries_train),\n",
    "    group_id=queries_train,\n",
    ")\n",
    "\n",
    "test_with_weights = cb.Pool(\n",
    "    data=X_test,\n",
    "    label=y_test,\n",
    "    group_weight=create_weights(queries_test),\n",
    "    group_id=queries_test,\n",
    ")\n",
    "\n",
    "fit_model(\n",
    "    \"RMSE\",\n",
    "    additional_params={\"train_dir\": \"RMSE_weigths\"},\n",
    "    train_pool=train_with_weights,\n",
    "    test_pool=test_with_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing ploblem, step 2\n",
    "\n",
    "Now lets look at example of documents relevance:\n",
    "\n",
    "$$ \n",
    "    \\begin{align}\n",
    "    labels(q_1) &= \\begin{bmatrix}\n",
    "           4 \\\\\n",
    "           3 \\\\\n",
    "           3 \\\\\n",
    "           1\n",
    "         \\end{bmatrix},\n",
    "    labels(q_2) &= \\begin{bmatrix}\n",
    "           2 \\\\\n",
    "           1 \\\\\n",
    "           1 \\\\\n",
    "           0\n",
    "         \\end{bmatrix}\n",
    "   \\end{align}\n",
    "$$\n",
    "\n",
    "This means that with RMSE loss function we pay more attention to q1 than q2. \n",
    "\n",
    "To avoid this problem we introduce into RMSE a coefficient $c_q$ which depends only on query (and if fact equals to the mean of the difference between prediction and label).\n",
    "\n",
    "$$\\frac{1}{N}\\sqrt{ \\sum_q \\sum_{d_{qk}} \\left(f(d_{qk}) - l_{qk} - \\color{red}{c_{q}} \\right)^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(\"QueryRMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing problem, step 3\n",
    "\n",
    "Since the goal of ranking is to predict a list of documents (which can be generated from given document relevances) RMSE loss function doesn't take into account relations between documents: the first is better than second, second is better than third and fifth etc.\n",
    "\n",
    "We can easily bring this information into the loss function, reducing problem not to regression but classification for two documents $(d_i, d_j)$ -- does $i$th better than $j$th or not.\n",
    "\n",
    "So we minimize the negative loglikelihood:\n",
    "\n",
    "$$ - \\sum_{i,j \\in Pairs} \\log \\left( \\frac{1}{1 + e^{-(f(d_i) - f(d_j))}} \\right) $$\n",
    "\n",
    "Methods based on pair comparisons called __pairwise__ in CatBoostRanker this objective called __PairLogit__.\n",
    "\n",
    "There's no need to change the dataset CatBoost generate the pairs for us. The number of generating pairs managed via parameter max_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(\"PairLogit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing problem, step 4, ~~LambdaRank~~ YetiRank\n",
    "\n",
    "Previous loss function directly minimize \n",
    "Method __YetiRank__ take this effect into account and generates weights for pairs according to their positions ([paper](https://cache-mskstoredata08.cdn.yandex.net/download.yandex.ru/company/to_rank_challenge_with_yetirank.pdf)).\n",
    "\n",
    "YetiRank:\n",
    "$$ - \\sum_{i,j \\in Pairs} \\color{red}{w_{ij}} \\log \\left( \\frac{1}{1 + \\exp{-(f(d_i) - f(d_j))}} \\right) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A special case: top-1 prediction with __QuerySoftMax__\n",
    "\n",
    "$$\n",
    "- \\frac{\n",
    "\\sum_{\\text{Group} \\in \\text{Groups}} \\sum_{i \\in \\text{Group}} w_i t_i \\log\\left( \\frac{w_i e^{\\beta a_i}}{\\sum_{j \\in \\text{Group}} w_j e^{\\beta a_j}} \\right)\n",
    "}{\n",
    "\\sum_{\\text{Group} \\in \\text{Groups}} \\sum_{i \\in \\text{Group}} w_i t_i\n",
    "}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_documents(labels, queries):\n",
    "    \"\"\"relevance 1 for top outputs\"\"\"\n",
    "    query_set = np.unique(queries)\n",
    "    num_queries = query_set.shape[0]\n",
    "    by_query_arg_max = {query: -1 for query in query_set}\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        best_idx = by_query_arg_max[query]\n",
    "        if best_idx == -1 or labels[best_idx] < labels[i]:\n",
    "            by_query_arg_max[query] = i\n",
    "\n",
    "    binary_best_docs = np.zeros(shape=labels.shape)\n",
    "    for arg_max in by_query_arg_max.values():\n",
    "        binary_best_docs[arg_max] = 1.0\n",
    "\n",
    "    return binary_best_docs\n",
    "\n",
    "\n",
    "get_best_documents(y_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_docs_train = get_best_documents(y_train, queries_train)\n",
    "best_docs_test = get_best_documents(y_test, queries_test)\n",
    "\n",
    "train_with_weights = cb.Pool(\n",
    "    data=X_train,\n",
    "    label=best_docs_train,\n",
    "    group_id=queries_train,\n",
    "    group_weight=create_weights(queries_train),\n",
    ")\n",
    "\n",
    "test_with_weights = cb.Pool(\n",
    "    data=X_test,\n",
    "    label=best_docs_test,\n",
    "    group_id=queries_test,\n",
    "    group_weight=create_weights(queries_test),\n",
    ")\n",
    "\n",
    "fit_model(\"QuerySoftMax\", train_pool=train_with_weights, test_pool=test_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1\n",
    "\n",
    "As in step 3.1 __YetiRankPairwise__ is slower than __YetiRank__, but gives more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRankPairwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = MetricVisualizer(\n",
    "    ['RMSE', 'QueryRMSE', 'PairLogit', 'PairLogitPairwise', 'YetiRank', 'YetiRankPairwise']\n",
    ")\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look on NDCG metric of method YetiRank $-$ it's underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRank', {'train_dir': 'YetiRank-lr-0.3', 'learning_rate': 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "widget = MetricVisualizer(['YetiRank', 'YetiRank-lr-0.3'])\n",
    "widget.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Metric period__\n",
    "\n",
    "Period in iterations of calculation metrics. This parameter can speed up training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_model('YetiRank', {'metric_period': 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task type__\n",
    "\n",
    "You can significantly speed up training procedure switching to gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit_model('YetiRank', {'task_type': 'GPU'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "This notebook is heavily based on official [catboost tutorial](https://github.com/catboost/catboost/blob/master/catboost/tutorials/ranking/ranking_tutorial.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
