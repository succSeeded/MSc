{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99961,"databundleVersionId":12000016,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Импорты","metadata":{}},{"cell_type":"code","source":"from typing import Any, Callable, Dict, List, Union\nimport numpy as np\nimport numpy.typing as npt\nimport pandas as pd","metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_file = \"/kaggle/input/sporify-songs/train.csv\"\ntest_file = \"/kaggle/input/sporify-songs/test.csv\"\nsubmission_path = \"/kaggle/working/submission.csv\"\n\ndef root_mean_squared_error(x1: npt.ArrayLike, x2: npt.ArrayLike) -> float:\n    return np.sqrt(np.power(x1 - x2, 2).mean())","metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Дерево регрессии","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"class DecisionTreeRegressor():\n    \"\"\"\n    Decision tree regressor, something to build gradient boosting algorithms off of.\n    \"\"\"\n\n    SPLIT_CRITERIA = {\n        \"mae\": lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred)),\n        \"mse\": lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2),\n    }\n\n    def __init__(\n        self,\n        max_depth: int = None,\n        min_samples_split: int = None,\n        min_samples_leaf: int = None,\n        criterion: str = None,\n    ) -> None:\n        if criterion is None:\n            self.criterion = DecisionTreeRegressor.SPLIT_CRITERIA[\"mse\"]\n        else:\n            self.criterion = DecisionTreeRegressor.SPLIT_CRITERIA[criterion.lower()]\n        self.max_depth = -1 if max_depth is None else max_depth\n        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n\n    def fit(\n        self,\n        X: npt.ArrayLike,\n        y: npt.ArrayLike,\n        features: list = None,\n        cat_features: list = None,\n    ) -> None:\n        \"\"\"Fit the decision tree to passed data.\"\"\"\n\n        if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n            X = X.to_numpy()\n            y = y.to_numpy()\n\n        if features is None:\n            self.features_ = [f\"x_{i}\" for i in range(X.shape[1])]\n        else:\n            self.features_ = features\n\n        if cat_features is None:\n            self.cat_features_ = []\n        else:\n            self.cat_features_ = cat_features\n\n        self.tree_ = DecisionTreeRegressor.build_tree_(\n            X,\n            y,\n            self.features_,\n            depth=0,\n            max_depth=self.max_depth,\n            min_samples_leaf=self.min_samples_leaf,\n            min_samples_split=self.min_samples_split,\n            cat_features=self.cat_features_,\n            criterion=self.criterion,\n        )\n\n        self.n_leaves_ = DecisionTreeRegressor.count_leaves_(self.tree_)\n\n    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n        \"\"\"Predict class labels for given data.\"\"\"\n        ans = np.zeros((X.shape[0],))\n        for i in range(ans.shape[0]):\n            node = self.tree_\n            while not node[\"is_leaf\"]:\n                # Get the index a the feature on which the split was performed\n                feature_idx = self.features_.index(node[\"feature\"])\n\n                if node[\"feature\"] in self.cat_features_:\n                    # If the category has not appeared in training set, the tree\n                    # traversal is terminated and the current node value is used\n                    if node[\"children\"].get(X[i, feature_idx], False):\n                        node = node[\"children\"].get(X[i, feature_idx], False)\n                    else:\n                        break\n                else:\n                    if X[i, feature_idx] <= node[\"threshold\"]:\n                        node = node[\"children\"][\"lower\"]\n                    else:\n                        node = node[\"children\"][\"upper\"]\n\n            ans[i] = node[\"value\"]\n        return ans\n\n    def get_n_leaves(self):\n        return self.n_leaves_\n\n    # =============================================================================\n    # Tree construction\n    # =============================================================================\n\n    def build_tree_(\n        X: npt.ArrayLike,\n        y: npt.ArrayLike,\n        features: list,\n        depth: int,\n        max_depth: int,\n        min_samples_split: int,\n        min_samples_leaf: int,\n        cat_features: list,\n        criterion: Callable,\n    ) -> Dict[str, Any]:\n        \"\"\"Recursively build a regression tree.\"\"\"\n\n        default_value = np.mean(y)\n\n        # Terminate if there are no more features to split on\n        if X.shape[1] == 0 or len(features) == 0:\n            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n\n        # Terminate if all the targets are duplicates of eachother\n        if np.unique(y).shape[0] == 1:\n            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n\n        # Terminate if all the datapoints are duplicates of eachother\n        if np.unique(X, axis=0).shape[0] == 1:\n            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n\n        # Terminate if node does not contain enough elements to split\n        if y.shape[0] < min_samples_split:\n            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n\n        # Terminate if max tree depth is reached\n        if depth == max_depth:\n            return {\"value\": default_value, \"depth\": depth, \"is_leaf\": True}\n\n        best_id, best_feature, threshold = DecisionTreeRegressor.select_best_feature_(\n            X, y, features, cat_features, criterion\n        )\n        new_features = [feature for feature in features if feature != best_feature]\n\n        tree = {\n            \"depth\": depth,\n            \"feature\": best_feature,\n            \"is_leaf\": False,\n            \"value\": default_value,\n            \"children\": {},\n        }\n\n        if best_feature in cat_features:\n            categories = np.unique(X[:, best_id])\n\n            for category in categories:\n\n                mask = X[:, best_id] == category\n\n                X_sub, y_sub = (\n                    np.delete(X, best_id, axis=1)[mask],\n                    y[mask],\n                )\n\n                # If there is not enough samples to make a leaf the split is aborted and\n                # the node is considered a leaf\n                if len(y_sub) < min_samples_leaf:\n                    tree[\"is_leaf\"] = True\n                    tree[\"children\"] = {}\n                    return tree\n\n                tree[\"children\"][category] = DecisionTreeRegressor.build_tree_(\n                    X_sub,\n                    y_sub,\n                    new_features,\n                    depth=depth + 1,\n                    max_depth=max_depth,\n                    min_samples_leaf=min_samples_leaf,\n                    min_samples_split=min_samples_split,\n                    cat_features=cat_features,\n                    criterion=criterion,\n                )\n        else:\n            mask_left = X[:, best_id] <= threshold\n            mask_right = X[:, best_id] > threshold\n\n            tree[\"threshold\"] = threshold\n\n            # If there is not enough samples to make a leaf the split is aborted and\n            # the node is considered a leaf\n            if (\n                mask_left.sum() < min_samples_leaf\n                or mask_right.sum() < min_samples_leaf\n            ):\n                tree[\"is_leaf\"] = True\n                return tree\n\n            X_sub = np.delete(X, best_id, axis=1)\n\n            tree[\"children\"][\"lower\"] = DecisionTreeRegressor.build_tree_(\n                X_sub[mask_left],\n                y[mask_left],\n                new_features,\n                depth=depth + 1,\n                max_depth=max_depth,\n                min_samples_leaf=min_samples_leaf,\n                min_samples_split=min_samples_split,\n                cat_features=cat_features,\n                criterion=criterion,\n            )\n\n            tree[\"children\"][\"upper\"] = DecisionTreeRegressor.build_tree_(\n                X_sub[mask_right],\n                y[mask_right],\n                new_features,\n                depth=depth + 1,\n                max_depth=max_depth,\n                min_samples_leaf=min_samples_leaf,\n                min_samples_split=min_samples_split,\n                cat_features=cat_features,\n                criterion=criterion,\n            )\n        return tree\n\n    def feature_score_(\n        X: npt.ArrayLike,\n        y: npt.ArrayLike,\n        feature_idx: int,\n        is_cat_feature: bool,\n        criterion: Callable,\n    ) -> float:\n        \"\"\"Calculate score for a given feature.\"\"\"\n\n        if is_cat_feature is None or is_cat_feature == False:\n\n            uniques = np.unique(X[:, feature_idx])\n\n            # Splits are not done if all the feature values are the same\n            if uniques.shape[0] == 1:\n                return {\"value\": np.inf, \"threshold\": None}\n\n            thresholds = [\n                0.5 * (curr + prev) for prev, curr in zip(uniques, uniques[1:])\n            ]\n            split_scores = []\n\n            for theta in thresholds:\n\n                mask_left = X[:, feature_idx] <= theta\n                mask_right = X[:, feature_idx] > theta\n\n                # Elements lower and higher than threshold are compared to their respective means\n                y_pred = np.where(\n                    mask_left, np.mean(y[mask_left]), np.mean(y[mask_right])\n                )\n                split_scores += [criterion(y_pred, y)]\n\n            best_split_id = np.argmin(split_scores)\n            best_threshold = thresholds[best_split_id]\n            return {\"value\": split_scores[best_split_id], \"threshold\": best_threshold}\n\n        else:\n            categories = np.unique(X[:, feature_idx])\n            y_pred = np.zeros_like(y)\n\n            for category in categories:\n\n                mask = X[:, feature_idx] == category\n                # Elements in each(surviving!) category are compared to their respective means\n                y_pred = np.where(mask, np.mean(y[mask]), y_pred)\n\n            score = criterion(y_pred, y)\n            return {\"value\": score, \"threshold\": None}\n\n    def select_best_feature_(\n        X: npt.ArrayLike,\n        y: npt.ArrayLike,\n        features: list,\n        cat_features: list,\n        criterion: Callable,\n    ) -> list:\n        \"\"\"Select the feature with the highest information gain.\"\"\"\n        scores = [\n            DecisionTreeRegressor.feature_score_(\n                X, y, i, feature in cat_features, criterion\n            )\n            for i, feature in enumerate(features)\n        ]\n\n        best_idx = np.argmin([score[\"value\"] for score in scores])\n        return [best_idx, features[best_idx], scores[best_idx][\"threshold\"]]\n\n    # =============================================================================\n    # Tree pruning\n    # =============================================================================\n\n    def count_leaves_(tree: Dict) -> int:\n        \"\"\"Count the number of leaf nodes in a (sub)tree.\"\"\"\n        if tree[\"is_leaf\"]:\n            return 1\n\n        total_leaves = 0\n        for child in tree[\"children\"]:\n            total_leaves += DecisionTreeRegressor.count_leaves_(tree[\"children\"][child])\n\n        return total_leaves","metadata":{"editable":true,"slideshow":{"slide_type":""},"tags":[]},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Градиентный бустинг","metadata":{}},{"cell_type":"code","source":"class GradientBoostingRegressor():\n\n    def __init__(\n        self,\n        learning_rate: float = None,\n        max_depth: int = None,\n        n_estimators: int = None,\n        min_samples_split: int = None,\n        min_samples_leaf: int = None,\n        criterion: str = None,\n        tol: float = None,\n    ) -> None:\n        self.criterion = criterion\n        self.tol = 1e-4 if tol is None else tol\n        self.max_depth = 4 if max_depth is None else max_depth\n        self.n_estimators = 100 if n_estimators is None else n_estimators\n        self.learning_rate = 0.1 if learning_rate is None else learning_rate\n        self.min_samples_split = 2 if min_samples_split is None else min_samples_split\n        self.min_samples_leaf = 1 if min_samples_leaf is None else min_samples_leaf\n\n    def fit(\n        self,\n        X: npt.ArrayLike,\n        y: npt.ArrayLike,\n    ) -> None:\n        \"\"\"Fit the regressor to data. Note that it requires the categorical features\n        to be already encoded since otherwise the won't be processed.\n        This is done in order to reduce the tree sizes and increase learning speeds.\"\"\"\n        self.trees_ = [\n            DecisionTreeRegressor(\n                max_depth=0,\n                min_samples_split=self.min_samples_split,\n                min_samples_leaf=self.min_samples_leaf,\n                criterion=self.criterion,\n            )\n        ]\n        self.trees_[0].fit(X, y)\n        curr_guess = self.trees_[0].predict(X)\n        prev_guess = np.copy(y)\n        residue = np.sqrt(np.power((prev_guess - curr_guess), 2).sum())\n        num_estimators = 1\n\n        while num_estimators < self.n_estimators:\n            if residue < self.tol:\n                break\n\n            self.trees_ += [\n                DecisionTreeRegressor(\n                    max_depth=self.max_depth,\n                    min_samples_split=self.min_samples_split,\n                    min_samples_leaf=self.min_samples_leaf,\n                    criterion=self.criterion,\n                )\n            ]\n            self.trees_[-1].fit(X, y - curr_guess)\n            prev_guess = np.copy(curr_guess)\n            curr_guess += self.learning_rate * self.trees_[-1].predict(X)\n            residue = np.sqrt(np.power((prev_guess - curr_guess), 2).sum())\n            num_estimators += 1\n\n    def predict(self, X: npt.ArrayLike) -> npt.ArrayLike:\n        ans = self.trees_[0].predict(X)\n        for tree in self.trees_[1:]:\n            ans += tree.predict(X) * self.learning_rate\n        return ans","metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"### Реализация `OrdinalEncoder`","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"class OrdinalEncoder():\n    def __init__(\n        self,\n        handle_unknown: str = None,\n        unknown_value: int = None,\n    ) -> None:\n        self.encoded_missing_value_ = np.nan\n\n        # Check if `handle_unknown` is set to a known value:\n        if handle_unknown is None:\n            self.handle_unknown_ = \"error\"\n        else:\n            if handle_unknown == \"error\":\n                self.handle_unknown_ = \"error\"\n            elif handle_unknown == \"use_encoded_value\":\n                self.handle_unknown_ = \"use_encoded_value\"\n                self.unknown_value_ = -1 if unknown_value is None else unknown_value\n\n    def fit(\n        self,\n        data: Union[npt.ArrayLike, pd.DataFrame],\n    ):\n        self.feature_dict_ = {}\n        if isinstance(data, pd.DataFrame):\n            data = data.to_numpy()\n\n        for feature in range(data.shape[1]):\n            uniques = np.unique(data[:, feature])\n            self.feature_dict_[feature] = {val: idx for idx, val in enumerate(uniques)}\n\n        return self\n\n    def fit_transform(\n        self,\n        data: Union[npt.ArrayLike, pd.Series, pd.DataFrame],\n    ) -> npt.ArrayLike:\n        self.fit(data)\n        return self.transform(data)\n\n    def transform(self, X: npt.ArrayLike) -> npt.ArrayLike:\n\n        ans = np.zeros_like(X)\n\n        for feature in range(X.shape[1]):\n            for unique_val in set(X[:, feature]):\n                substitute_value = self.feature_dict_[feature].get(unique_val, False) if self.feature_dict_[feature].get(unique_val, False) else self.unknown_value_\n                ans[:, feature] = np.where(\n                    X[:, feature] == unique_val, substitute_value, ans[:, feature]\n                )\n        return ans","metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Реализация `K-Fold Target encoder`","metadata":{}},{"cell_type":"code","source":"### KFold(n_splits=self.cv_, shuffle=self.shuffle_)\n\nclass KFold():\n    def __init__(self, n_splits: int=None, shuffle: bool=None, random_seed: int=None):\n        self.n_splits_ = 2 if n_splits is None else n_splits\n        self.shuffle_ = False if shuffle is None else shuffle\n        self.random_seed_ = None if random_seed is None else random_seed\n\n    def split(self, data):\n        fold_size = len(data) // self.n_splits_\n        indices = np.arange(len(data))\n\n        if self.shuffle_:\n            rng = np.random.default_rng(seed=self.random_seed_)\n            rng.shuffle(indices)\n\n        folds = []\n        for i in range(self.n_splits_):\n            test_indices = indices[i * fold_size: (i + 1) * fold_size]\n            train_indices = np.concatenate([indices[:i * fold_size], indices[(i + 1) * fold_size:]])\n            folds.append((train_indices, test_indices))\n        return folds","metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class KFoldTargetEncoder():\n    def __init__(\n        self,\n        cv: int = None,\n        handle_unknown: str = None,\n        unknown_value: float = None,\n        shuffle: bool = None,\n    ) -> None:\n        self.encoded_missing_value_ = np.nan\n        self.cv_ = 5 if cv is None else cv\n        self.shuffle_ = False if shuffle is None else shuffle\n\n        # Check if `handle_unknown` is set to a known value:\n        if handle_unknown is None:\n            self.handle_unknown_ = \"mean\"\n        else:\n            if handle_unknown == \"mean\":\n                self.handle_unknown_ = \"mean\"\n            elif handle_unknown == \"use_encoded_value\":\n                self.handle_unknown_ = \"use_encoded_value\"\n                self.unknown_value_ = 0.0 if unknown_value is None else unknown_value\n\n    def fit(\n        self,\n        data: Union[npt.ArrayLike, pd.Series, pd.DataFrame],\n        target: npt.ArrayLike,\n    ):\n        self.data_ = data\n        self.target_ = target\n\n        if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series):\n            self.data_ = self.data_.to_numpy(dtype=np.object_)\n\n        self.transformed_data_ = np.zeros_like(self.data_, dtype=float)\n\n        if isinstance(target, pd.DataFrame):\n            self.target_ = self.target_.to_numpy(dtype=np.object_)\n\n        if self.handle_unknown_ == \"mean\":\n            self.unknown_value_ = self.target_.mean()\n\n        kf = KFold(n_splits=self.cv_, shuffle=self.shuffle_)\n\n        for train_idx, val_idx in kf.split(self.data_):\n\n            X_train, X_val = (self.data_[train_idx, :], self.data_[val_idx, :])\n            y_train = self.target_[train_idx]\n\n            for feature in range(X_train.shape[1]):\n                for unique_val in set(X_val[:, feature]):\n                    train_mask = X_train[:, feature] == unique_val\n                    weight = (\n                        train_mask.sum()\n                        * (\n                            y_train[train_mask].mean()\n                            if len(y_train[train_mask]) >= 1\n                            else 0.0\n                        )\n                        + y_train.mean() * (X_train.shape[0] - train_mask.sum())\n                    ) / X_train.shape[0]\n                    self.transformed_data_[val_idx, feature] = np.where(\n                        X_val[:, feature] == unique_val,\n                        weight,\n                        self.transformed_data_[val_idx, feature],\n                    )\n\n        return self\n\n    def fit_transform(\n        self,\n        data: Union[npt.ArrayLike, pd.Series, pd.DataFrame],\n        target: npt.ArrayLike,\n    ) -> npt.ArrayLike:\n        self.fit(data, target)\n        return self.transformed_data_\n\n    def transform(self, X: npt.ArrayLike) -> npt.ArrayLike:\n\n        ans = np.zeros_like(X)\n\n        for feature in range(self.data_.shape[1]):\n            for unique_val in set(X[:, feature]):\n                mask = self.data_[:, feature] == unique_val\n                weight = (\n                    self.unknown_value_\n                    if mask.sum() == 0\n                    else self.target_[mask].mean()\n                )\n                ans[:, feature] = np.where(\n                    X[:, feature] == unique_val, weight, ans[:, feature]\n                )\n        return ans","metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### Сбор результатов бустинга на отправку","metadata":{}},{"cell_type":"code","source":"drop_list_gb = [\n    \"type\",\n    \"track_href\",\n    \"track_href\",\n    \"uri\",\n    \"analysis_url\",\n    \"track_id\",\n    \"track_album_id\",\n    \"track_name\",\n    \"track_album_name\",\n    \"id\",\n    \"playlist_id\",\n]\n\ncat_features_gb = [\n    \"playlist_name\",\n    \"playlist_genre\",\n    \"track_artist\",\n    \"time_signature\",\n    \"playlist_subgenre\",\n    \"track_album_release_date\",\n    \"mode\",\n    \"key\",\n]\n\ndf_train = pd.read_csv(train_file)\ndf_train = df_train.drop(\n    drop_list_gb + [\"Unnamed: 0\"],\n    axis=1,\n)\ndf_train = df_train.dropna()\ndf_train = df_train.drop_duplicates()\n\ndf_test = pd.read_csv(test_file)\ndf_test = df_test.drop(\n    drop_list_gb,\n    axis=1,\n)\ndf_test = df_test.dropna()\n\ndf_train.loc[:, \"track_album_release_date\"] = (\n    pd.to_datetime(\n        df_train[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n    ).dt.year\n).astype(str)\n\ndf_test.loc[:, \"track_album_release_date\"] = (\n    pd.to_datetime(\n        df_test[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n    ).dt.year\n).astype(str)\n\ngbX, gby = (df_train.drop(\"popularity\", axis=1), df_train[\"popularity\"])\n\nencoder = KFoldTargetEncoder(cv=5)\ngbX.loc[:, cat_features_gb] = encoder.fit_transform(gbX[cat_features_gb].to_numpy(), gby.to_numpy())\ndf_test.loc[:, cat_features_gb] = encoder.transform(df_test[cat_features_gb].to_numpy())\n\ngb_regressor = GradientBoostingRegressor(\n    n_estimators=100,\n    max_depth=5,\n    min_samples_leaf=40,\n    min_samples_split=100,\n    learning_rate=0.1,\n    tol=1e-4,\n)\ngb_regressor.fit(\n    gbX.to_numpy(dtype=float),\n    gby.to_numpy(dtype=float),\n)\n\ny_pred_gb = pd.DataFrame(\n    gb_regressor.predict(df_test.to_numpy(dtype=float)),\n    index=pd.Index(df_test.index, name=\"Id\"),\n    columns=[\"popularity\"],\n)","metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"### Сбор результатов дерева на отправку","metadata":{}},{"cell_type":"code","source":"drop_list_dt = [\n    \"type\",\n    \"track_href\",\n    \"track_href\",\n    \"uri\",\n    \"track_album_name\",\n    \"analysis_url\",\n    \"track_id\",\n    \"track_name\",\n    \"track_artist\",\n    \"track_album_id\",\n    \"id\",\n    \"playlist_id\",\n    \"playlist_name\",\n]\n\ncat_features_dt = [\n    \"playlist_genre\",\n    \"time_signature\",\n    \"playlist_subgenre\",\n    \"mode\",\n    \"key\",\n    \"track_album_release_date\",\n]\n\ndf_train = pd.read_csv(train_file)\ndf_train = df_train.drop(\n    drop_list_dt + [\"Unnamed: 0\"],\n    axis=1,\n)\ndf_train = df_train.dropna()\ndf_train = df_train.drop_duplicates()\n\ndf_test = pd.read_csv(test_file)\ndf_test = df_test.drop(\n    drop_list_dt,\n    axis=1,\n)\ndf_test = df_test.dropna()\n\ndf_train.loc[:, \"track_album_release_date\"] = (\n    pd.to_datetime(\n        df_train[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n    ).dt.year\n).astype(str)\n\ndf_test.loc[:, \"track_album_release_date\"] = (\n    pd.to_datetime(\n        df_test[\"track_album_release_date\"], format=\"mixed\", yearfirst=True\n    ).dt.year\n).astype(str)\n\ndtX, dty = (df_train.drop(\"popularity\", axis=1), df_train[\"popularity\"])\n\nfor feature in cat_features_dt:\n    for val, count in dtX[feature].value_counts().items():\n        if count <= 2:\n            dtX.loc[dtX[feature] == val, feature] = \"other\"\n            df_test.loc[df_test[feature] == val, feature] = \"other\"\n\nenc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\ndtX[cat_features_dt] = enc.fit_transform(dtX[cat_features_dt].to_numpy())\ndf_test[cat_features_dt] = enc.transform(df_test[cat_features_dt].to_numpy())\n\nbest_tree_params = {'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 3}\ndt_regressor = DecisionTreeRegressor(**best_tree_params)\n\ndt_regressor.fit(    \n    dtX.to_numpy(dtype=float),\n    dty.to_numpy(dtype=float),\n    features=list(dtX.columns),\n    cat_features=cat_features_dt,\n)\n\ny_pred_dt = pd.DataFrame(\n    dt_regressor.predict(df_test.to_numpy()),\n    index=pd.Index(df_test.index, name=\"Id\"),\n    columns=[\"popularity\"],\n)","metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":"y_pred_dt.to_csv(\n    path_or_buf=submission_path\n)","metadata":{},"outputs":[],"execution_count":null}]}