{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayessian Optimization\n",
    "\n",
    "---\n",
    "Few popular acquisition functions:\n",
    "\n",
    "1. **Expected Improvement (EI)**\n",
    "$$\n",
    "\\text{EI}(x) = (\\mu(x) - f^*)\\Phi(z^*) + \\sigma(x)\\phi(z^*)\n",
    "$$\n",
    "\n",
    "2. **Upper Confidence Bound (UCB)**\n",
    "$$\n",
    "\\text{UCB}(x) = \\mu(x) + \\beta \\sigma(x)\n",
    "$$\n",
    "\n",
    "3. **Maximum Probability of Improvement (MPI)**\n",
    "$$\n",
    "\\text{MPI}(x) = \\Phi\\left(\\frac{\\mu(x) - f^*}{\\sigma(x)}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 600\n",
    "plt.rcParams[\"savefig.format\"] = \"pdf\"\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-1.0, 2.0]])\n",
    "noise = 0.2\n",
    "\n",
    "\n",
    "def f(X, noise=noise):\n",
    "    return -np.sin(3 * X) - X**2 + 0.7 * X + noise * np.random.randn(*X.shape)\n",
    "\n",
    "\n",
    "X_init = np.array([[-0.9], [1.1]])\n",
    "Y_init = f(X_init)\n",
    "# Dense grid of points within bounds\n",
    "X = np.arange(bounds[:, 0], bounds[:, 1], 0.01).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense grid of points within bounds\n",
    "X = np.arange(bounds[:, 0], bounds[:, 1], 0.01).reshape(-1, 1)\n",
    "\n",
    "# Noise-free objective function values at X\n",
    "Y = f(X, 0)\n",
    "\n",
    "# Plot optimization objective with noise level\n",
    "plt.plot(X, Y, \"y--\", lw=2, label=\"Noise-free objective\")\n",
    "plt.plot(X, f(X), \"bx\", lw=1, alpha=0.1, label=\"Noisy samples\")\n",
    "plt.plot(X_init, Y_init, \"kx\", mew=3, label=\"Initial samples\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([[-1.0, 2.0]])\n",
    "noise = 0.2\n",
    "\n",
    "\n",
    "def f(X, noise=noise):\n",
    "    return -np.sin(3 * X) - X**2 + 0.7 * X + noise * np.random.randn(*X.shape)\n",
    "\n",
    "\n",
    "X_init = np.array([[-0.9], [1.1]])\n",
    "Y_init = f(X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next=None, show_legend=False):\n",
    "    mu, std = gpr.predict(X, return_std=True)\n",
    "    plt.fill_between(\n",
    "        X.ravel(), mu.ravel() + 1.96 * std, mu.ravel() - 1.96 * std, alpha=0.1\n",
    "    )\n",
    "    plt.plot(X, Y, \"y--\", lw=1, label=\"Noise-free objective\")\n",
    "    plt.plot(X, mu, \"b-\", lw=1, label=\"Surrogate function\")\n",
    "    plt.plot(X_sample, Y_sample, \"kx\", mew=3, label=\"Noisy samples\")\n",
    "    if X_next:\n",
    "        plt.axvline(x=X_next, ls=\"--\", c=\"k\", lw=1)\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "def plot_acquisition(X, Y, X_next, show_legend=False):\n",
    "    plt.plot(X, Y, \"r-\", lw=1, label=\"Acquisition function\")\n",
    "    plt.axvline(x=X_next, ls=\"--\", c=\"k\", lw=1, label=\"Next sampling location\")\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "def plot_convergence(X_sample, Y_sample, n_init=2):\n",
    "    plt.figure(figsize=(12, 3))\n",
    "\n",
    "    x = X_sample[n_init:].ravel()\n",
    "    y = Y_sample[n_init:].ravel()\n",
    "    r = range(1, len(x) + 1)\n",
    "\n",
    "    x_neighbor_dist = [np.abs(a - b) for a, b in zip(x, x[1:])]\n",
    "    y_max_watermark = np.maximum.accumulate(y)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(r[1:], x_neighbor_dist, \"bo-\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.title(\"Distance between consecutive x's\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(r, y_max_watermark, \"ro-\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Best Y\")\n",
    "    plt.title(\"Value of best selected sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayessian_optimization(acquisiition_fn):\n",
    "    ...\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        ...\n",
    "        \n",
    "        # Plot samples, surrogate function, noise-free objective and next sampling location\n",
    "        plt.subplot(n_iter, 2, 2 * i + 1)\n",
    "        plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next, show_legend=i == 0)\n",
    "        plt.title(f\"Iteration {i+1}\")\n",
    "    \n",
    "        plt.subplot(n_iter, 2, 2 * i + 2)\n",
    "        plot_acquisition(\n",
    "            X,\n",
    "            probability_of_improvement(X, X_sample, Y_sample, gpr),\n",
    "            X_next,\n",
    "            show_legend=i == 0,\n",
    "        )\n",
    "    \n",
    "        # Add sample to previous samples\n",
    "        X_sample = np.vstack((X_sample, X_next))\n",
    "        Y_sample = np.vstack((Y_sample, Y_next))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_location(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=25):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_confidence_bound(X, X_sample, Y_sample, gpr, kappa=1):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.01):\n",
    "    return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization libraries\n",
    "\n",
    "There are numerous Bayesian optimization libraries out there and giving a comprehensive overview is not the goal of this article. Instead, I'll pick two that I used in the past and show the minimum setup needed to get the previous example running.\n",
    "\n",
    "### Scikit-optimize\n",
    "\n",
    "[Scikit-optimize](https://scikit-optimize.github.io/) is a library for sequential model-based optimization that is based on [scikit-learn](http://scikit-learn.org/). It also supports Bayesian optimization using Gaussian processes. The API is designed around minimization, hence, we have to provide negative objective function values.  The results obtained here slightly differ from previous results because of non-deterministic optimization behavior and different noisy samples drawn from the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from skopt import gp_minimize\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import ConstantKernel, Matern\n",
    "\n",
    "# Use custom kernel and estimator to match previous example\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "r = gp_minimize(\n",
    "    lambda x: -f(np.array(x))[0],\n",
    "    bounds.tolist(),\n",
    "    base_estimator=gpr,\n",
    "    acq_func=\"EI\",  # expected improvement\n",
    "    xi=0.01,  # exploitation-exploration trade-off\n",
    "    n_calls=10,  # number of iterations\n",
    "    n_random_starts=0,  # initial samples are provided\n",
    "    x0=X_init.tolist(),  # initial samples\n",
    "    y0=-Y_init.ravel(),\n",
    ")\n",
    "\n",
    "# Fit GP model to samples for plotting results\n",
    "gpr.fit(r.x_iters, -r.func_vals)\n",
    "\n",
    "# Plot the fitted model and the noisy samples\n",
    "plot_approximation(gpr, X, Y, r.x_iters, -r.func_vals, show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(np.array(r.x_iters), -r.func_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: tuning model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load the diabetes dataset (for regression)\n",
    "X, Y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Instantiate an XGBRegressor with default hyperparameter settings\n",
    "xgb = XGBRegressor()\n",
    "\n",
    "# and compute a baseline to beat with hyperparameter optimization\n",
    "baseline = cross_val_score(xgb, X, Y, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with random search\n",
    "\n",
    "For hyperparameter tuning with random search, we use `RandomSearchCV` of scikit-learn and compute a cross-validation score for each randomly selected point in hyperparameter space. Results will be discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune and their ranges\n",
    "param_dist = {\n",
    "    \"learning_rate\": uniform(0, 1),\n",
    "    \"gamma\": uniform(0, 5),\n",
    "    \"max_depth\": range(1, 50),\n",
    "    \"n_estimators\": range(1, 300),\n",
    "    \"min_child_weight\": range(1, 10),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    xgb, param_distributions=param_dist, scoring=\"neg_mean_squared_error\", n_iter=25\n",
    ")\n",
    "\n",
    "# Run random search for 25 iterations\n",
    "rs.fit(X, Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with Bayesian optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This noptebook rely heavily on Martin Krasser's [course](https://github.com/krasserm) on Bayessian Machine Learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
