{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Внутренние меры кластеризации\n",
    "\n",
    "Или как оценивать, если правильных меток не существует в природе? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, true_labels = make_blobs(\n",
    "    n_samples=200, centers=3, cluster_std=2.75, random_state=42\n",
    ")\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=10)\n",
    "kmeans.fit(features)\n",
    "\n",
    "## Кстати, кто такой Rand Index ?\n",
    "\n",
    "print(\"KMeans Rand Index: \", metrics.rand_score(kmeans.labels_, true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## А тут вы сколько ожидаете увидеть? А если пар -- 100?\n",
    "random_labels = np.random.randint(0, 3, size=true_labels.shape)\n",
    "print(\"Random Rand Index: \", metrics.rand_score(random_labels, true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (left, right) = plt.subplots(1, 2)\n",
    "\n",
    "left.scatter(\n",
    "    features[:, 0],\n",
    "    features[:, 1],\n",
    "    s=15,\n",
    "    c=random_labels,\n",
    ")\n",
    "\n",
    "right.scatter(\n",
    "    features[:, 0],\n",
    "    features[:, 1],\n",
    "    s=15,\n",
    "    c=kmeans.labels_,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "print(f\"Silhouette score\")\n",
    "print(f\"random clustering: {silhouette_score(features, random_labels)}\")\n",
    "print(f\"kmeans: {silhouette_score(features, kmeans.labels_)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"David-Bouldin Index\")\n",
    "print(f\"random clustering: {davies_bouldin_score(features, random_labels)}\")\n",
    "print(f\"kmeans: {davies_bouldin_score(features, kmeans.labels_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Как выбрать число кластеров\n",
    "\n",
    "\n",
    "### Elbow plot\n",
    "Самый простой метод, продолжающий предыдущую тему.\n",
    "\n",
    "Какое максимальное и минимальное значения $EV$? Как они достигаются?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cd/DataClustering_ElbowCriterion.JPG\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yellowbrick\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "visualizer = KElbowVisualizer(kmeans, k=(1, 9))\n",
    "\n",
    "visualizer.fit(features)\n",
    "visualizer.show()  # тут вместо explained variance сырые расстояния"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette analysis\n",
    "\n",
    "Выше мы рассмотрели глобальный (усредненный) Silhouette score. Давайте теперь росмотрим на эти коэффициенты, сгруппированные по кластерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "# поменять число кластеров и посмотркть картинки\n",
    "model = KMeans(2, random_state=42, n_init=10)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "\n",
    "visualizer.fit(features)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Case study: сжатие изображений\n",
    "\n",
    "Адаптация примера из курса К.В. Воронцова.\n",
    "\n",
    "Преобразуем изображение, приведя все значения\n",
    "в интервал [0, 1]: можно использовать функцию `img_as_float` из модуля `skimage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import skimage\n",
    "from skimage import data, io\n",
    "from skimage.io import imread, imsave\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "image = imread(\"Lenna.png\")\n",
    "print(image.shape, image.max(), image.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каптинка в т.н. \"полноцветном\" формате, т.е. по 1 байту на каждый из трех каналов.\n",
    "\n",
    " **Сколько всего цветов ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PSNR - peak signal-to-noise ratio**\n",
    "\n",
    "Соотношение между максимумом возможного значения сигнала и мощностью шума, искажающего значения сигнала. Поскольку многие сигналы имеют широкий динамический диапазон, PSNR обычно измеряется в логарифмической шкале в децибелах.\n",
    "\n",
    "PSNR наиболее часто используется для измерения уровня искажений при сжатии изображений. \n",
    "\n",
    "$$PSNR = 10 ~ \\log_{10} \\frac{MAX_{orig}^2}{MSE}$$\n",
    "\n",
    "Довольно плохая метрика, кстати говоря; как думаете, в чем ее проблема?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(X_true, X_clustered):\n",
    "\n",
    "    mse = np.mean((X_true - X_clustered) ** 2)\n",
    "    max_2 = np.max(X_true) ** 2\n",
    "\n",
    "    return 10 * np.log10(max_2 / mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(image, image + np.ones_like(image) * 0.0000000000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кластеризуем, и все пиксели, отнесенные в один кластер, попробуем заполнить двумя способами: медианным и средним цветом по кластеру. Таким образом мы \"сжимаем\" изображение. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_image = skimage.img_as_float(image, force_copy=True)\n",
    "float_image.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведём в набор троек RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB = ...\n",
    "\n",
    "RGB.shape, RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('Lenna', exist_ok=True)\n",
    "\n",
    "clusters = [25] + list(reversed(range(2, 10)))\n",
    "\n",
    "for n_clusters in clusters:\n",
    "    \n",
    "    print(n_clusters, end=\" \")\n",
    "    \n",
    "    # группируем цвета, которые есть в изображении\n",
    "    model = KMeans(n_clusters=n_clusters, verbose=False, random_state=100, n_init=10)\n",
    "    model.fit(RGB)\n",
    "    X = RGB.copy()\n",
    "    \n",
    "    # запоминаем метки кластеров\n",
    "    labels = model.labels_.T\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        \n",
    "        # по каким индексам в списке пикселей живёт этот кластер?\n",
    "        ...\n",
    "        \n",
    "        # заполняем заполненное средним\n",
    "        ...\n",
    "        \n",
    "    # обратно в трёхмерный вид\n",
    "    im = ...\n",
    "    \n",
    "    print(f\"Frobenius norm: {np.linalg.norm(im - float_image):3.2f}\" , end=\" | \")    \n",
    "    print(f\"PSNR: {psnr(float_image, im):3.3f}\" % psnr(float_image, im))\n",
    "    \n",
    "    rescaled_im = (im * 255).astype(np.uint8)\n",
    "    \n",
    "    # сохраняем\n",
    "    imsave(\"Lenna/\" + str(n_clusters) + \".png\", rescaled_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = imread(\"Lenna/6.png\")\n",
    "# io.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i, c in enumerate(clusters):\n",
    "    img = imread(f\"Lenna/{c}.png\")\n",
    "    ax = fig.add_subplot(3, 3, i+1)\n",
    "    ax.title.set_text(f'{c} clusters')\n",
    "    plt.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Case study 2: слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget http://vectors.nlpl.eu/repository/20/182.zip\n",
    "# ! bash -c \"unzip -o 182.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть три файла - `meta.json` с метаданными, `lemma.num` с частотами слов и `model.txt` собственно с эмбеддингами.\n",
    "\n",
    "Давайте кластеризовывать слова на основе их векторных представлений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! bash -c \"tail -2 model.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "STOPS = set(stopwords.words(\"russian\"))\n",
    "STOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбрасываем стоп-слова\n",
    "freq_list = ...\n",
    "len(freq_list)\n",
    "\n",
    "for i, item in enumerate(freq_list):\n",
    "    print(item)\n",
    "    if i == 5:\n",
    "        break\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "raw_vectors = []\n",
    "\n",
    "with open(\"model.txt\", \"r+\", encoding=\"utf-8\") as rf:\n",
    "\n",
    "    # пропускаем первую строку\n",
    "    next(rf)\n",
    "\n",
    "    for line in tqdm(rf):\n",
    "        # parse line, extract only vectors corresponding to frequent nouns\n",
    "        ...\n",
    "\n",
    "\n",
    "len(tokens), len(raw_vectors)\n",
    "\n",
    "token2id = {t: i for i, t in enumerate(tokens)}\n",
    "vectors = np.array(raw_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормализуем?\n",
    "vectors = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape, vectors.sum(axis=1).shape, vectors.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "king = raw_vectors[token2id[\"король\"]]\n",
    "queen = raw_vectors[token2id[\"королева\"]]\n",
    "\n",
    "man = raw_vectors[token2id[\"мужчина\"]]\n",
    "woman = raw_vectors[token2id[\"женщина\"]]\n",
    "cosine(king - man, queen - woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "mbk_means = cluster.MiniBatchKMeans(\n",
    "    n_clusters=70,\n",
    "    batch_size=1000,\n",
    "    max_iter=10000,\n",
    "    n_init=20,\n",
    "    random_state=100,\n",
    "    reassignment_ratio=0.1,\n",
    ")\n",
    "mbk_means.fit(vectors)\n",
    "\n",
    "mbk_means.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2clusters = {t: c for t, c in zip(tokens, mbk_means.labels_)}\n",
    "cluster2tokens = {l: [] for l in mbk_means.labels_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, c in tokens2clusters.items():\n",
    "    cluster2tokens[c].append(t)\n",
    "\n",
    "for c in sorted(list(cluster2tokens)[:50]):\n",
    "    print(f\"\\n>-- Cluster #{c}, {len(cluster2tokens[c])} objects.\")\n",
    "    print(\" \".join([w for w in cluster2tokens[c][:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Case study 3: dedublication\n",
    "\n",
    "Довольно часто бывает нужно избавиться от неточных дубликатов коротких текстов, и никаких особых методов родом из предметной области (или внешних вспомогательных данных) для этого нет. Кроме того, как первое приближение для понимания, о чём вообще все эти короткие тексты, как их можно потом размечать, объединять в группы и так далее, -- может помочь кластеризация. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [w.strip() for w in open(\"fallout_possible_items.txt\", \"r+\", encoding=\"utf-8\").readlines() if w.strip()]\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(set(items))\n",
    "sorted(items)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А нам точно понадобится машинное обучение?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity[idx, :][:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим своеобразный формат для `scipy.cluster`: треугольный кусок матрицы под диагональю, представленный списком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = 1 - similarity\n",
    "distances_prepared = []\n",
    "\n",
    "for i in tqdm(range(distances.shape[0]), \"distances matrix rows\"):\n",
    "    for j in range(distances.shape[0]):\n",
    "        if i < j:\n",
    "            distances_prepared.append(distances[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "inside_cluster_dist = 0.4\n",
    "\n",
    "Z = linkage(np.array(distances_prepared), method=\"ward\")\n",
    "result = fcluster(Z, t=inside_cluster_dist, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дендрограммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "# TODO: слишком много всего, надо нарисовать только часть входов\n",
    "\n",
    "plt.figure(figsize=(8, 15))\n",
    "dendrogram(Z, orientation=\"left\", labels=items)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {i: [] for i in result}\n",
    "\n",
    "for i, idx in enumerate(result):\n",
    "    clusters[idx].append(items[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in clusters:\n",
    "    print(\"\\nCluster\", cluster)\n",
    "    print(clusters[cluster])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 4: Червяки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x, y = [], []\n",
    "\n",
    "with open(\"worms/worms_2d.txt\", \"r+\", encoding=\"utf-8\") as rf:\n",
    "    for line in rf:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            spl = line.split(\" \")\n",
    "            x.append(float(spl[0]))\n",
    "            y.append(float(spl[1]))\n",
    "\n",
    "x, y = np.array(x), np.array(y)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.xlim(1500, 5500)\n",
    "plt.ylim(2000, 5500)\n",
    "plt.scatter(x, y, s=0.5, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще у нас в этот раз есть правильные метки\n",
    "labels = []\n",
    "\n",
    "with open(\"worms/worms_2d-gt.pa\", \"r+\", encoding=\"utf-8\") as rf:\n",
    "    for line in rf.readlines()[4:]:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            labels.append(int(line))\n",
    "\n",
    "print(\"Labels count:\", len(set(labels)))\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "assert x.shape == labels.shape == y.shape\n",
    "\n",
    "plt.xlim(1500, 5500)\n",
    "plt.ylim(2000, 5500)\n",
    "plt.scatter(x, y, s=0.5, alpha=0.05, c=labels, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Казалось бы, чего тут сложного... Давайте попробуем это кластеризовать, что ли"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание со звездочкой 17. Dunn index\n",
    "Реализуйте Dunn Index. Будет засчитываться не абы-какая реализация, а соответствующая по стилю и оформлению реализации метрик в sklearn: обязательны докстринги, валидаторы и все такое.\n",
    "\n",
    "То, как реализованы другие метрики, можно посмотреть [тут](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/cluster/_unsupervised.py#L195)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils._param_validation import (\n",
    "    validate_params,\n",
    ")\n",
    "\n",
    "@validate_params(...)\n",
    "def dunn_score(...):\n",
    "    \"\"\"\n",
    "    Compute the Dunn Index by given within-cluster distances (callable or precomputed) and\n",
    "    between-cluster distances(callable or precomputed).\n",
    "    ...\n",
    "    \n",
    "    \"\"\"\n",
    "    ...\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание со звездочкой 18. Кластеризация текстов\n",
    "\n",
    "Мы сделали кластеризацию **слов** на основе их векторных представлений. На самом деле, кластеризовывать тексты гораздо сложнее и интереснее; для этой задачи существуют специальные модели, эти модели обучаются на **корпусе текстов**. Мы поговорим о таких моделях в следующем семестре, и быстро обобщим их до **тематического моделирования**, но это будет потом. А что делать, если датасета с текстами нет? Наивный подход состоит в том, чтобы как-нибудь аггрегировать векторные представления слов, например взять их среднее.\n",
    "\n",
    "Ваша задача - предложите способ кластеризации **предложений**, который бы использовал только векторные представления слов, реализуйте и продемонстрируйте, что ваш способ работает лучше, чем наивный. \n",
    "\n",
    "**Подсказка:** _того, что было рассказано сегодня, должно быть достаточно, нужно только грамотно скомпоновать разные части практики._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
