{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b1bf1f-89e0-4da0-ba01-923248293828",
   "metadata": {},
   "source": [
    "# Gaussian Processes II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04e04b-1add-48cb-80d3-4e38731e77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551c9f76-1c7c-4ea6-91f4-caa78ddf4b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.special import expit as sigmoid\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 600\n",
    "plt.rcParams[\"savefig.format\"] = \"pdf\"\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1c733-e495-423c-9f1b-cddf6656b1d0",
   "metadata": {},
   "source": [
    "## GP for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c898471-db7f-45bb-93e6-123dab91eb18",
   "metadata": {},
   "source": [
    "### How to find a the kernel parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341d345-487a-4d4e-aaed-31bb7d057800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(a, b, theta):\n",
    "    sq_dist = (a**2).sum(1).reshape(-1, 1) + (b**2).sum(1) - 2 * a @ b.T\n",
    "    return theta[1] * np.exp(-0.5 * theta[0] * sq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea64c29-a8d0-46f5-99d9-cb0010a429d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points sampled from sin\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 2 * np.pi, 10)[:, None]\n",
    "y = np.sin(X) + 0.1 * np.random.randn(*X.shape)\n",
    "X_test = np.linspace(0, 2 * np.pi, 100)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e52fb69-62b8-4c01-91f9-9a7eb09f8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood(params, X, y):\n",
    "    theta = params[:2]\n",
    "    noise = params[2]\n",
    "    K = kernel(X, X, theta) + noise**2 * np.eye(len(X))\n",
    "    try:\n",
    "        L = cholesky(K, lower=True)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.inf\n",
    "    # we had:\n",
    "    # n_log_likelihood = -sp.stats.multivariate_normal(\n",
    "    #    mu, Sigma, allow_singular=False\n",
    "    # ).logpdf(y)\n",
    "\n",
    "    alpha = solve_triangular(L.T, solve_triangular(L, y, lower=True))\n",
    "    return (\n",
    "        0.5 * y.T @ alpha\n",
    "        + np.sum(np.log(np.diag(L)))\n",
    "        + 0.5 * len(X) * np.log(2 * np.pi)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428b2a3-ae2e-4f7d-9233-0f651632fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = [30.0, 50.0, 0.1]  # [theta0, theta1, noise]\n",
    "result = minimize(\n",
    "    neg_log_likelihood,\n",
    "    initial_params,\n",
    "    args=(X, y),\n",
    "    bounds=((1e-5, None), (1e-5, None), (1e-5, None)),\n",
    "    method=\"L-BFGS-B\",\n",
    ")\n",
    "theta_opt = result.x[:2]\n",
    "noise_opt = result.x[2]\n",
    "theta_opt, initial_params[:2],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba0596-8ecc-4135-a451-2e5e41c5d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log_likelihood(initial_params, X, y), neg_log_likelihood(result.x, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1773c-8c6e-4aad-9d0d-b6655e377e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_opt, initial_params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f06e3-09b9-450c-ad50-71c30be456bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_predict(X_train, y_train, X_test, theta, noise):\n",
    "    K = kernel(X_train, X_train, theta) + noise**2 * np.eye(len(X_train))\n",
    "    K_star = kernel(X_train, X_test, theta)\n",
    "    K_starstar = kernel(X_test, X_test, theta)\n",
    "\n",
    "    L = cholesky(K, lower=True)\n",
    "    # we had: \n",
    "    # mu = K_star.T @ np.linalg.inv(K + 1e-6 * np.eye(N, N)) @ y \n",
    "    alpha = solve_triangular(L.T, solve_triangular(L, y_train, lower=True))\n",
    "    mu = K_star.T @ alpha\n",
    "\n",
    "    # we had: \n",
    "    # K_predict = K_star2 - K_star.T @ np.linalg.inv(K + 1e-6 * np.eye(N, N)) @ K_star\n",
    "    # s2 = np.diag(K_predict)\n",
    "    v = solve_triangular(L, K_star, lower=True) \n",
    "    cov = K_starstar - v.T @ v\n",
    "    return mu, cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98ee3a-62c3-403e-bba6-8fc975591fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfitted (initial parameters)\n",
    "mu_initial, cov_initial = gp_predict(\n",
    "    X, y, X_test, initial_params[:2], initial_params[2]\n",
    ")\n",
    "std_initial = np.sqrt(np.diag(cov_initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dcfd67-d226-4a3d-9fc9-4f99bc55aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitted (optimized parameters)\n",
    "mu_opt, cov_opt = gp_predict(X, y, X_test, theta_opt, noise_opt)\n",
    "std_opt = np.sqrt(np.diag(cov_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b321a-4232-44ed-91bc-610b34f5c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X, y, c=\"k\", label=\"Training Data\")\n",
    "plt.plot(X_test, np.sin(X_test), \"g--\", label=\"True Function\")\n",
    "plt.plot(X_test, mu_initial, \"b-\", lw=1, label=\"Unfitted GP\")\n",
    "# plt.fill_between(X_test.ravel(), mu_initial-2*std_initial, mu_initial+2*std_initial, alpha=0.1)\n",
    "plt.plot(X_test, mu_opt, \"r-\", lw=1, label=\"Fitted GP\")\n",
    "# plt.fill_between(X_test.ravel(), mu_opt-2*std_opt, mu_opt+2*std_opt, alpha=0.1)\n",
    "plt.title(\"GP Regression Fit\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c2fba-a571-4693-9968-19bf08ec3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 2)\n",
    "theta0_grid = np.linspace(theta_opt[0] - 0.2, theta_opt[0] + 0.3, 50)\n",
    "theta1_grid = np.linspace(theta_opt[1] - 0.2, theta_opt[1] + 0.3, 50)\n",
    "Theta0, Theta1 = np.meshgrid(theta0_grid, theta1_grid)\n",
    "nll = np.zeros_like(Theta0)\n",
    "\n",
    "for i in range(Theta0.shape[0]):\n",
    "    for j in range(Theta0.shape[1]):\n",
    "        params = [Theta0[i, j], Theta1[i, j], noise_opt]\n",
    "        nll[i, j] = neg_log_likelihood(params, X, y)\n",
    "\n",
    "plt.contourf(Theta0, Theta1, nll, levels=20, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"Negative Log Likelihood\")\n",
    "plt.scatter(*theta_opt, c=\"red\", marker=\"x\", label=\"Optimum\")\n",
    "plt.xlabel(\"Theta0 (1/length_scale²)\")\n",
    "plt.ylabel(\"Theta1 (variance)\")\n",
    "plt.title(\"Log-Likelihood Landscape\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c99c1d2-3bd8-4008-a12e-8900444c8a06",
   "metadata": {},
   "source": [
    "### Reminder: Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503c361-0546-4193-b564-849999934072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = 4\n",
    "scale = 1\n",
    "loc = 0\n",
    "# target_dist = stats.gamma(a=a, scale=scale)\n",
    "target_dist = stats.skewnorm(a, loc, scale)\n",
    "\n",
    "\n",
    "def neg_log_p(x):\n",
    "    return -target_dist.logpdf(x)\n",
    "\n",
    "\n",
    "result = minimize(neg_log_p, x0=np.array([1.0]), method=\"BFGS\")\n",
    "mode = result.x[0]\n",
    "print(f\"Mode found at: {mode:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444047e5-80b6-49ec-b1b9-7cbfd639d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the second derivative of the log-density at the mode numerically\n",
    "h = 1e-5\n",
    "deriv2 = ...\n",
    "print(f\"Second derivative at mode: {deriv2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580ae52-489d-4e4d-a9aa-0750d27a1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_laplace = -1 / deriv2\n",
    "std_laplace = np.sqrt(var_laplace)\n",
    "print(f\"Laplace approximation variance: {var_laplace:.4f}\")\n",
    "\n",
    "laplace_approx = stats.norm(loc=mode, scale=std_laplace)\n",
    "\n",
    "# Plot both distributions\n",
    "x = np.linspace(-6, 6, 500)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, target_dist.pdf(x), label=\"Skewed Normal\")\n",
    "plt.plot(x, laplace_approx.pdf(x), \"--\", label=\"Laplace Approximation\")\n",
    "plt.title(\"Laplace Approximation of a Skewed Normal Distribution\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bad495-a88c-4e11-ac3b-5825eea71641",
   "metadata": {},
   "source": [
    "## GP for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4b88d-b84b-4233-9e41-6088f420e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp(mu, cov, X, X_train=None, Y_train=None, samples=[]):\n",
    "    X = X.ravel()\n",
    "    mu = mu.ravel()\n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\n",
    "\n",
    "    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.1)\n",
    "    plt.plot(X, mu, label=\"Mean\")\n",
    "    for i, sample in enumerate(samples):\n",
    "        plt.plot(X, sample, lw=1, ls=\"--\", label=f\"Sample {i+1}\")\n",
    "    if X_train is not None:\n",
    "        plt.plot(X_train, Y_train, \"rx\")\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_gp_2D(gx, gy, mu, X_train, Y_train, title, i):\n",
    "    ax = plt.gcf().add_subplot(1, 2, i, projection=\"3d\")\n",
    "    ax.plot_surface(\n",
    "        gx,\n",
    "        gy,\n",
    "        mu.reshape(gx.shape),\n",
    "        cmap=cm.coolwarm,\n",
    "        linewidth=0,\n",
    "        alpha=0.2,\n",
    "        antialiased=False,\n",
    "    )\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], Y_train, c=Y_train, cmap=cm.coolwarm)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_data_1D(X, t):\n",
    "    class_0 = t == 0\n",
    "    class_1 = t == 1\n",
    "\n",
    "    plt.scatter(X[class_1], t[class_1], label=\"Class 1\", marker=\"x\", color=\"red\")\n",
    "    plt.scatter(\n",
    "        X[class_0],\n",
    "        t[class_0],\n",
    "        label=\"Class 0\",\n",
    "        marker=\"o\",\n",
    "        edgecolors=\"blue\",\n",
    "        facecolors=\"none\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_data_2D(X, t):\n",
    "    class_1 = np.ravel(t == 1)\n",
    "    class_0 = np.ravel(t == 0)\n",
    "\n",
    "    plt.scatter(X[class_1, 0], X[class_1, 1], label=\"Class 1\", marker=\"x\", c=\"red\")\n",
    "    plt.scatter(\n",
    "        X[class_0, 0],\n",
    "        X[class_0, 1],\n",
    "        label=\"Class 0\",\n",
    "        marker=\"o\",\n",
    "        edgecolors=\"blue\",\n",
    "        facecolors=\"none\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.ylabel(\"$x_2$\")\n",
    "\n",
    "\n",
    "def plot_py_2D(grid_x, grid_y, grid_z):\n",
    "    plt.contourf(\n",
    "        grid_x, grid_y, grid_z, cmap=\"plasma\", alpha=0.3, levels=np.linspace(0, 1, 11)\n",
    "    )\n",
    "    plt.colorbar(format=\"%.2f\")\n",
    "\n",
    "\n",
    "def plot_db_2D(grid_x, grid_y, grid_z, decision_boundary=0.5):\n",
    "    levels = [decision_boundary]\n",
    "    cs = plt.contour(\n",
    "        grid_x,\n",
    "        grid_y,\n",
    "        grid_z,\n",
    "        levels=levels,\n",
    "        colors=\"black\",\n",
    "        linestyles=\"dashed\",\n",
    "        linewidths=2,\n",
    "    )\n",
    "    plt.clabel(cs, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181aa61-779c-45a0-90d9-a1a2e43b7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = np.arange(0, 5, 0.2).reshape(-1, 1)\n",
    "X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)\n",
    "\n",
    "a = np.sin(X * np.pi * 0.5) * 2\n",
    "y = bernoulli.rvs(sigmoid(a))\n",
    "\n",
    "plot_data_1D(X, y)\n",
    "plt.title(\"1D training dataset\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$t$\")\n",
    "plt.yticks([0, 1])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b04b475-6162-4924-bf11-cc3d1255ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_(X, theta, diag_only=False, nu=1e-5):\n",
    "    \"\"\"K + \\nu I\"\"\"\n",
    "    if diag_only:\n",
    "        # Specific solution for isotropic squared exponential kernel.\n",
    "        return theta[1] ** 2 + nu\n",
    "    else:\n",
    "        return kernel(X, X, theta) + nu * np.eye(X.shape[0])\n",
    "\n",
    "\n",
    "def W_(a):\n",
    "    \"\"\"W\"\"\"\n",
    "    r = sigmoid(a) * (1 - sigmoid(a))\n",
    "    return np.diag(r.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fc825-ebca-4a22-bbd3-e6e12d0d8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_mode(X, y, K, max_iter=10, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the mode of posterior p(f|y).\n",
    "    \"\"\"\n",
    "    f = np.zeros_like(y)\n",
    "    I = np.eye(X.shape[0])\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        W = W_(f)\n",
    "        Q_inv = np.linalg.inv(I + W @ K)\n",
    "        f_new = (K @ Q_inv).dot(y - sigmoid(f) + W.dot(f))\n",
    "        f_diff = np.abs(f_new - f)\n",
    "        f = f_new\n",
    "\n",
    "        if not np.any(f_diff > tol):\n",
    "            break\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f6193-ed91-4207-a972-6579399b480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_fn(X, y):\n",
    "    \"\"\"\n",
    "    Returns the negative log-likelihood function for data X, y.\n",
    "    P(y |X) as a function of theta\n",
    "    \"\"\"\n",
    "\n",
    "    y = y.ravel()\n",
    "\n",
    "    def nll(theta):\n",
    "        ...\n",
    "        return -ll\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb91550-f3eb-4d4d-9b14-cbd896af073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(\n",
    "    nll_fn(X, y), [1, 1], bounds=((1e-3, None), (1e-3, None)), method=\"L-BFGS-B\"\n",
    ")\n",
    "\n",
    "theta = res.x\n",
    "\n",
    "print(\n",
    "    f\"Optimized theta = [{theta[0]:.3f}, {theta[1]:.3f}], negative log likelihood = {res.fun:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266416c-4ba7-4eff-b9e5-63fc20915e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_f(X_test, X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the mean and variance of logits at points X_test\n",
    "    given training data X, y and kernel parameters theta.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "    return f_test_mu, f_test_var2\n",
    "\n",
    "\n",
    "def predict_py(X_test, X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the probability of y=1 at points X_test\n",
    "    given training data X, y and kernel parameters theta.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d62adc8-6c03-4f93-bd68-0c43ba4613c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mu, f_var2 = predict_f(X_test, X, y, theta)\n",
    "f_mu.shape, f_var2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3014a7-542d-4f54-b154-bcbb52c01d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_test = predict_py(X_test, X, y, theta)\n",
    "pt_test.shape\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plot_data_1D(X, y)\n",
    "plt.plot(X_test, pt_test, label=\"Prediction\", color=\"green\")\n",
    "\n",
    "plt.axhline(0.5, X_test.min(), X_test.max(), color=\"black\", ls=\"--\", lw=0.5)\n",
    "plt.title(\"Predicted class 1 probabilities\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$p(t_*=1|t)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92b652-4b4f-44ee-89a6-27e88bd8f1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_test_mu, f_test_var = predict_f(X_test, X, y, theta)\n",
    "f_test_mu = f_test_mu.ravel()\n",
    "f_test_var = f_test_var.ravel()\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(X_test, f_test_mu, label=r\"logits mean $\\mu_{a_*}$\", color=\"green\", alpha=0.3)\n",
    "plt.fill_between(\n",
    "    X_test.ravel(),\n",
    "    f_test_mu + f_test_var,\n",
    "    f_test_mu - f_test_var,\n",
    "    label=r\"logits variance $\\sigma^2_{a_*}$\",\n",
    "    color=\"lightcyan\",\n",
    ")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7c8d7-714f-4ac0-a195-74d8cc2ecd33",
   "metadata": {},
   "source": [
    "## sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f50520e-a789-4f6a-9a55-5331d3ea33d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=rbf)\n",
    "\n",
    "gpc.fit(X, y.ravel())\n",
    "\n",
    "# Obtain optimized kernel parameters\n",
    "sklearn_theta_0 = gpc.kernel_.k2.get_params()[\"length_scale\"]\n",
    "sklearn_theta_1 = np.sqrt(gpc.kernel_.k1.get_params()[\"constant_value\"])\n",
    "\n",
    "sklearn_theta_0, sklearn_theta_1, -gpc.log_marginal_likelihood_value_\n",
    "print(\n",
    "    f\"Optimized theta = [{sklearn_theta_0:.3f}, {sklearn_theta_1:.3f}], negative log likelihood = {-gpc.log_marginal_likelihood_value_:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab33c6-4ab3-437a-baa3-2cc7616e89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_test_gpc = gpc.predict_proba(X_test.reshape(-1, 1))[:, 1]\n",
    "\n",
    "plot_data_1D(X, y)\n",
    "plt.plot(X_test, pt_test, label=\"Prediction\", color=\"green\")\n",
    "plt.plot(X_test, pt_test_gpc, label=\"Prediction (GPC)\", color=\"black\", ls=\":\")\n",
    "plt.title(\"Comparison with GaussianProcessClassifier (GPC)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(r\"$p(t_*=1|\\mathbf{t})$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d40bb4e-2f52-42cc-bec4-a50baf463ed3",
   "metadata": {},
   "source": [
    "## 2D Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666e1197-a59c-4477-b287-259c2197d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(200, noise=0.5, random_state=1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "plot_data_2D(X, y)\n",
    "plt.title(\"2D training dataset\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f0a08-80ef-442b-a24b-604d5a712262",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(\n",
    "    nll_fn(X, y), [1, 1], bounds=((1e-3, None), (1e-3, None)), method=\"L-BFGS-B\"\n",
    ")\n",
    "\n",
    "theta = res.x\n",
    "\n",
    "print(\n",
    "    f\"Optimized theta = [{theta[0]:.3f}, {theta[1]:.3f}], negative log likelihood = {res.fun:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8ef9d-1575-4604-905f-b162681e27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=rbf)\n",
    "\n",
    "gpc.fit(X, y.ravel())\n",
    "\n",
    "# Obtain optimized kernel parameters\n",
    "sklearn_theta_0 = gpc.kernel_.k2.get_params()[\"length_scale\"]\n",
    "sklearn_theta_1 = np.sqrt(gpc.kernel_.k1.get_params()[\"constant_value\"])\n",
    "\n",
    "print(\n",
    "    f\"Optimized theta = [{sklearn_theta_0:.3f}, {sklearn_theta_1:.3f}], negative log likelihood = {-gpc.log_marginal_likelihood_value_:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b5690-300c-4ab9-9035-eac4f31b3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x, grid_y = np.mgrid[-4:4:200j, -4:4:200j]\n",
    "grid = np.stack([grid_x, grid_y], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25150a-2e98-49c1-a64a-c519abd1f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_test = predict_py(grid.reshape(-1, 2), X, y, theta).reshape(*grid_x.shape)\n",
    "\n",
    "plot_pt_2D(grid_x, grid_y, pt_test)\n",
    "plot_db_2D(grid_x, grid_y, pt_test, decision_boundary=0.5)\n",
    "plot_data_2D(X, y)\n",
    "plt.title(\"Predicted class 1 probabilities\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706a89d-7c5d-44c7-abfd-0d28529ea994",
   "metadata": {},
   "source": [
    "## Задача со звездочкой 22: Свертка\n",
    "Докажите, что \n",
    "$$\n",
    "\\int\\Phi(\\lambda a) N(a|\\mu, \\Sigma) = \\Phi(\\frac{\\mu}{\\sqrt{\\lambda^{-2} + \\sigma^2}})\n",
    "$$\n",
    "\n",
    "\n",
    "## Задача со звездочкой 23: Метод Ньютона\n",
    "Метод Ньютона - итеративный алгоритм для поиска корней функции $f(x)$:\n",
    "\n",
    "$x_{new} = x_{old} - J_f(x_{old})^{-1} f(x_{old})$\n",
    "\n",
    "где $J$ - матрица Якоби. Выведите из этого определения формулу, которую мы использовали для поиска моды $\\mathbb{P}(f|y)$:\n",
    "\n",
    "$$\n",
    "f^{new} = K(I+WK)^{-1} \\cdot \\Big[y-\\sigma(f)+Wf\\Big]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b2606-22d5-4524-b365-ba800bd4023d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "This noptebook rely heavily on Martin Krasser's [course](https://github.com/krasserm) on Bayessian Machine Learning "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teaching",
   "language": "python",
   "name": "teaching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
