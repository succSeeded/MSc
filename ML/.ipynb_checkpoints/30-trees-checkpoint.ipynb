{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "from IPython.display import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data = pd.read_csv('tennis.csv.txt')\n",
    "toy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A decision tree for data above\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt4.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decision Tree Algorithms\n",
    "\n",
    "| Feature                | ID.3 (Ross Quinlan et al.)    | CART (Leo Breiman et al.)     | C4.5, C5 (Ross Quinlan et al.) |\n",
    "|------------------------|-------------------------------|-------------------------------|-------------------------------|\n",
    "| **Splitting Criterion**| Information Gain              | Gini (classification) / Variance (regression) | Gain Ratio                   |\n",
    "| **Data Types**         | Categorical only              | Categorical + Continuous      | Categorical + Continuous      |\n",
    "| **Tree Structure**     | Multi-way                     | Binary                        | Multi-way for categorical features, binary for numerical           |\n",
    "| **Pruning**            | None                          | Cost-complexity               | Pessimistic pruning           |\n",
    "| **Task Support**       | Classification                | Classification + Regression   | Classification                |\n",
    "| **Overfitting**        | High (no pruning)             | Moderate (pruning, binary splits) | Low (pruning, gain ratio) |\n",
    "| **Missing Values**     | Not handled                   | Surrogate splits              | Fractional instances          |\n",
    "\n",
    "**scikit-learn decision trees are based on CART**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Criterions a.k.a Impurity measures\n",
    "\n",
    "In the decision tree, each node corresponds to a subset of the train data $S$. Let $\\{p_i\\}_{i=1}^C$ be the target distribution over $C$ classes for this subset $S$.\n",
    "\n",
    "* __Entropy:__ $H(S) = -\\sum p_i\\log(p_i)$\n",
    "* __Gini Index:__ $Gini(S) = 1-\\sum p_i^2$\n",
    "\n",
    "> NOTE: the perfomance is usually similar, it doesn't matter which one to chose\n",
    "\n",
    "These are an **Impurity measures** of single node, now we want to select an optimal split. For candidate $S=S_1 \\sqcup S_2 \\dots \\sqcup S_{K_A}$ respecting the values of feature $A$\n",
    "\n",
    "* __Information gain__, aka mutual information $$IG(S, A) = H(S) - H(S|A) = I(S;A) = \\sum_{i=1}^{K_A}\\frac{|S_i|}{|S|}H(S_i)$$\n",
    "> NOTE: What are the limits of IG values? Can it be nega\n",
    "* __Gain Ratio__: $$GainRatio(S, A) = IG(S, A) \\cdot \\frac{1}{-\\sum_{i=1}^{K_A} \\frac{|S_i|}{|S|}\\log\\frac{|S_i|}{|S|}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Construction for the toy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy of play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data.play.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play = ...\n",
    "Entropy_Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain on splitting by Outlook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'sunny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=Sunny)\n",
    "Entropy_Play_Outlook_Sunny = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play_Outlook_Sunny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'overcast']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=overcast)\n",
    "# Since, it's a homogenous data entropy will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'rainy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=rainy)\n",
    "Entropy_Play_Outlook_Rain = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play_Outlook_Rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gain on splitting by attribute outlook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Play - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other gains\n",
    "* Gain(Play, Temperature) - 0.029\n",
    "* Gain(Play, Humidity) - 0.151\n",
    "* Gain(Play, Wind) - 0.048\n",
    "\n",
    "#### Conclusion - Outlook is winner & thus becomes root of the tree\n",
    "<img src=\"https://i1.wp.com/sefiks.com/wp-content/uploads/2017/11/tree-v1.png?zoom=1.25&resize=728%2C252&ssl=1\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to find the next splitting criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'overcast']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion - If outlook is overcast, play is true\n",
    "\n",
    "### Let's find the next splitting feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_data[toy_data.outlook == 'sunny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy(Play|Outlook=Sunny)\n",
    "Entropy_Play_Outlook_Sunny = ...\n",
    "Entropy_Play_Outlook_Sunny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for windy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entropy_Windy_False = ...\n",
    "Entropy_Windy_True = ...\n",
    "IG_Windy= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain for temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Humidity is the best choice on sunny branch\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt3.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Tree\n",
    "\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt4.PNG?raw=true\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn\n",
    "[The guide on decision trees is awesome](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz, ExtraTreeClassifier\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX,trainY,testY = train_test_split(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(dt,'dt.tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the tree\n",
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt6.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.predict(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE:\n",
    "> * CART (which is used inside sklearn DecisionTreeClassifier) will convert features with continues values into categorical values\n",
    "> * Different tree will be generated with same faeatures given in different order. This is because `max_features` algorithm and random feature subsampling inside.\n",
    "\n",
    "#### Feature Importances\n",
    "* Important features will be higher up the tree\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n",
    "Warning: impurity-based feature importances can be misleading for high cardinality features (many unique values). See sklearn.inspection.permutation_importance as an alternative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Decision Boundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = make_blobs(n_features=2, n_samples=1000, cluster_std=.8, centers=4, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=Y,s=5, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_step = 0.2\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = dt.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=Y,s=5,cmap='viridis')\n",
    "plt.scatter(xx.ravel(),yy.ravel(),c=outcome,s=1,alpha=1, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/awantik/machine-learning-slides/blob/master/dt8.PNG?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Implementation\n",
    "\n",
    "For now, we will focus on the ID3 algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "# Load dataset and convert to DataFrame\n",
    "dataset = {\n",
    "    'Taste': ['Salty','Spicy','Spicy','Spicy','Spicy','Sweet','Salty','Sweet','Spicy','Salty'],\n",
    "    'Temperature': ['Hot','Hot','Hot','Cold','Hot','Cold','Cold','Hot','Cold','Hot'],\n",
    "    'Texture': ['Soft','Soft','Hard','Hard','Hard','Soft','Soft','Soft','Soft','Hard'],\n",
    "    'Eat': ['No','No','Yes','No','Yes','Yes','No','Yes','Yes','Yes']\n",
    "}\n",
    "dataframe = pd.DataFrame(dataset, columns=['Taste','Temperature','Texture','Eat'])\n",
    "\n",
    "# Prepare features and target\n",
    "feature_names = dataframe.columns[:-1].tolist()\n",
    "X = dataframe[feature_names].to_numpy()\n",
    "y = dataframe['Eat'].to_numpy()\n",
    "features = list(range(len(feature_names)))\n",
    "\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y: np.ndarray) -> float:\n",
    "    \"\"\"Calculate the entropy of a target array.\"\"\"\n",
    "    ...\n",
    "\n",
    "entropy(np.array([1,1,1,0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X: np.ndarray, y: np.ndarray, feature_idx: int) -> float:\n",
    "    \"\"\"Calculate the information gain for a given feature.\"\"\"\n",
    "    ...\n",
    "    return parent_entropy - conditional_entropy\n",
    "\n",
    "# TODO:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_feature(X: np.ndarray, y: np.ndarray, features: list) -> int:\n",
    "    \"\"\"Select the feature with the highest information gain.\"\"\"\n",
    "    ...\n",
    "    return features[best_idx]\n",
    "\n",
    "# TODO: test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_algorithm(X: np.ndarray, y: np.ndarray, features: list) -> Dict[str, Any]:\n",
    "    \"\"\"Recursively build the ID3 decision tree.\"\"\"\n",
    "    classes, counts = ...\n",
    "    majority_class = ...\n",
    "    \n",
    "    # Base case: all samples same class\n",
    "    if len(classes) == 1 or not features:\n",
    "        return {'class': classes[0], 'majority_class': classes[0]}\n",
    "       \n",
    "    best_feature = ...\n",
    "    feature_values = ...\n",
    "    new_features = ...\n",
    "    \n",
    "    tree = {\n",
    "        'feature': best_feature,\n",
    "        'majority_class': majority_class,\n",
    "        'children': {}\n",
    "    }\n",
    "    \n",
    "    for value in feature_values:\n",
    "        mask = ...\n",
    "        X_sub, y_sub = X[mask], y[mask]\n",
    "        if len(y_sub) == 0:\n",
    "            tree['children'][value] = {'class': majority_class, 'majority_class': majority_class}\n",
    "        else:\n",
    "            tree['children'][value] = id3_algorithm(X_sub, y_sub, new_features)\n",
    "    \n",
    "    return tree\n",
    "\n",
    "# TODO:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree\n",
    "decision_tree = id3_algorithm(X, y, features.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(tree: Dict[str, Any], feature_names: list, dot: Digraph = None, parent: str = None, edge_label: str = None) -> Digraph:\n",
    "    \"\"\"Recursively visualize the decision tree using Graphviz.\"\"\"\n",
    "    if dot is None:\n",
    "        dot = Digraph(comment='Decision Tree')\n",
    "    \n",
    "    # Create a unique node ID\n",
    "    node_id = str(id(tree))\n",
    "    \n",
    "    # Add the current node\n",
    "    if 'class' in tree:\n",
    "        node_label = f\"Class: {tree['class']}\"\n",
    "    else:\n",
    "        node_label = f\"Feature: {feature_names[tree['feature']]}\"\n",
    "    dot.node(node_id, node_label)\n",
    "    \n",
    "    # Connect to parent node if exists\n",
    "    if parent is not None:\n",
    "        dot.edge(parent, node_id, label=edge_label)\n",
    "    \n",
    "    # Recursively add children\n",
    "    if 'children' in tree:\n",
    "        for value, child in tree['children'].items():\n",
    "            visualize_tree(child, feature_names, dot, node_id, str(value))\n",
    "    \n",
    "    return dot\n",
    "\n",
    "# Visualize the decision tree\n",
    "dot = visualize_tree(decision_tree, feature_names)\n",
    "display(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree: Dict[str, Any], feature_names: list, data_point: Dict[str, str]) -> str:\n",
    "    \"\"\"Predict the class for a new data point using the decision tree.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data point for prediction\n",
    "new_data_point = {'Taste': 'Salty', 'Temperature': 'Hot', 'Texture': 'Hard'}\n",
    "prediction = predict(decision_tree, feature_names, new_data_point)\n",
    "\n",
    "print(f\"Prediction for the new data point: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning\n",
    "* Trees always tends to overfit\n",
    "* A technique of machine learning which reduces height of the tree by chopping off parts of the tree that's not doing anything significant in prediction\n",
    "* Prepruning & Postpruning\n",
    "  - Prepruning : Don't allow tree to grow beyond this point (`min_leaf_size`, `max_depth`, `min_impurity_decrease`)\n",
    "  - Postpruning : Allows tree to grow as much as possible, then prune the tree (`ccp_alpha`)\n",
    "\n",
    "## The cost-complexity prunning\n",
    "Cost-complexity measure:\n",
    "$$\n",
    "R_\\alpha(T)=R(T)+\\alpha∣T∣\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $R(T)$ - The misclassification error (or impurity) of the tree T on the training data.\n",
    "* $|T∣$ - The number of terminal nodes (leaves) in the tree TT, representing its complexity.\n",
    "* $\\alpha$ - A tuning parameter that controls the trade-off between the tree's accuracy and complexity.\n",
    "\n",
    "The we want to minimize the $R_\\alpha$ over the space of subtrees of given tree T. \n",
    "\n",
    "> BWT, what kind of tree we will get if set $\\alpha$ to $0, \\infty$?\n",
    "\n",
    "Greedy algorithm:\n",
    "1. Build the full tree\n",
    "2. For each node $t$ compute $R_\\alpha(T_t)$ of corresponding subtree $T_t$ and $R_\\alpha(t)$ of subtree collapsed into that node.\n",
    "3. Prune the branch, if the $R_\\alpha(T_t) < R_\\alpha(T)$\n",
    "\n",
    "> Do we need to recaclucate the $R_\\alpha$ for non-prunned nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation?\n",
    "\n",
    "import copy\n",
    "\n",
    "def compute_subtree_error(tree: Dict) -> int:\n",
    "    \"\"\"Calculate the total misclassification error of a subtree.\"\"\"\n",
    "    ...\n",
    "    return total_error\n",
    "\n",
    "def compute_subtree_leaves(tree: Dict) -> int:\n",
    "    \"\"\"Count the number of leaf nodes in a subtree.\"\"\"\n",
    "    ...\n",
    "    return total_leaves\n",
    "\n",
    "def collect_pruning_candidates(tree: Dict, candidates: list) -> None:\n",
    "    \"\"\"Collect non-leaf nodes with their effective alpha values.\"\"\"\n",
    "    ...\n",
    "\n",
    "def cost_complexity_pruning(tree: Dict, alpha: float) -> Dict:\n",
    "    \"\"\"Prune the tree using cost-complexity pruning with parameter alpha.\"\"\"\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the tree with alpha=0.1 (adjust alpha based on your needs)\n",
    "alpha = 0.1\n",
    "pruned_tree = cost_complexity_pruning(decision_tree, alpha)\n",
    "\n",
    "# Visualize the pruned tree in Jupyter Notebook\n",
    "dot_pruned = visualize_tree(pruned_tree, feature_names)\n",
    "display(dot_pruned)  # Display directly in the notebook\n",
    "\n",
    "# Test prediction with the pruned tree\n",
    "pruned_prediction = predict(pruned_tree, feature_names, new_data_point)\n",
    "print(f\"Pruned tree prediction: {pruned_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus task 23: Better DT (<1 week deadline!)\n",
    "Re-Implement the **ID3 algorithm** in the scikit-learn compatible maner. The most convenient implementation (for my taste) will be used for following classes, so you should be ready for 5-10 minute talk about your implementation.\n",
    "\n",
    "### Bonus Task 24: Prunning\n",
    "Select any kind of prunning algorithms except for Cost-Complexity Prunnig (e.g. from [here](http://dspace.mit.edu/bitstream/handle/1721.1/6453/AIM-930.pdf?sequence=2)) and implement it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
