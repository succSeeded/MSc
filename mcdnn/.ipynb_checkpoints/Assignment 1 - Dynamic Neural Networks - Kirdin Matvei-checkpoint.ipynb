{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/succSeeded/MSc/blob/main/mcdnn/Assignment%201%20-%20Dynamic%20Neural%20Networks%20-%20Kirdin%20Matvei.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85TMTuHr7Vst"
   },
   "source": [
    "Notebook made by: **Kirdin Matvei**, e-mail: `mdkirdin@student.hse.ru`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wmKzwzP9jcn"
   },
   "source": [
    "# Question 1.\n",
    "\n",
    "**Build and Train a Simple Neural Network.**\n",
    "\n",
    "* Create a basic neural network from scratch using Python and a library of your choice (e.g., TensorFlow, PyTorch, or Keras).\n",
    "* Explain the importance of layers, activation functions, and a training loop for backpropagation and Gradient descent.\n",
    "* Document your code, explain each layer's purpose, and discuss the network's performance. You can use any dataset for explaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the random generator for the purpose of reproducability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28e0c9faa30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 42\n",
    "rng = torch.Generator()\n",
    "rng.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example I will use the ```Fashion MNIST``` dataset, as it is readily available and comes with PyTorch. It contains 28x28 images of clothing, which are to be classified into 10 categories: \n",
    "* T-shirt/top &#150; 0,\n",
    "* Trouser &#150; 1,\n",
    "* Pullover &#150; 2,\n",
    "* Dress &#150; 3,\n",
    "* Coat &#150; 4,\n",
    "* Sandal &#150; 5,\n",
    "* Shirt &#150; 6,\n",
    "* Sneaker &#150; 7,\n",
    "* Bag &#150; 8,\n",
    "* Ankle boot &#150; 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders and slice data up into batches of 64 instances on which the gradient is to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([1024, 1, 28, 28])\n",
      "Shape of y: torch.Size([1024]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 128\n",
    "test_batch_size = 1024\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=train_batch_size, generator=rng)\n",
    "test_dataloader = DataLoader(test_data, batch_size=test_batch_size, generator=rng)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example I decided to use ReLU activation function to prevent the issue of vanishing gradients during the backpropogation. This network consists of 3 layers:\n",
    "\n",
    "* Pooling layer, that downsizes the data to be 512-dimensional.\n",
    "* Dense (fully connected) layer.\n",
    "* Output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transform 28*28 images into flat 1*784 arrays\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the network object and display its overall structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           Flatten-1                  [-1, 784]               0\n",
      "            Linear-2                  [-1, 512]         401,920\n",
      "              ReLU-3                  [-1, 512]               0\n",
      "            Linear-4                  [-1, 512]         262,656\n",
      "              ReLU-5                  [-1, 512]               0\n",
      "            Linear-6                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 2.55\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use cross-entropy loss function and do the backpropogation with stochastic gradient descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error:\\n  Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296760  [  128/60000]\n",
      "loss: 2.278818  [12928/60000]\n",
      "loss: 2.280411  [25728/60000]\n",
      "loss: 2.270170  [38528/60000]\n",
      "loss: 2.244569  [51328/60000]\n",
      "Test Error:\n",
      "  Accuracy: 32.6%, Avg loss: 2.238836 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.236032  [  128/60000]\n",
      "loss: 2.210918  [12928/60000]\n",
      "loss: 2.213142  [25728/60000]\n",
      "loss: 2.203188  [38528/60000]\n",
      "loss: 2.170315  [51328/60000]\n",
      "Test Error:\n",
      "  Accuracy: 43.1%, Avg loss: 2.161447 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.160761  [  128/60000]\n",
      "loss: 2.123993  [12928/60000]\n",
      "loss: 2.123912  [25728/60000]\n",
      "loss: 2.110413  [38528/60000]\n",
      "loss: 2.068359  [51328/60000]\n",
      "Test Error:\n",
      "  Accuracy: 56.7%, Avg loss: 2.053065 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.052167  [  128/60000]\n",
      "loss: 1.999121  [12928/60000]\n",
      "loss: 1.994293  [25728/60000]\n",
      "loss: 1.973079  [38528/60000]\n",
      "loss: 1.925820  [51328/60000]\n",
      "Test Error:\n",
      "  Accuracy: 62.3%, Avg loss: 1.900475 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.894767  [  128/60000]\n",
      "loss: 1.825074  [12928/60000]\n",
      "loss: 1.817431  [25728/60000]\n",
      "loss: 1.786273  [38528/60000]\n",
      "loss: 1.746906  [51328/60000]\n",
      "Test Error:\n",
      "  Accuracy: 63.1%, Avg loss: 1.710935 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "test_acc1 = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_acc1 += [test(test_dataloader, model, loss_fn)]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2.\n",
    "\n",
    "**Implement Dynamic Routing in a Capsule Network.**\n",
    "\n",
    "* Design a small capsule network with dynamic routing to classify images from the CIFAR-10 or MNIST dataset.\n",
    "* Implement dynamic routing between the primary and digit capsules.\n",
    "* Explain the difference in performance when using dynamic routing versus traditional layers.\n",
    "\n",
    "For this task, I will use ```FashionMNIST``` dataset as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the squash function, which will be used in dynamic routing instead of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s):\n",
    "    lengths2 = s.pow(2).sum(dim=2)\n",
    "    lengths = lengths2.sqrt()\n",
    "    s = s * (lengths2 / (1 + lengths2**2) / lengths).view(s.size(0), s.size(1), 1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define the dynamic algorythm itself. It takes in the number of capsules in previous layer and in current layer, as well as the number of iterations of routing process. Coefficients b_ij are initialized as zeroes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Routing(nn.Module):\n",
    "    def __init__(self, input_caps, output_caps, n_iterations):\n",
    "        super(Routing, self).__init__()\n",
    "        self.n_iterations = n_iterations\n",
    "        self.b = nn.Parameter(torch.zeros((input_caps, output_caps)))\n",
    "\n",
    "    def forward(self, u_predict):\n",
    "        batch_size, input_caps, output_caps, output_dim = u_predict.size()\n",
    "\n",
    "        c = nn.functional.softmax(self.b, dim=1)\n",
    "        s = (c.unsqueeze(2) * u_predict).sum(dim=1)\n",
    "        v = squash(s)\n",
    "\n",
    "        if self.n_iterations > 0:\n",
    "            b_batch = self.b.expand((batch_size, input_caps, output_caps))\n",
    "            for r in range(self.n_iterations):\n",
    "                v = v.unsqueeze(1)\n",
    "                b_batch = b_batch + (u_predict * v).sum(-1)\n",
    "\n",
    "                c = nn.functional.softmax(b_batch.view(-1, output_caps), 1).view(\n",
    "                    -1, input_caps, output_caps, 1\n",
    "                )\n",
    "                s = (c * u_predict).sum(dim=1)\n",
    "                v = squash(s)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define primary and digit capsule layers. All layers containing capsules(except for the first one) will be of digit layers. We randomly initialize weights for each capsule in digit layer, and do all the neccessary calculations on their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimaryCapsLayer(nn.Module):\n",
    "    def __init__(self, input_channels, output_caps, output_dim, kernel_size, stride):\n",
    "        super(PrimaryCapsLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_channels,\n",
    "            output_caps * output_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "        )\n",
    "        self.input_channels = input_channels\n",
    "        self.output_caps = output_caps\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv(input)\n",
    "        N, C, H, W = out.size()\n",
    "        out = out.view(N, self.output_caps, self.output_dim, H, W)\n",
    "\n",
    "        # will output N x OUT_CAPS x OUT_DIM\n",
    "        out = out.permute(0, 1, 3, 4, 2).contiguous()\n",
    "        out = out.view(out.size(0), -1, out.size(4))\n",
    "        out = squash(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitCapsLayer(nn.Module):\n",
    "    def __init__(self, input_caps, input_dim, output_caps, output_dim, routing_module):\n",
    "        super(DigitCapsLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_caps = input_caps\n",
    "        self.output_dim = output_dim\n",
    "        self.output_caps = output_caps\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.Tensor(input_caps, input_dim, output_caps * output_dim)\n",
    "        )\n",
    "        self.routing_module = routing_module\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.input_caps)\n",
    "        self.weights.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, caps_output):\n",
    "        caps_output = caps_output.unsqueeze(2)\n",
    "        u_predict = caps_output.matmul(self.weights)\n",
    "        u_predict = u_predict.view(\n",
    "            u_predict.size(0), self.input_caps, self.output_caps, self.output_dim\n",
    "        )\n",
    "        v = self.routing_module(u_predict)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN itself consists of a convolutional and pooling layers to cut down on calculational costs and capuse layers after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleNetwork(nn.Module):\n",
    "    def __init__(self, routing_iterations, n_classes=10):\n",
    "        super(CapsuleNetwork, self).__init__()\n",
    "        self.num_primaryCaps = 32 * 6 * 6\n",
    "        routing_module = Routing(self.num_primaryCaps, n_classes, routing_iterations)\n",
    "        self.CNN_stack = nn.Sequential(\n",
    "            nn.Conv2d(1, 256, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            PrimaryCapsLayer(256, 32, 8, kernel_size=3, stride=2),  # outputs 6*6\n",
    "            DigitCapsLayer(self.num_primaryCaps, 8, n_classes, 16, routing_module),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.CNN_stack(x)\n",
    "        probs = x.pow(2).sum(dim=2).sqrt()\n",
    "        return x, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CapsuleNetwork(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 256, 26, 26]           2,560\n",
      "              ReLU-2          [-1, 256, 26, 26]               0\n",
      "         MaxPool2d-3          [-1, 256, 13, 13]               0\n",
      "            Conv2d-4            [-1, 256, 6, 6]         590,080\n",
      "  PrimaryCapsLayer-5              [-1, 1152, 8]               0\n",
      "           Routing-6               [-1, 10, 16]               0\n",
      "    DigitCapsLayer-7               [-1, 10, 16]               0\n",
      "================================================================\n",
      "Total params: 592,640\n",
      "Trainable params: 592,640\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.11\n",
      "Params size (MB): 2.26\n",
      "Estimated Total Size (MB): 5.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define loss for this model as it was defined in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarginLoss(nn.Module):\n",
    "    def __init__(self, m_pos, m_neg, lambda_):\n",
    "        super(MarginLoss, self).__init__()\n",
    "        self.m_pos = m_pos\n",
    "        self.m_neg = m_neg\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, lengths, targets):\n",
    "        t = torch.zeros(lengths.size()).long()\n",
    "        t = t.scatter_(1, targets.data.view(-1, 1), 1)\n",
    "        targets = torch.autograd.Variable(t)\n",
    "        losses = targets.float() * nn.functional.relu(self.m_pos - lengths).pow(\n",
    "            2\n",
    "        ) + self.lambda_ * (1.0 - targets.float()) * nn.functional.relu(\n",
    "            lengths - self.m_neg\n",
    "        ).pow(\n",
    "            2\n",
    "        )\n",
    "        return losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MarginLoss(0.9, 0.1, 0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = torch.autograd.Variable(X), torch.autograd.Variable(\n",
    "            y, requires_grad=False\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute prediction error\n",
    "        output, probs = model(X)\n",
    "        loss = loss_fn(probs, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.data, (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = torch.autograd.Variable(X), torch.autograd.Variable(y)\n",
    "            output, probs = model(X)\n",
    "            test_loss += loss_fn(probs, y)\n",
    "            # pred = probs.data.max(1, keepdim=True)[\n",
    "            #     1\n",
    "            # ]  # get the index of the max probability\n",
    "            # correct += (\n",
    "            #     pred.eq(y.data.view_as(pred)).cpu().sum()\n",
    "            # )  # number of all correct classifications\n",
    "            correct += (probs.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"\\nTesting:\\n  Accuracy: {100.0 * correct:>0.1f}%, Avg loss: {test_loss:>4f}\\n\"\n",
    "    )\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 103.260178  [  128/60000]\n",
      "loss: 28.245445  [12928/60000]\n",
      "loss: 27.956593  [25728/60000]\n",
      "loss: 26.425293  [38528/60000]\n",
      "loss: 27.989502  [51328/60000]\n",
      "\n",
      "Testing:\n",
      "  Accuracy: 87.9%, Avg loss: 213.582596\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 25.175121  [  128/60000]\n",
      "loss: 25.243233  [12928/60000]\n",
      "loss: 25.300674  [25728/60000]\n",
      "loss: 24.915222  [38528/60000]\n",
      "loss: 26.195204  [51328/60000]\n",
      "\n",
      "Testing:\n",
      "  Accuracy: 89.8%, Avg loss: 208.513519\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 24.477428  [  128/60000]\n",
      "loss: 24.346073  [12928/60000]\n",
      "loss: 24.553593  [25728/60000]\n",
      "loss: 24.456835  [38528/60000]\n",
      "loss: 25.065624  [51328/60000]\n",
      "\n",
      "Testing:\n",
      "  Accuracy: 90.7%, Avg loss: 204.317108\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 23.898336  [  128/60000]\n",
      "loss: 23.773193  [12928/60000]\n",
      "loss: 24.169317  [25728/60000]\n",
      "loss: 24.097225  [38528/60000]\n",
      "loss: 24.390648  [51328/60000]\n",
      "\n",
      "Testing:\n",
      "  Accuracy: 91.2%, Avg loss: 201.009460\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 23.423166  [  128/60000]\n",
      "loss: 23.324677  [12928/60000]\n",
      "loss: 23.751348  [25728/60000]\n",
      "loss: 23.643316  [38528/60000]\n",
      "loss: 23.864738  [51328/60000]\n",
      "\n",
      "Testing:\n",
      "  Accuracy: 91.4%, Avg loss: 200.336456\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "test_acc2 = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_acc2 += [test(test_dataloader, model, loss_fn)]\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compate performances of a regular neural network and CNN in terms of accuracy achieved in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "X = np.arange(1, 6)\n",
    "plt.xticks(X)\n",
    "plt.plot(X, np.array(test_acc1).T, c=\"blue\", label=\"Regular NN\")\n",
    "plt.scatter(X, np.array(test_acc1).T, c=\"blue\")\n",
    "plt.plot(X, np.array(test_acc2).T, c=\"orange\", label=\"CNN\")\n",
    "plt.scatter(X, np.array(test_acc2).T, c=\"orange\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPjhe2WtCOgLZQeXK/m5pHG",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
